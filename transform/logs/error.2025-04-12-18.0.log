2025-04-12 18:35:30 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Starting 6 consumers for the 'contacts' topic...
2025-04-12 18:35:30 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:35:30 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482931185
2025-04-12 18:35:31 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:35:31 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:31 [Worker-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:35:31 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482931223
2025-04-12 18:35:31 [Worker-2] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:35:31 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:35:31 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:35:31 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482931249
2025-04-12 18:35:31 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:35:31 [Worker-3] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:35:31 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:35:31 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482931274
2025-04-12 18:35:31 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:35:31 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:31 [Worker-4] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:35:31 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482931303
2025-04-12 18:35:31 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:35:31 [Worker-5] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:35:31 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:31 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:31 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482931328
2025-04-12 18:35:31 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Started 6 consumers for the 'contacts' topic.
2025-04-12 18:35:31 [Worker-6] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:35:31 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:35:32 [Worker-6] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:32 [Worker-4] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:32 [Worker-3] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:32 [Worker-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:32 [Worker-2] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:32 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:35:32 [Worker-5] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:32 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:35:32 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:35:32 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:35:32 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:35:32 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:35:32 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:35:32 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:35:32 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:35:32 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:35:32 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:35:32 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:35:32 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-5-72b8d544-a6a3-4489-af52-681377f702f6
2025-04-12 18:35:32 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-4-e333f896-36b8-4b40-9ce1-5eb23f1bb4f1
2025-04-12 18:35:32 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-1-78341b27-5275-41ec-9f70-5659cd4bc835
2025-04-12 18:35:32 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-3-19f9bbe2-5dc7-4e90-a4b3-e89874657593
2025-04-12 18:35:32 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:35:32 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-2-5fbccb31-aadd-4938-aefc-39e8da5797e2
2025-04-12 18:35:32 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:35:32 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:35:32 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:35:32 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:35:32 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-6-790cca21-3c86-4e2b-bae9-2788b6b0983e
2025-04-12 18:35:32 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:35:35 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=97, memberId='consumer-contacts-test-group-1-78341b27-5275-41ec-9f70-5659cd4bc835', protocol='range'}
2025-04-12 18:35:35 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=97, memberId='consumer-contacts-test-group-2-5fbccb31-aadd-4938-aefc-39e8da5797e2', protocol='range'}
2025-04-12 18:35:35 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=97, memberId='consumer-contacts-test-group-3-19f9bbe2-5dc7-4e90-a4b3-e89874657593', protocol='range'}
2025-04-12 18:35:35 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=97, memberId='consumer-contacts-test-group-4-e333f896-36b8-4b40-9ce1-5eb23f1bb4f1', protocol='range'}
2025-04-12 18:35:35 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=97, memberId='consumer-contacts-test-group-5-72b8d544-a6a3-4489-af52-681377f702f6', protocol='range'}
2025-04-12 18:35:35 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Finished assignment for group at generation 97: {consumer-contacts-test-group-1-78341b27-5275-41ec-9f70-5659cd4bc835=Assignment(partitions=[test-topic-0]), consumer-contacts-test-group-2-5fbccb31-aadd-4938-aefc-39e8da5797e2=Assignment(partitions=[]), consumer-contacts-test-group-3-19f9bbe2-5dc7-4e90-a4b3-e89874657593=Assignment(partitions=[]), consumer-contacts-test-group-6-790cca21-3c86-4e2b-bae9-2788b6b0983e=Assignment(partitions=[]), consumer-contacts-test-group-5-72b8d544-a6a3-4489-af52-681377f702f6=Assignment(partitions=[]), consumer-contacts-test-group-4-e333f896-36b8-4b40-9ce1-5eb23f1bb4f1=Assignment(partitions=[])}
2025-04-12 18:35:35 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=97, memberId='consumer-contacts-test-group-6-790cca21-3c86-4e2b-bae9-2788b6b0983e', protocol='range'}
2025-04-12 18:35:35 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=97, memberId='consumer-contacts-test-group-2-5fbccb31-aadd-4938-aefc-39e8da5797e2', protocol='range'}
2025-04-12 18:35:35 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=97, memberId='consumer-contacts-test-group-6-790cca21-3c86-4e2b-bae9-2788b6b0983e', protocol='range'}
2025-04-12 18:35:35 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=97, memberId='consumer-contacts-test-group-5-72b8d544-a6a3-4489-af52-681377f702f6', protocol='range'}
2025-04-12 18:35:35 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=97, memberId='consumer-contacts-test-group-3-19f9bbe2-5dc7-4e90-a4b3-e89874657593', protocol='range'}
2025-04-12 18:35:35 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=97, memberId='consumer-contacts-test-group-1-78341b27-5275-41ec-9f70-5659cd4bc835', protocol='range'}
2025-04-12 18:35:35 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=97, memberId='consumer-contacts-test-group-4-e333f896-36b8-4b40-9ce1-5eb23f1bb4f1', protocol='range'}
2025-04-12 18:35:35 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:35:35 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:35:35 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:35:35 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:35:35 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-12 18:35:35 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:35:35 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:35:35 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:35:35 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:35:35 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:35:35 [Worker-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:35:35 [Worker-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-12 18:35:35 [Worker-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=70349, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=14}}
2025-04-12 18:35:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_70, value: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}, offset: 70349
2025-04-12 18:35:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_70
2025-04-12 18:35:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}%n
2025-04-12 18:35:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:35:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:35:40 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:40 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:40 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-04-12 18:35:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482940178
2025-04-12 18:35:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_70', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:35:40 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:40 [kafka-producer-network-thread | producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 8097 with epoch 0
2025-04-12 18:35:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:40 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_81, value: {"_id": {"$oid": "67f10b97558ab0f6396b145b"}, "review": "Not bad, not great either.", "authId": "user_81", "datetime": "2025-03-11T12:19:55.240Z", "sentiment": null}, offset: 70350
2025-04-12 18:35:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_81
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145b"}, "review": "Not bad, not great either.", "authId": "user_81", "datetime": "2025-03-11T12:19:55.240Z", "sentiment": null}%n
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:35:42 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:42 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:42 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-2] Instantiated an idempotent producer.
2025-04-12 18:35:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482942157
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:42 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-2] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_81', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:35:42 [kafka-producer-network-thread | producer-2] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-2] ProducerId set to 8098 with epoch 0
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_24, value: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}, offset: 70351
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_24
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}%n
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-12 18:35:42 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:42 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:42 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-3] Instantiated an idempotent producer.
2025-04-12 18:35:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482942750
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_24', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-12 18:35:42 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-3] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:42 [kafka-producer-network-thread | producer-3] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-3] ProducerId set to 8099 with epoch 0
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 70352
2025-04-12 18:35:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-12 18:35:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-12 18:35:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:35:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:35:44 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:44 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:44 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-4] Instantiated an idempotent producer.
2025-04-12 18:35:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482944342
2025-04-12 18:35:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:35:44 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-4] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:44 [kafka-producer-network-thread | producer-4] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-4] ProducerId set to 8100 with epoch 0
2025-04-12 18:35:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_48, value: {"_id": {"$oid": "67f10b97558ab0f6396b143a"}, "review": "Terrible experience, very disappointed.", "authId": "user_48", "datetime": "2025-01-15T02:57:46.191Z", "sentiment": null}, offset: 70353
2025-04-12 18:35:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_48
2025-04-12 18:35:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143a"}, "review": "Terrible experience, very disappointed.", "authId": "user_48", "datetime": "2025-01-15T02:57:46.191Z", "sentiment": null}%n
2025-04-12 18:35:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:35:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:35:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-5] Instantiated an idempotent producer.
2025-04-12 18:35:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482945896
2025-04-12 18:35:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_48', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:35:45 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-5] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:45 [kafka-producer-network-thread | producer-5] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-5] ProducerId set to 8101 with epoch 0
2025-04-12 18:35:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 70354
2025-04-12 18:35:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-12 18:35:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}%n
2025-04-12 18:35:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:35:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:35:47 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:47 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:47 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-6] Instantiated an idempotent producer.
2025-04-12 18:35:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482947461
2025-04-12 18:35:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_91', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:35:47 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-6] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:47 [kafka-producer-network-thread | producer-6] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-6] ProducerId set to 8102 with epoch 0
2025-04-12 18:35:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:47 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_1, value: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}, offset: 70355
2025-04-12 18:35:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_1
2025-04-12 18:35:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}%n
2025-04-12 18:35:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:35:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:35:49 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:49 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:49 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-7] Instantiated an idempotent producer.
2025-04-12 18:35:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482949025
2025-04-12 18:35:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_1', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:35:49 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-7] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:49 [kafka-producer-network-thread | producer-7] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-7] ProducerId set to 8103 with epoch 0
2025-04-12 18:35:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:49 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_78, value: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}, offset: 70356
2025-04-12 18:35:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_78
2025-04-12 18:35:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}%n
2025-04-12 18:35:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:35:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:35:50 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:50 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:50 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-8] Instantiated an idempotent producer.
2025-04-12 18:35:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482950569
2025-04-12 18:35:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_78', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:35:50 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-8] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:50 [kafka-producer-network-thread | producer-8] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-8] ProducerId set to 8104 with epoch 0
2025-04-12 18:35:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:50 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_19, value: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}, offset: 70357
2025-04-12 18:35:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_19
2025-04-12 18:35:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}%n
2025-04-12 18:35:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-12 18:35:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-12 18:35:52 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:52 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:52 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-9] Instantiated an idempotent producer.
2025-04-12 18:35:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482952112
2025-04-12 18:35:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_19', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-12 18:35:52 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-9] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:52 [kafka-producer-network-thread | producer-9] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-9] ProducerId set to 8105 with epoch 0
2025-04-12 18:35:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:52 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 70358
2025-04-12 18:35:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-12 18:35:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-12 18:35:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:35:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:35:53 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:53 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:53 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-10] Instantiated an idempotent producer.
2025-04-12 18:35:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482953670
2025-04-12 18:35:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:35:53 [kafka-producer-network-thread | producer-10] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-10] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:53 [kafka-producer-network-thread | producer-10] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-10] ProducerId set to 8106 with epoch 0
2025-04-12 18:35:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:53 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_97, value: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}, offset: 70359
2025-04-12 18:35:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_97
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}%n
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:35:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-11] Instantiated an idempotent producer.
2025-04-12 18:35:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482955228
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_97', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:35:55 [kafka-producer-network-thread | producer-11] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-11] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:55 [kafka-producer-network-thread | producer-11] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-11] ProducerId set to 8107 with epoch 0
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_42, value: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}, offset: 70360
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_42
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}%n
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:35:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-12] Instantiated an idempotent producer.
2025-04-12 18:35:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482955978
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_42', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:35:55 [kafka-producer-network-thread | producer-12] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-12] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:55 [kafka-producer-network-thread | producer-12] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-12] ProducerId set to 8108 with epoch 0
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_34, value: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}, offset: 70361
2025-04-12 18:35:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_34
2025-04-12 18:35:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}%n
2025-04-12 18:35:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:35:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:35:56 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:56 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:56 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-13] Instantiated an idempotent producer.
2025-04-12 18:35:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482956534
2025-04-12 18:35:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_34', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:35:56 [kafka-producer-network-thread | producer-13] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-13] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:56 [kafka-producer-network-thread | producer-13] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-13] ProducerId set to 8109 with epoch 0
2025-04-12 18:35:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:56 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_92, value: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}, offset: 70362
2025-04-12 18:35:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_92
2025-04-12 18:35:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}%n
2025-04-12 18:35:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:35:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:35:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-14] Instantiated an idempotent producer.
2025-04-12 18:35:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482958069
2025-04-12 18:35:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_92', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:35:58 [kafka-producer-network-thread | producer-14] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-14] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:58 [kafka-producer-network-thread | producer-14] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-14] ProducerId set to 8110 with epoch 0
2025-04-12 18:35:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_64, value: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}, offset: 70363
2025-04-12 18:35:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_64
2025-04-12 18:35:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}%n
2025-04-12 18:35:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:35:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:35:59 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:35:59 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:35:59 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-15] Instantiated an idempotent producer.
2025-04-12 18:35:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:35:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:35:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482959595
2025-04-12 18:35:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:35:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_64', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:35:59 [kafka-producer-network-thread | producer-15] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-15] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:35:59 [kafka-producer-network-thread | producer-15] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-15] ProducerId set to 8111 with epoch 0
2025-04-12 18:35:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:35:59 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_3, value: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}, offset: 70364
2025-04-12 18:35:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_3
2025-04-12 18:36:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}%n
2025-04-12 18:36:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 18:36:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 18:36:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-16] Instantiated an idempotent producer.
2025-04-12 18:36:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482961160
2025-04-12 18:36:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_3', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 18:36:01 [kafka-producer-network-thread | producer-16] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-16] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:01 [kafka-producer-network-thread | producer-16] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-16] ProducerId set to 8112 with epoch 0
2025-04-12 18:36:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:01 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_64, value: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}, offset: 70365
2025-04-12 18:36:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_64
2025-04-12 18:36:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}%n
2025-04-12 18:36:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:02 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:02 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:02 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-17] Instantiated an idempotent producer.
2025-04-12 18:36:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482962685
2025-04-12 18:36:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_64', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:02 [kafka-producer-network-thread | producer-17] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-17] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:02 [kafka-producer-network-thread | producer-17] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-17] ProducerId set to 8113 with epoch 0
2025-04-12 18:36:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:02 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_56, value: {"_id": {"$oid": "67f10b97558ab0f6396b1442"}, "review": "It's okay, does the job.", "authId": "user_56", "datetime": "2024-12-14T20:56:35.004Z", "sentiment": null}, offset: 70366
2025-04-12 18:36:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_56
2025-04-12 18:36:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1442"}, "review": "It's okay, does the job.", "authId": "user_56", "datetime": "2024-12-14T20:56:35.004Z", "sentiment": null}%n
2025-04-12 18:36:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-12 18:36:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-12 18:36:04 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:04 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:04 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-18] Instantiated an idempotent producer.
2025-04-12 18:36:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482964214
2025-04-12 18:36:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_56', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-12 18:36:04 [kafka-producer-network-thread | producer-18] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-18] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:04 [kafka-producer-network-thread | producer-18] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-18] ProducerId set to 8114 with epoch 0
2025-04-12 18:36:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:04 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_41, value: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}, offset: 70367
2025-04-12 18:36:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_41
2025-04-12 18:36:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}%n
2025-04-12 18:36:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:05 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:05 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:05 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-19] Instantiated an idempotent producer.
2025-04-12 18:36:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482965736
2025-04-12 18:36:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_41', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:05 [kafka-producer-network-thread | producer-19] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-19] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:05 [kafka-producer-network-thread | producer-19] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-19] ProducerId set to 8115 with epoch 0
2025-04-12 18:36:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:05 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_11, value: {"_id": {"$oid": "67f10b97558ab0f6396b1415"}, "review": "Would not recommend to anyone.", "authId": "user_11", "datetime": "2024-12-10T23:20:11.570Z", "sentiment": null}, offset: 70368
2025-04-12 18:36:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_11
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1415"}, "review": "Would not recommend to anyone.", "authId": "user_11", "datetime": "2024-12-10T23:20:11.570Z", "sentiment": null}%n
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:07 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:07 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:07 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-20] Instantiated an idempotent producer.
2025-04-12 18:36:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482967251
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_11', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:07 [kafka-producer-network-thread | producer-20] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-20] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:07 [kafka-producer-network-thread | producer-20] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-20] ProducerId set to 8116 with epoch 0
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 70369
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:36:07 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:07 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:07 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-21] Instantiated an idempotent producer.
2025-04-12 18:36:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482967800
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:36:07 [kafka-producer-network-thread | producer-21] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-21] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:07 [kafka-producer-network-thread | producer-21] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-21] ProducerId set to 8117 with epoch 0
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_66, value: {"_id": {"$oid": "67f10b97558ab0f6396b144c"}, "review": "Great quality, will buy again.", "authId": "user_66", "datetime": "2025-02-01T17:37:18.002Z", "sentiment": null}, offset: 70370
2025-04-12 18:36:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_66
2025-04-12 18:36:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144c"}, "review": "Great quality, will buy again.", "authId": "user_66", "datetime": "2025-02-01T17:37:18.002Z", "sentiment": null}%n
2025-04-12 18:36:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: 0.1

2025-04-12 18:36:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: 0.1

2025-04-12 18:36:08 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:08 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:08 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-22] Instantiated an idempotent producer.
2025-04-12 18:36:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482968335
2025-04-12 18:36:08 [kafka-producer-network-thread | producer-22] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-22] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:08 [kafka-producer-network-thread | producer-22] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-22] ProducerId set to 8118 with epoch 0
2025-04-12 18:36:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_66', sentiment=magnitude: 0.1
score: 0.1
}
2025-04-12 18:36:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:08 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_90, value: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}, offset: 70371
2025-04-12 18:36:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_90
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}%n
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: -0.1

2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: -0.1

2025-04-12 18:36:09 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-23
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:09 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:09 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-23] Instantiated an idempotent producer.
2025-04-12 18:36:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482969083
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_90', sentiment=magnitude: 0.1
score: -0.1
}
2025-04-12 18:36:09 [kafka-producer-network-thread | producer-23] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-23] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:09 [kafka-producer-network-thread | producer-23] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-23] ProducerId set to 8119 with epoch 0
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70372
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}%n
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:36:09 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-24
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:09 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:09 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-24] Instantiated an idempotent producer.
2025-04-12 18:36:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482969614
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_50', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:36:09 [kafka-producer-network-thread | producer-24] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-24] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:09 [kafka-producer-network-thread | producer-24] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-24] ProducerId set to 8120 with epoch 0
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_93, value: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}, offset: 70373
2025-04-12 18:36:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_93
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}%n
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:11 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-25
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:11 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:11 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-25] Instantiated an idempotent producer.
2025-04-12 18:36:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482971120
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_93', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:11 [kafka-producer-network-thread | producer-25] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-25] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:11 [kafka-producer-network-thread | producer-25] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-25] ProducerId set to 8121 with epoch 0
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_63, value: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}, offset: 70374
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_63
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}%n
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 18:36:11 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-26
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:11 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:11 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-26] Instantiated an idempotent producer.
2025-04-12 18:36:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482971688
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_63', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 18:36:11 [kafka-producer-network-thread | producer-26] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-26] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:11 [kafka-producer-network-thread | producer-26] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-26] ProducerId set to 8122 with epoch 0
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_34, value: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}, offset: 70375
2025-04-12 18:36:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_34
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}%n
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:36:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-27
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:12 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-27] Instantiated an idempotent producer.
2025-04-12 18:36:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482972226
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:12 [kafka-producer-network-thread | producer-27] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-27] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_34', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:36:12 [kafka-producer-network-thread | producer-27] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-27] ProducerId set to 8123 with epoch 0
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_98, value: {"_id": {"$oid": "67f10b97558ab0f6396b146c"}, "review": "Not bad, not great either.", "authId": "user_98", "datetime": "2025-03-21T23:54:21.817Z", "sentiment": null}, offset: 70376
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_98
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146c"}, "review": "Not bad, not great either.", "authId": "user_98", "datetime": "2025-03-21T23:54:21.817Z", "sentiment": null}%n
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:36:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-28
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:12 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-28] Instantiated an idempotent producer.
2025-04-12 18:36:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482972739
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_98', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:36:12 [kafka-producer-network-thread | producer-28] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-28] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:12 [kafka-producer-network-thread | producer-28] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-28] ProducerId set to 8124 with epoch 0
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_64, value: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}, offset: 70377
2025-04-12 18:36:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_64
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}%n
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:13 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-29
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:13 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:13 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-29] Instantiated an idempotent producer.
2025-04-12 18:36:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482973264
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_64', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:13 [kafka-producer-network-thread | producer-29] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-29] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:13 [kafka-producer-network-thread | producer-29] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-29] ProducerId set to 8125 with epoch 0
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_37, value: {"_id": {"$oid": "67f10b97558ab0f6396b142f"}, "review": "Mediocre quality, nothing special.", "authId": "user_37", "datetime": "2024-12-22T11:29:54.974Z", "sentiment": null}, offset: 70378
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_37
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142f"}, "review": "Mediocre quality, nothing special.", "authId": "user_37", "datetime": "2024-12-22T11:29:54.974Z", "sentiment": null}%n
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:13 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-30
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:13 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:13 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-30] Instantiated an idempotent producer.
2025-04-12 18:36:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482973798
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_37', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:13 [kafka-producer-network-thread | producer-30] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-30] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:13 [kafka-producer-network-thread | producer-30] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-30] ProducerId set to 8126 with epoch 0
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_32, value: {"_id": {"$oid": "67f10b97558ab0f6396b142a"}, "review": "Fantastic performance and very durable.", "authId": "user_32", "datetime": "2025-01-03T02:26:59.600Z", "sentiment": null}, offset: 70379
2025-04-12 18:36:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_32
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142a"}, "review": "Fantastic performance and very durable.", "authId": "user_32", "datetime": "2025-01-03T02:26:59.600Z", "sentiment": null}%n
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 18:36:14 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-31
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:14 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:14 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-31] Instantiated an idempotent producer.
2025-04-12 18:36:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482974321
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_32', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 18:36:14 [kafka-producer-network-thread | producer-31] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-31] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:14 [kafka-producer-network-thread | producer-31] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-31] ProducerId set to 8127 with epoch 0
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_93, value: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}, offset: 70380
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_93
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}%n
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:14 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-32
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:14 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:14 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-32] Instantiated an idempotent producer.
2025-04-12 18:36:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482974835
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_93', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:14 [kafka-producer-network-thread | producer-32] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-32] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:14 [kafka-producer-network-thread | producer-32] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-32] ProducerId set to 8128 with epoch 0
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_67, value: {"_id": {"$oid": "67f10b97558ab0f6396b144d"}, "review": "Works like a charm, perfect!", "authId": "user_67", "datetime": "2025-02-10T15:03:00.523Z", "sentiment": null}, offset: 70381
2025-04-12 18:36:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_67
2025-04-12 18:36:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144d"}, "review": "Works like a charm, perfect!", "authId": "user_67", "datetime": "2025-02-10T15:03:00.523Z", "sentiment": null}%n
2025-04-12 18:36:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 18:36:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 18:36:16 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-33
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:16 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:16 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-33] Instantiated an idempotent producer.
2025-04-12 18:36:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482976356
2025-04-12 18:36:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_67', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 18:36:16 [kafka-producer-network-thread | producer-33] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-33] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:16 [kafka-producer-network-thread | producer-33] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-33] ProducerId set to 8129 with epoch 0
2025-04-12 18:36:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:16 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_79, value: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}, offset: 70382
2025-04-12 18:36:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_79
2025-04-12 18:36:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}%n
2025-04-12 18:36:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:17 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-34
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:17 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:17 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-34] Instantiated an idempotent producer.
2025-04-12 18:36:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482977836
2025-04-12 18:36:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_79', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:17 [kafka-producer-network-thread | producer-34] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-34] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:17 [kafka-producer-network-thread | producer-34] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-34] ProducerId set to 8130 with epoch 0
2025-04-12 18:36:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:17 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_82, value: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}, offset: 70383
2025-04-12 18:36:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_82
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}%n
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-35
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-35] Instantiated an idempotent producer.
2025-04-12 18:36:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482978376
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_82', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:18 [kafka-producer-network-thread | producer-35] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-35] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:18 [kafka-producer-network-thread | producer-35] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-35] ProducerId set to 8131 with epoch 0
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 70384
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:36:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-36
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-36] Instantiated an idempotent producer.
2025-04-12 18:36:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482978928
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:36:18 [kafka-producer-network-thread | producer-36] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-36] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:18 [kafka-producer-network-thread | producer-36] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-36] ProducerId set to 8132 with epoch 0
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_79, value: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}, offset: 70385
2025-04-12 18:36:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_79
2025-04-12 18:36:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}%n
2025-04-12 18:36:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:20 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-37
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:20 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:20 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-37] Instantiated an idempotent producer.
2025-04-12 18:36:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482980458
2025-04-12 18:36:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_79', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:20 [kafka-producer-network-thread | producer-37] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-37] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:20 [kafka-producer-network-thread | producer-37] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-37] ProducerId set to 8133 with epoch 0
2025-04-12 18:36:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:20 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_89, value: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}, offset: 70386
2025-04-12 18:36:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_89
2025-04-12 18:36:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}%n
2025-04-12 18:36:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-12 18:36:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-12 18:36:21 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-38
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:21 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:21 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-38] Instantiated an idempotent producer.
2025-04-12 18:36:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482981115
2025-04-12 18:36:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_89', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-12 18:36:21 [kafka-producer-network-thread | producer-38] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-38] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:21 [kafka-producer-network-thread | producer-38] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-38] ProducerId set to 8134 with epoch 0
2025-04-12 18:36:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:21 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_82, value: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}, offset: 70387
2025-04-12 18:36:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_82
2025-04-12 18:36:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}%n
2025-04-12 18:36:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:22 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-39
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:22 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:22 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-39] Instantiated an idempotent producer.
2025-04-12 18:36:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482982625
2025-04-12 18:36:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_82', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:22 [kafka-producer-network-thread | producer-39] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-39] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:22 [kafka-producer-network-thread | producer-39] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-39] ProducerId set to 8135 with epoch 0
2025-04-12 18:36:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:22 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_63, value: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}, offset: 70388
2025-04-12 18:36:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_63
2025-04-12 18:36:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}%n
2025-04-12 18:36:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 18:36:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 18:36:23 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-40
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:23 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:23 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-40] Instantiated an idempotent producer.
2025-04-12 18:36:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482983426
2025-04-12 18:36:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:23 [kafka-producer-network-thread | producer-40] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-40] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_63', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 18:36:23 [kafka-producer-network-thread | producer-40] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-40] ProducerId set to 8136 with epoch 0
2025-04-12 18:36:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:23 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_57, value: {"_id": {"$oid": "67f10b97558ab0f6396b1443"}, "review": "This changed my life!", "authId": "user_57", "datetime": "2025-03-21T00:28:58.918Z", "sentiment": null}, offset: 70389
2025-04-12 18:36:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_57
2025-04-12 18:36:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1443"}, "review": "This changed my life!", "authId": "user_57", "datetime": "2025-03-21T00:28:58.918Z", "sentiment": null}%n
2025-04-12 18:36:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-12 18:36:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-12 18:36:24 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-41
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:24 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:24 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-41] Instantiated an idempotent producer.
2025-04-12 18:36:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482984908
2025-04-12 18:36:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_57', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-12 18:36:24 [kafka-producer-network-thread | producer-41] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-41] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:24 [kafka-producer-network-thread | producer-41] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-41] ProducerId set to 8137 with epoch 0
2025-04-12 18:36:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:24 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_57, value: {"_id": {"$oid": "67f10b97558ab0f6396b1443"}, "review": "This changed my life!", "authId": "user_57", "datetime": "2025-03-21T00:28:58.918Z", "sentiment": null}, offset: 70390
2025-04-12 18:36:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_57
2025-04-12 18:36:25 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1443"}, "review": "This changed my life!", "authId": "user_57", "datetime": "2025-03-21T00:28:58.918Z", "sentiment": null}%n
2025-04-12 18:36:25 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-12 18:36:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-12 18:36:25 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-42
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:25 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:25 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-42] Instantiated an idempotent producer.
2025-04-12 18:36:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482985465
2025-04-12 18:36:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_57', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-12 18:36:25 [kafka-producer-network-thread | producer-42] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-42] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:25 [kafka-producer-network-thread | producer-42] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-42] ProducerId set to 8138 with epoch 0
2025-04-12 18:36:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:25 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_69, value: {"_id": {"$oid": "67f10b97558ab0f6396b144f"}, "review": "Decent, but not amazing.", "authId": "user_69", "datetime": "2024-12-28T22:45:48.069Z", "sentiment": null}, offset: 70391
2025-04-12 18:36:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_69
2025-04-12 18:36:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144f"}, "review": "Decent, but not amazing.", "authId": "user_69", "datetime": "2024-12-28T22:45:48.069Z", "sentiment": null}%n
2025-04-12 18:36:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:36:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:36:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-43
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-43] Instantiated an idempotent producer.
2025-04-12 18:36:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482987000
2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_69', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:36:27 [kafka-producer-network-thread | producer-43] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-43] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:27 [kafka-producer-network-thread | producer-43] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-43] ProducerId set to 8139 with epoch 0
2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_47, value: {"_id": {"$oid": "67f10b97558ab0f6396b1439"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_47", "datetime": "2025-01-08T17:35:19.231Z", "sentiment": null}, offset: 70392
2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_47
2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1439"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_47", "datetime": "2025-01-08T17:35:19.231Z", "sentiment": null}%n
2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.8
score: 0.4

2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.8
score: 0.4

2025-04-12 18:36:27 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-44
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:27 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:27 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-44] Instantiated an idempotent producer.
2025-04-12 18:36:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482987548
2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:27 [kafka-producer-network-thread | producer-44] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-44] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_47', sentiment=magnitude: 0.8
score: 0.4
}
2025-04-12 18:36:27 [kafka-producer-network-thread | producer-44] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-44] ProducerId set to 8140 with epoch 0
2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_59, value: {"_id": {"$oid": "67f10b97558ab0f6396b1445"}, "review": "It's okay, does the job.", "authId": "user_59", "datetime": "2024-12-03T13:32:51.362Z", "sentiment": null}, offset: 70393
2025-04-12 18:36:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_59
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1445"}, "review": "It's okay, does the job.", "authId": "user_59", "datetime": "2024-12-03T13:32:51.362Z", "sentiment": null}%n
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-12 18:36:28 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-45
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:28 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:28 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-45] Instantiated an idempotent producer.
2025-04-12 18:36:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482988084
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_59', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-12 18:36:28 [kafka-producer-network-thread | producer-45] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-45] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:28 [kafka-producer-network-thread | producer-45] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-45] ProducerId set to 8141 with epoch 0
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_93, value: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}, offset: 70394
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_93
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}%n
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:28 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-46
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:28 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:28 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-46] Instantiated an idempotent producer.
2025-04-12 18:36:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482988594
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_93', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:28 [kafka-producer-network-thread | producer-46] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-46] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:28 [kafka-producer-network-thread | producer-46] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-46] ProducerId set to 8142 with epoch 0
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_36, value: {"_id": {"$oid": "67f10b97558ab0f6396b142e"}, "review": "Works like a charm, perfect!", "authId": "user_36", "datetime": "2024-12-07T01:28:05.981Z", "sentiment": null}, offset: 70395
2025-04-12 18:36:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_36
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142e"}, "review": "Works like a charm, perfect!", "authId": "user_36", "datetime": "2024-12-07T01:28:05.981Z", "sentiment": null}%n
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:36:29 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-47
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:29 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:29 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-47] Instantiated an idempotent producer.
2025-04-12 18:36:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482989130
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_36', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:36:29 [kafka-producer-network-thread | producer-47] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-47] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:29 [kafka-producer-network-thread | producer-47] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-47] ProducerId set to 8143 with epoch 0
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_99, value: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}, offset: 70396
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_99
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}%n
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-12 18:36:29 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-48
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:29 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:29 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-48] Instantiated an idempotent producer.
2025-04-12 18:36:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482989663
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:29 [kafka-producer-network-thread | producer-48] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-48] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_99', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-12 18:36:29 [kafka-producer-network-thread | producer-48] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-48] ProducerId set to 8144 with epoch 0
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_17, value: {"_id": {"$oid": "67f10b97558ab0f6396b141b"}, "review": "Terrible experience, very disappointed.", "authId": "user_17", "datetime": "2025-03-01T00:30:53.030Z", "sentiment": null}, offset: 70397
2025-04-12 18:36:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_17
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141b"}, "review": "Terrible experience, very disappointed.", "authId": "user_17", "datetime": "2025-03-01T00:30:53.030Z", "sentiment": null}%n
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:30 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-49
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:30 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:30 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-49] Instantiated an idempotent producer.
2025-04-12 18:36:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482990193
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_17', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:30 [kafka-producer-network-thread | producer-49] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-49] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:30 [kafka-producer-network-thread | producer-49] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-49] ProducerId set to 8145 with epoch 0
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_48, value: {"_id": {"$oid": "67f10b97558ab0f6396b143a"}, "review": "Terrible experience, very disappointed.", "authId": "user_48", "datetime": "2025-01-15T02:57:46.191Z", "sentiment": null}, offset: 70398
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_48
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143a"}, "review": "Terrible experience, very disappointed.", "authId": "user_48", "datetime": "2025-01-15T02:57:46.191Z", "sentiment": null}%n
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:30 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-50
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:30 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:30 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-50] Instantiated an idempotent producer.
2025-04-12 18:36:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482990707
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_48', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:30 [kafka-producer-network-thread | producer-50] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-50] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:30 [kafka-producer-network-thread | producer-50] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-50] ProducerId set to 8146 with epoch 0
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_18, value: {"_id": {"$oid": "67f10b97558ab0f6396b141c"}, "review": "Doesn't work as advertised.", "authId": "user_18", "datetime": "2025-02-20T09:53:18.980Z", "sentiment": null}, offset: 70399
2025-04-12 18:36:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_18
2025-04-12 18:36:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141c"}, "review": "Doesn't work as advertised.", "authId": "user_18", "datetime": "2025-02-20T09:53:18.980Z", "sentiment": null}%n
2025-04-12 18:36:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:31 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-51
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:31 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:31 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-51] Instantiated an idempotent producer.
2025-04-12 18:36:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482991290
2025-04-12 18:36:31 [kafka-producer-network-thread | producer-51] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-51] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:31 [kafka-producer-network-thread | producer-51] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-51] ProducerId set to 8147 with epoch 0
2025-04-12 18:36:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_18', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:31 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_2, value: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}, offset: 70400
2025-04-12 18:36:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_2
2025-04-12 18:36:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}%n
2025-04-12 18:36:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-12 18:36:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-12 18:36:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-52
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-52] Instantiated an idempotent producer.
2025-04-12 18:36:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482992818
2025-04-12 18:36:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_2', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-12 18:36:32 [kafka-producer-network-thread | producer-52] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-52] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:32 [kafka-producer-network-thread | producer-52] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-52] ProducerId set to 8148 with epoch 0
2025-04-12 18:36:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_86, value: {"_id": {"$oid": "67f10b97558ab0f6396b1460"}, "review": "Smooth and easy to use.", "authId": "user_86", "datetime": "2025-02-22T20:30:11.182Z", "sentiment": null}, offset: 70401
2025-04-12 18:36:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_86
2025-04-12 18:36:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1460"}, "review": "Smooth and easy to use.", "authId": "user_86", "datetime": "2025-02-22T20:30:11.182Z", "sentiment": null}%n
2025-04-12 18:36:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = 
2025-04-12 18:36:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data 
2025-04-12 18:36:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-53
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-53] Instantiated an idempotent producer.
2025-04-12 18:36:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482993378
2025-04-12 18:36:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_86', sentiment=}
2025-04-12 18:36:33 [kafka-producer-network-thread | producer-53] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-53] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:33 [kafka-producer-network-thread | producer-53] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-53] ProducerId set to 8149 with epoch 0
2025-04-12 18:36:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_65, value: {"_id": {"$oid": "67f10b97558ab0f6396b144b"}, "review": "Horrible customer service.", "authId": "user_65", "datetime": "2025-01-22T00:39:23.653Z", "sentiment": null}, offset: 70402
2025-04-12 18:36:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_65
2025-04-12 18:36:34 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144b"}, "review": "Horrible customer service.", "authId": "user_65", "datetime": "2025-01-22T00:39:23.653Z", "sentiment": null}%n
2025-04-12 18:36:34 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:36:34 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:36:34 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-54
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:34 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:34 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-54] Instantiated an idempotent producer.
2025-04-12 18:36:34 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:34 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:34 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482994902
2025-04-12 18:36:34 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:34 [kafka-producer-network-thread | producer-54] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-54] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:34 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_65', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:36:34 [kafka-producer-network-thread | producer-54] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-54] ProducerId set to 8150 with epoch 0
2025-04-12 18:36:34 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:34 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 70403
2025-04-12 18:36:34 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-12 18:36:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-12 18:36:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-55
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-55] Instantiated an idempotent producer.
2025-04-12 18:36:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482995441
2025-04-12 18:36:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:35 [kafka-producer-network-thread | producer-55] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-55] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:35 [kafka-producer-network-thread | producer-55] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-55] ProducerId set to 8151 with epoch 0
2025-04-12 18:36:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_20, value: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}, offset: 70404
2025-04-12 18:36:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_20
2025-04-12 18:36:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}%n
2025-04-12 18:36:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-12 18:36:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-12 18:36:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-56
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-56] Instantiated an idempotent producer.
2025-04-12 18:36:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482995997
2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_20', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-12 18:36:36 [kafka-producer-network-thread | producer-56] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-56] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:36 [kafka-producer-network-thread | producer-56] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-56] ProducerId set to 8152 with epoch 0
2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70405
2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}%n
2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:36:36 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-57
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:36 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:36 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-57] Instantiated an idempotent producer.
2025-04-12 18:36:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482996510
2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:36 [kafka-producer-network-thread | producer-57] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-57] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_50', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:36:36 [kafka-producer-network-thread | producer-57] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-57] ProducerId set to 8153 with epoch 0
2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_21, value: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}, offset: 70406
2025-04-12 18:36:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_21
2025-04-12 18:36:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}%n
2025-04-12 18:36:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:36:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:36:38 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-58
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:38 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:38 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-58] Instantiated an idempotent producer.
2025-04-12 18:36:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482998030
2025-04-12 18:36:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_21', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:36:38 [kafka-producer-network-thread | producer-58] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-58] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:38 [kafka-producer-network-thread | producer-58] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-58] ProducerId set to 8154 with epoch 0
2025-04-12 18:36:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_49, value: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "Its alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}, offset: 70407
2025-04-12 18:36:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_49
2025-04-12 18:36:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "Its alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}%n
2025-04-12 18:36:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-12 18:36:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-12 18:36:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-59
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-59] Instantiated an idempotent producer.
2025-04-12 18:36:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744482999545
2025-04-12 18:36:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_49', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-12 18:36:39 [kafka-producer-network-thread | producer-59] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-59] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:39 [kafka-producer-network-thread | producer-59] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-59] ProducerId set to 8155 with epoch 0
2025-04-12 18:36:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:39 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_97, value: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}, offset: 70408
2025-04-12 18:36:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_97
2025-04-12 18:36:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}%n
2025-04-12 18:36:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:41 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-60
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:41 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:41 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-60] Instantiated an idempotent producer.
2025-04-12 18:36:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483001081
2025-04-12 18:36:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_97', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:41 [kafka-producer-network-thread | producer-60] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-60] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:41 [kafka-producer-network-thread | producer-60] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-60] ProducerId set to 8156 with epoch 0
2025-04-12 18:36:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:41 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_8, value: {"_id": {"$oid": "67f10b97558ab0f6396b1412"}, "review": "Serves its purpose, but nothing more.", "authId": "user_8", "datetime": "2024-12-17T21:27:23.328Z", "sentiment": null}, offset: 70409
2025-04-12 18:36:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_8
2025-04-12 18:36:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1412"}, "review": "Serves its purpose, but nothing more.", "authId": "user_8", "datetime": "2024-12-17T21:27:23.328Z", "sentiment": null}%n
2025-04-12 18:36:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-12 18:36:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-12 18:36:42 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-61
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:42 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:42 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-61] Instantiated an idempotent producer.
2025-04-12 18:36:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483002614
2025-04-12 18:36:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:42 [kafka-producer-network-thread | producer-61] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-61] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_8', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-12 18:36:42 [kafka-producer-network-thread | producer-61] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-61] ProducerId set to 8157 with epoch 0
2025-04-12 18:36:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:42 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_55, value: {"_id": {"$oid": "67f10b97558ab0f6396b1441"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_55", "datetime": "2025-02-18T10:02:17.105Z", "sentiment": null}, offset: 70410
2025-04-12 18:36:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_55
2025-04-12 18:36:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1441"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_55", "datetime": "2025-02-18T10:02:17.105Z", "sentiment": null}%n
2025-04-12 18:36:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.3
score: 0.6

2025-04-12 18:36:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.3
score: 0.6

2025-04-12 18:36:44 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-62
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:44 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:44 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-62] Instantiated an idempotent producer.
2025-04-12 18:36:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483004086
2025-04-12 18:36:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_55', sentiment=magnitude: 1.3
score: 0.6
}
2025-04-12 18:36:44 [kafka-producer-network-thread | producer-62] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-62] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:44 [kafka-producer-network-thread | producer-62] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-62] ProducerId set to 8158 with epoch 0
2025-04-12 18:36:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_77, value: {"_id": {"$oid": "67f10b97558ab0f6396b1457"}, "review": "Smooth and easy to use.", "authId": "user_77", "datetime": "2025-01-05T21:10:33.619Z", "sentiment": null}, offset: 70411
2025-04-12 18:36:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_77
2025-04-12 18:36:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1457"}, "review": "Smooth and easy to use.", "authId": "user_77", "datetime": "2025-01-05T21:10:33.619Z", "sentiment": null}%n
2025-04-12 18:36:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = 
2025-04-12 18:36:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data 
2025-04-12 18:36:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-63
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-63] Instantiated an idempotent producer.
2025-04-12 18:36:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483005620
2025-04-12 18:36:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_77', sentiment=}
2025-04-12 18:36:45 [kafka-producer-network-thread | producer-63] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-63] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:45 [kafka-producer-network-thread | producer-63] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-63] ProducerId set to 8159 with epoch 0
2025-04-12 18:36:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_14, value: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}, offset: 70412
2025-04-12 18:36:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_14
2025-04-12 18:36:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}%n
2025-04-12 18:36:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:36:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:36:47 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-64
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:47 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:47 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-64] Instantiated an idempotent producer.
2025-04-12 18:36:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483007155
2025-04-12 18:36:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:47 [kafka-producer-network-thread | producer-64] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-64] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_14', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:36:47 [kafka-producer-network-thread | producer-64] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-64] ProducerId set to 8160 with epoch 0
2025-04-12 18:36:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:47 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_35, value: {"_id": {"$oid": "67f10b97558ab0f6396b142d"}, "review": "Terrible experience, very disappointed.", "authId": "user_35", "datetime": "2025-03-06T15:09:01.869Z", "sentiment": null}, offset: 70413
2025-04-12 18:36:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_35
2025-04-12 18:36:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142d"}, "review": "Terrible experience, very disappointed.", "authId": "user_35", "datetime": "2025-03-06T15:09:01.869Z", "sentiment": null}%n
2025-04-12 18:36:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:48 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-65
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:48 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:48 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-65] Instantiated an idempotent producer.
2025-04-12 18:36:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483008690
2025-04-12 18:36:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:48 [kafka-producer-network-thread | producer-65] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-65] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_35', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:48 [kafka-producer-network-thread | producer-65] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-65] ProducerId set to 8161 with epoch 0
2025-04-12 18:36:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:48 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_33, value: {"_id": {"$oid": "67f10b97558ab0f6396b142b"}, "review": "Terrible experience, very disappointed.", "authId": "user_33", "datetime": "2024-12-24T07:32:50.779Z", "sentiment": null}, offset: 70414
2025-04-12 18:36:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_33
2025-04-12 18:36:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142b"}, "review": "Terrible experience, very disappointed.", "authId": "user_33", "datetime": "2024-12-24T07:32:50.779Z", "sentiment": null}%n
2025-04-12 18:36:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:50 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-66
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:50 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:50 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-66] Instantiated an idempotent producer.
2025-04-12 18:36:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483010201
2025-04-12 18:36:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:50 [kafka-producer-network-thread | producer-66] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-66] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_33', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:50 [kafka-producer-network-thread | producer-66] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-66] ProducerId set to 8162 with epoch 0
2025-04-12 18:36:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:50 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_46, value: {"_id": {"$oid": "67f10b97558ab0f6396b1438"}, "review": "Not bad, not great either.", "authId": "user_46", "datetime": "2025-03-08T03:51:19.059Z", "sentiment": null}, offset: 70415
2025-04-12 18:36:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_46
2025-04-12 18:36:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1438"}, "review": "Not bad, not great either.", "authId": "user_46", "datetime": "2025-03-08T03:51:19.059Z", "sentiment": null}%n
2025-04-12 18:36:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:36:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:36:51 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-67
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:51 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:51 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-67] Instantiated an idempotent producer.
2025-04-12 18:36:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483011721
2025-04-12 18:36:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_46', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:36:51 [kafka-producer-network-thread | producer-67] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-67] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:51 [kafka-producer-network-thread | producer-67] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-67] ProducerId set to 8163 with epoch 0
2025-04-12 18:36:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:51 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 70416
2025-04-12 18:36:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-12 18:36:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-12 18:36:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:36:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:36:53 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-68
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:53 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:53 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-68] Instantiated an idempotent producer.
2025-04-12 18:36:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483013281
2025-04-12 18:36:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:36:53 [kafka-producer-network-thread | producer-68] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-68] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:53 [kafka-producer-network-thread | producer-68] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-68] ProducerId set to 8164 with epoch 0
2025-04-12 18:36:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:53 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_25, value: {"_id": {"$oid": "67f10b97558ab0f6396b1423"}, "review": "Serves its purpose, but nothing more.", "authId": "user_25", "datetime": "2025-03-27T11:39:09.683Z", "sentiment": null}, offset: 70417
2025-04-12 18:36:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_25
2025-04-12 18:36:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1423"}, "review": "Serves its purpose, but nothing more.", "authId": "user_25", "datetime": "2025-03-27T11:39:09.683Z", "sentiment": null}%n
2025-04-12 18:36:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-12 18:36:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-12 18:36:54 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-69
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:54 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:54 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-69] Instantiated an idempotent producer.
2025-04-12 18:36:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483014803
2025-04-12 18:36:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_25', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-12 18:36:54 [kafka-producer-network-thread | producer-69] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-69] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:54 [kafka-producer-network-thread | producer-69] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-69] ProducerId set to 8165 with epoch 0
2025-04-12 18:36:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:54 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_14, value: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}, offset: 70418
2025-04-12 18:36:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_14
2025-04-12 18:36:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}%n
2025-04-12 18:36:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:36:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:36:56 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-70
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:56 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:56 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-70] Instantiated an idempotent producer.
2025-04-12 18:36:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483016359
2025-04-12 18:36:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:56 [kafka-producer-network-thread | producer-70] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-70] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_14', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:36:56 [kafka-producer-network-thread | producer-70] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-70] ProducerId set to 8166 with epoch 0
2025-04-12 18:36:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:56 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_39, value: {"_id": {"$oid": "67f10b97558ab0f6396b1431"}, "review": "Some features are good, others not so much.", "authId": "user_39", "datetime": "2025-01-13T19:31:50.928Z", "sentiment": null}, offset: 70419
2025-04-12 18:36:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_39
2025-04-12 18:36:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1431"}, "review": "Some features are good, others not so much.", "authId": "user_39", "datetime": "2025-01-13T19:31:50.928Z", "sentiment": null}%n
2025-04-12 18:36:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:36:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:36:57 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-71
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:57 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:57 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-71] Instantiated an idempotent producer.
2025-04-12 18:36:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483017906
2025-04-12 18:36:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_39', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:36:57 [kafka-producer-network-thread | producer-71] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-71] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:57 [kafka-producer-network-thread | producer-71] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-71] ProducerId set to 8167 with epoch 0
2025-04-12 18:36:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:57 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_13, value: {"_id": {"$oid": "67f10b97558ab0f6396b1417"}, "review": "It broke after a week of use.", "authId": "user_13", "datetime": "2024-12-05T23:56:15.604Z", "sentiment": null}, offset: 70420
2025-04-12 18:36:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_13
2025-04-12 18:36:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1417"}, "review": "It broke after a week of use.", "authId": "user_13", "datetime": "2024-12-05T23:56:15.604Z", "sentiment": null}%n
2025-04-12 18:36:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-12 18:36:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-12 18:36:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-72
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:36:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:36:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-72] Instantiated an idempotent producer.
2025-04-12 18:36:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:36:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:36:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483018430
2025-04-12 18:36:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:36:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_13', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-12 18:36:58 [kafka-producer-network-thread | producer-72] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-72] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:36:58 [kafka-producer-network-thread | producer-72] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-72] ProducerId set to 8168 with epoch 0
2025-04-12 18:36:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:36:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_83, value: {"_id": {"$oid": "67f10b97558ab0f6396b145d"}, "review": "Fantastic performance and very durable.", "authId": "user_83", "datetime": "2025-01-10T22:14:10.352Z", "sentiment": null}, offset: 70421
2025-04-12 18:36:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_83
2025-04-12 18:37:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145d"}, "review": "Fantastic performance and very durable.", "authId": "user_83", "datetime": "2025-01-10T22:14:10.352Z", "sentiment": null}%n
2025-04-12 18:37:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 18:37:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 18:37:00 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-73
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:00 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:00 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-73] Instantiated an idempotent producer.
2025-04-12 18:37:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483020018
2025-04-12 18:37:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_83', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 18:37:00 [kafka-producer-network-thread | producer-73] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-73] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:00 [kafka-producer-network-thread | producer-73] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-73] ProducerId set to 8169 with epoch 0
2025-04-12 18:37:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:00 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_26, value: {"_id": {"$oid": "67f10b97558ab0f6396b1424"}, "review": "Doesn't work as advertised.", "authId": "user_26", "datetime": "2025-02-15T00:52:47.341Z", "sentiment": null}, offset: 70422
2025-04-12 18:37:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_26
2025-04-12 18:37:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1424"}, "review": "Doesn't work as advertised.", "authId": "user_26", "datetime": "2025-02-15T00:52:47.341Z", "sentiment": null}%n
2025-04-12 18:37:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-74
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-74] Instantiated an idempotent producer.
2025-04-12 18:37:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483021574
2025-04-12 18:37:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_26', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:01 [kafka-producer-network-thread | producer-74] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-74] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:01 [kafka-producer-network-thread | producer-74] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-74] ProducerId set to 8170 with epoch 0
2025-04-12 18:37:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:01 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_82, value: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}, offset: 70423
2025-04-12 18:37:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_82
2025-04-12 18:37:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}%n
2025-04-12 18:37:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:03 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-75
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:03 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:03 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-75] Instantiated an idempotent producer.
2025-04-12 18:37:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483023108
2025-04-12 18:37:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:03 [kafka-producer-network-thread | producer-75] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-75] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_82', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:03 [kafka-producer-network-thread | producer-75] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-75] ProducerId set to 8171 with epoch 0
2025-04-12 18:37:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:03 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_34, value: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}, offset: 70424
2025-04-12 18:37:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_34
2025-04-12 18:37:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}%n
2025-04-12 18:37:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:37:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:37:04 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-76
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:04 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:04 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-76] Instantiated an idempotent producer.
2025-04-12 18:37:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483024659
2025-04-12 18:37:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:04 [kafka-producer-network-thread | producer-76] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-76] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_34', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:37:04 [kafka-producer-network-thread | producer-76] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-76] ProducerId set to 8172 with epoch 0
2025-04-12 18:37:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:04 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70425
2025-04-12 18:37:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-12 18:37:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}%n
2025-04-12 18:37:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:37:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:37:06 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-77
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:06 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:06 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-77] Instantiated an idempotent producer.
2025-04-12 18:37:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483026177
2025-04-12 18:37:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_50', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:37:06 [kafka-producer-network-thread | producer-77] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-77] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:06 [kafka-producer-network-thread | producer-77] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-77] ProducerId set to 8173 with epoch 0
2025-04-12 18:37:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:06 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_9, value: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}, offset: 70426
2025-04-12 18:37:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_9
2025-04-12 18:37:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}%n
2025-04-12 18:37:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-12 18:37:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-12 18:37:07 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-78
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:07 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:07 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-78] Instantiated an idempotent producer.
2025-04-12 18:37:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483027711
2025-04-12 18:37:07 [kafka-producer-network-thread | producer-78] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-78] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:07 [kafka-producer-network-thread | producer-78] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-78] ProducerId set to 8174 with epoch 0
2025-04-12 18:37:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_9', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-12 18:37:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:07 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_80, value: {"_id": {"$oid": "67f10b97558ab0f6396b145a"}, "review": "Excellent! Totally worth the price.", "authId": "user_80", "datetime": "2024-12-05T07:49:50.162Z", "sentiment": null}, offset: 70427
2025-04-12 18:37:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_80
2025-04-12 18:37:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145a"}, "review": "Excellent! Totally worth the price.", "authId": "user_80", "datetime": "2024-12-05T07:49:50.162Z", "sentiment": null}%n
2025-04-12 18:37:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.4
score: 0.7

2025-04-12 18:37:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.4
score: 0.7

2025-04-12 18:37:09 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-79
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:09 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:09 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-79] Instantiated an idempotent producer.
2025-04-12 18:37:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483029221
2025-04-12 18:37:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:09 [kafka-producer-network-thread | producer-79] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-79] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_80', sentiment=magnitude: 1.4
score: 0.7
}
2025-04-12 18:37:09 [kafka-producer-network-thread | producer-79] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-79] ProducerId set to 8175 with epoch 0
2025-04-12 18:37:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:09 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_10, value: {"_id": {"$oid": "67f10b97558ab0f6396b1414"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_10", "datetime": "2025-03-16T01:58:57.821Z", "sentiment": null}, offset: 70428
2025-04-12 18:37:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_10
2025-04-12 18:37:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1414"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_10", "datetime": "2025-03-16T01:58:57.821Z", "sentiment": null}%n
2025-04-12 18:37:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.4
score: 0.7

2025-04-12 18:37:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.4
score: 0.7

2025-04-12 18:37:10 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-80
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:10 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:10 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-80] Instantiated an idempotent producer.
2025-04-12 18:37:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483030748
2025-04-12 18:37:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:10 [kafka-producer-network-thread | producer-80] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-80] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_10', sentiment=magnitude: 1.4
score: 0.7
}
2025-04-12 18:37:10 [kafka-producer-network-thread | producer-80] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-80] ProducerId set to 8176 with epoch 0
2025-04-12 18:37:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:10 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_18, value: {"_id": {"$oid": "67f10b97558ab0f6396b141c"}, "review": "Doesn't work as advertised.", "authId": "user_18", "datetime": "2025-02-20T09:53:18.980Z", "sentiment": null}, offset: 70429
2025-04-12 18:37:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_18
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141c"}, "review": "Doesn't work as advertised.", "authId": "user_18", "datetime": "2025-02-20T09:53:18.980Z", "sentiment": null}%n
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-81
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:12 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-81] Instantiated an idempotent producer.
2025-04-12 18:37:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483032300
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:12 [kafka-producer-network-thread | producer-81] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-81] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_18', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:12 [kafka-producer-network-thread | producer-81] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-81] ProducerId set to 8177 with epoch 0
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_9, value: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}, offset: 70430
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_9
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}%n
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-12 18:37:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-82
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:12 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-82] Instantiated an idempotent producer.
2025-04-12 18:37:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483032866
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_9', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-12 18:37:12 [kafka-producer-network-thread | producer-82] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-82] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:12 [kafka-producer-network-thread | producer-82] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-82] ProducerId set to 8178 with epoch 0
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_11, value: {"_id": {"$oid": "67f10b97558ab0f6396b1415"}, "review": "Would not recommend to anyone.", "authId": "user_11", "datetime": "2024-12-10T23:20:11.570Z", "sentiment": null}, offset: 70431
2025-04-12 18:37:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_11
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1415"}, "review": "Would not recommend to anyone.", "authId": "user_11", "datetime": "2024-12-10T23:20:11.570Z", "sentiment": null}%n
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:14 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-83
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:14 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:14 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-83] Instantiated an idempotent producer.
2025-04-12 18:37:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483034361
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_11', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:14 [kafka-producer-network-thread | producer-83] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-83] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:14 [kafka-producer-network-thread | producer-83] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-83] ProducerId set to 8179 with epoch 0
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_93, value: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}, offset: 70432
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_93
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}%n
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:14 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-84
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:14 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:14 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-84] Instantiated an idempotent producer.
2025-04-12 18:37:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483034915
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_93', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:14 [kafka-producer-network-thread | producer-84] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-84] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:14 [kafka-producer-network-thread | producer-84] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-84] ProducerId set to 8180 with epoch 0
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_81, value: {"_id": {"$oid": "67f10b97558ab0f6396b145b"}, "review": "Not bad, not great either.", "authId": "user_81", "datetime": "2025-03-11T12:19:55.240Z", "sentiment": null}, offset: 70433
2025-04-12 18:37:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_81
2025-04-12 18:37:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145b"}, "review": "Not bad, not great either.", "authId": "user_81", "datetime": "2025-03-11T12:19:55.240Z", "sentiment": null}%n
2025-04-12 18:37:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:37:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:37:16 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-85
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:16 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:16 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-85] Instantiated an idempotent producer.
2025-04-12 18:37:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483036433
2025-04-12 18:37:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_81', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:37:16 [kafka-producer-network-thread | producer-85] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-85] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:16 [kafka-producer-network-thread | producer-85] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-85] ProducerId set to 8181 with epoch 0
2025-04-12 18:37:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:16 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_59, value: {"_id": {"$oid": "67f10b97558ab0f6396b1445"}, "review": "It's okay, does the job.", "authId": "user_59", "datetime": "2024-12-03T13:32:51.362Z", "sentiment": null}, offset: 70434
2025-04-12 18:37:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_59
2025-04-12 18:37:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1445"}, "review": "It's okay, does the job.", "authId": "user_59", "datetime": "2024-12-03T13:32:51.362Z", "sentiment": null}%n
2025-04-12 18:37:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-12 18:37:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-12 18:37:17 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-86
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:17 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:17 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-86] Instantiated an idempotent producer.
2025-04-12 18:37:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483037935
2025-04-12 18:37:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_59', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-12 18:37:17 [kafka-producer-network-thread | producer-86] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-86] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:17 [kafka-producer-network-thread | producer-86] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-86] ProducerId set to 8182 with epoch 0
2025-04-12 18:37:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:17 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70435
2025-04-12 18:37:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-12 18:37:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}%n
2025-04-12 18:37:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:37:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:37:19 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-87
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:19 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:19 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-87] Instantiated an idempotent producer.
2025-04-12 18:37:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483039443
2025-04-12 18:37:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_50', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:37:19 [kafka-producer-network-thread | producer-87] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-87] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:19 [kafka-producer-network-thread | producer-87] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-87] ProducerId set to 8183 with epoch 0
2025-04-12 18:37:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:19 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_76, value: {"_id": {"$oid": "67f10b97558ab0f6396b1456"}, "review": "Great quality, will buy again.", "authId": "user_76", "datetime": "2025-01-27T21:25:36.715Z", "sentiment": null}, offset: 70436
2025-04-12 18:37:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_76
2025-04-12 18:37:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1456"}, "review": "Great quality, will buy again.", "authId": "user_76", "datetime": "2025-01-27T21:25:36.715Z", "sentiment": null}%n
2025-04-12 18:37:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: 0.1

2025-04-12 18:37:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: 0.1

2025-04-12 18:37:20 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-88
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:20 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:20 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-88] Instantiated an idempotent producer.
2025-04-12 18:37:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483040938
2025-04-12 18:37:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:20 [kafka-producer-network-thread | producer-88] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-88] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_76', sentiment=magnitude: 0.1
score: 0.1
}
2025-04-12 18:37:20 [kafka-producer-network-thread | producer-88] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-88] ProducerId set to 8184 with epoch 0
2025-04-12 18:37:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:20 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70437
2025-04-12 18:37:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-12 18:37:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}%n
2025-04-12 18:37:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:37:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:37:21 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-89
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:21 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:21 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-89] Instantiated an idempotent producer.
2025-04-12 18:37:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483041466
2025-04-12 18:37:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:21 [kafka-producer-network-thread | producer-89] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-89] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_50', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:37:21 [kafka-producer-network-thread | producer-89] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-89] ProducerId set to 8185 with epoch 0
2025-04-12 18:37:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:21 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_13, value: {"_id": {"$oid": "67f10b97558ab0f6396b1417"}, "review": "It broke after a week of use.", "authId": "user_13", "datetime": "2024-12-05T23:56:15.604Z", "sentiment": null}, offset: 70438
2025-04-12 18:37:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_13
2025-04-12 18:37:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1417"}, "review": "It broke after a week of use.", "authId": "user_13", "datetime": "2024-12-05T23:56:15.604Z", "sentiment": null}%n
2025-04-12 18:37:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-12 18:37:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-12 18:37:22 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-90
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:22 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:22 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-90] Instantiated an idempotent producer.
2025-04-12 18:37:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483042991
2025-04-12 18:37:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:22 [kafka-producer-network-thread | producer-90] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-90] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_13', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-12 18:37:22 [kafka-producer-network-thread | producer-90] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-90] ProducerId set to 8186 with epoch 0
2025-04-12 18:37:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:23 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_80, value: {"_id": {"$oid": "67f10b97558ab0f6396b145a"}, "review": "Excellent! Totally worth the price.", "authId": "user_80", "datetime": "2024-12-05T07:49:50.162Z", "sentiment": null}, offset: 70439
2025-04-12 18:37:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_80
2025-04-12 18:37:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145a"}, "review": "Excellent! Totally worth the price.", "authId": "user_80", "datetime": "2024-12-05T07:49:50.162Z", "sentiment": null}%n
2025-04-12 18:37:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.4
score: 0.7

2025-04-12 18:37:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.4
score: 0.7

2025-04-12 18:37:24 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-91
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:24 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:24 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-91] Instantiated an idempotent producer.
2025-04-12 18:37:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483044531
2025-04-12 18:37:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_80', sentiment=magnitude: 1.4
score: 0.7
}
2025-04-12 18:37:24 [kafka-producer-network-thread | producer-91] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-91] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:24 [kafka-producer-network-thread | producer-91] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-91] ProducerId set to 8187 with epoch 0
2025-04-12 18:37:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:24 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_33, value: {"_id": {"$oid": "67f10b97558ab0f6396b142b"}, "review": "Terrible experience, very disappointed.", "authId": "user_33", "datetime": "2024-12-24T07:32:50.779Z", "sentiment": null}, offset: 70440
2025-04-12 18:37:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_33
2025-04-12 18:37:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142b"}, "review": "Terrible experience, very disappointed.", "authId": "user_33", "datetime": "2024-12-24T07:32:50.779Z", "sentiment": null}%n
2025-04-12 18:37:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-92
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-92] Instantiated an idempotent producer.
2025-04-12 18:37:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483046051
2025-04-12 18:37:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:26 [kafka-producer-network-thread | producer-92] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-92] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_33', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:26 [kafka-producer-network-thread | producer-92] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-92] ProducerId set to 8188 with epoch 0
2025-04-12 18:37:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:26 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_45, value: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}, offset: 70441
2025-04-12 18:37:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_45
2025-04-12 18:37:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}%n
2025-04-12 18:37:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:27 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-93
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:27 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:27 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-93] Instantiated an idempotent producer.
2025-04-12 18:37:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483047560
2025-04-12 18:37:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:27 [kafka-producer-network-thread | producer-93] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-93] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_45', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:27 [kafka-producer-network-thread | producer-93] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-93] ProducerId set to 8189 with epoch 0
2025-04-12 18:37:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:27 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_12, value: {"_id": {"$oid": "67f10b97558ab0f6396b1416"}, "review": "Not satisfied at all.", "authId": "user_12", "datetime": "2025-01-13T23:31:43.542Z", "sentiment": null}, offset: 70442
2025-04-12 18:37:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_12
2025-04-12 18:37:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1416"}, "review": "Not satisfied at all.", "authId": "user_12", "datetime": "2025-01-13T23:31:43.542Z", "sentiment": null}%n
2025-04-12 18:37:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:29 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-94
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:29 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:29 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-94] Instantiated an idempotent producer.
2025-04-12 18:37:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483049078
2025-04-12 18:37:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:29 [kafka-producer-network-thread | producer-94] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-94] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_12', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:29 [kafka-producer-network-thread | producer-94] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-94] ProducerId set to 8190 with epoch 0
2025-04-12 18:37:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:29 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_1, value: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}, offset: 70443
2025-04-12 18:37:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_1
2025-04-12 18:37:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}%n
2025-04-12 18:37:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:30 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-95
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:30 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:30 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-95] Instantiated an idempotent producer.
2025-04-12 18:37:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483050609
2025-04-12 18:37:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_1', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:30 [kafka-producer-network-thread | producer-95] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-95] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:30 [kafka-producer-network-thread | producer-95] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-95] ProducerId set to 8191 with epoch 0
2025-04-12 18:37:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:30 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_49, value: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "Its alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}, offset: 70444
2025-04-12 18:37:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_49
2025-04-12 18:37:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "Its alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}%n
2025-04-12 18:37:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-12 18:37:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-12 18:37:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-96
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-96] Instantiated an idempotent producer.
2025-04-12 18:37:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483052145
2025-04-12 18:37:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:32 [kafka-producer-network-thread | producer-96] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-96] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_49', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-12 18:37:32 [kafka-producer-network-thread | producer-96] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-96] ProducerId set to 8192 with epoch 0
2025-04-12 18:37:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_77, value: {"_id": {"$oid": "67f10b97558ab0f6396b1457"}, "review": "Smooth and easy to use.", "authId": "user_77", "datetime": "2025-01-05T21:10:33.619Z", "sentiment": null}, offset: 70445
2025-04-12 18:37:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_77
2025-04-12 18:37:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1457"}, "review": "Smooth and easy to use.", "authId": "user_77", "datetime": "2025-01-05T21:10:33.619Z", "sentiment": null}%n
2025-04-12 18:37:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = 
2025-04-12 18:37:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data 
2025-04-12 18:37:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-97
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-97] Instantiated an idempotent producer.
2025-04-12 18:37:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483053680
2025-04-12 18:37:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:33 [kafka-producer-network-thread | producer-97] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-97] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_77', sentiment=}
2025-04-12 18:37:33 [kafka-producer-network-thread | producer-97] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-97] ProducerId set to 8193 with epoch 0
2025-04-12 18:37:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_88, value: {"_id": {"$oid": "67f10b97558ab0f6396b1462"}, "review": "Decent, but not amazing.", "authId": "user_88", "datetime": "2025-03-21T21:19:57.894Z", "sentiment": null}, offset: 70446
2025-04-12 18:37:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_88
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1462"}, "review": "Decent, but not amazing.", "authId": "user_88", "datetime": "2025-03-21T21:19:57.894Z", "sentiment": null}%n
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:37:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-98
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-98] Instantiated an idempotent producer.
2025-04-12 18:37:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483055201
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_88', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:37:35 [kafka-producer-network-thread | producer-98] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-98] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:35 [kafka-producer-network-thread | producer-98] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-98] ProducerId set to 8194 with epoch 0
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_63, value: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}, offset: 70447
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_63
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}%n
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 18:37:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-99
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-99] Instantiated an idempotent producer.
2025-04-12 18:37:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483055781
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:35 [kafka-producer-network-thread | producer-99] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-99] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_63', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 18:37:35 [kafka-producer-network-thread | producer-99] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-99] ProducerId set to 8195 with epoch 0
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_10, value: {"_id": {"$oid": "67f10b97558ab0f6396b1414"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_10", "datetime": "2025-03-16T01:58:57.821Z", "sentiment": null}, offset: 70448
2025-04-12 18:37:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_10
2025-04-12 18:37:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1414"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_10", "datetime": "2025-03-16T01:58:57.821Z", "sentiment": null}%n
2025-04-12 18:37:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.4
score: 0.7

2025-04-12 18:37:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.4
score: 0.7

2025-04-12 18:37:37 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-100
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:37 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:37 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-100] Instantiated an idempotent producer.
2025-04-12 18:37:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483057430
2025-04-12 18:37:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:37 [kafka-producer-network-thread | producer-100] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-100] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_10', sentiment=magnitude: 1.4
score: 0.7
}
2025-04-12 18:37:37 [kafka-producer-network-thread | producer-100] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-100] ProducerId set to 8196 with epoch 0
2025-04-12 18:37:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70449
2025-04-12 18:37:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-12 18:37:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}%n
2025-04-12 18:37:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:37:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:37:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-101
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-101] Instantiated an idempotent producer.
2025-04-12 18:37:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483059950
2025-04-12 18:37:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_50', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:37:39 [kafka-producer-network-thread | producer-101] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-101] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:39 [kafka-producer-network-thread | producer-101] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-101] ProducerId set to 8197 with epoch 0
2025-04-12 18:37:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:39 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_93, value: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}, offset: 70450
2025-04-12 18:37:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_93
2025-04-12 18:37:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}%n
2025-04-12 18:37:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:41 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-102
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:41 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:41 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-102] Instantiated an idempotent producer.
2025-04-12 18:37:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483061497
2025-04-12 18:37:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_93', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:41 [kafka-producer-network-thread | producer-102] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-102] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:41 [kafka-producer-network-thread | producer-102] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-102] ProducerId set to 8198 with epoch 0
2025-04-12 18:37:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:41 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_21, value: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}, offset: 70451
2025-04-12 18:37:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_21
2025-04-12 18:37:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}%n
2025-04-12 18:37:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:37:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:37:43 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-103
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:43 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:43 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-103] Instantiated an idempotent producer.
2025-04-12 18:37:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483063015
2025-04-12 18:37:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_21', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:37:43 [kafka-producer-network-thread | producer-103] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-103] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:43 [kafka-producer-network-thread | producer-103] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-103] ProducerId set to 8199 with epoch 0
2025-04-12 18:37:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:43 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_53, value: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}, offset: 70452
2025-04-12 18:37:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_53
2025-04-12 18:37:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}%n
2025-04-12 18:37:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-12 18:37:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-12 18:37:44 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-104
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:44 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:44 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-104] Instantiated an idempotent producer.
2025-04-12 18:37:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483064534
2025-04-12 18:37:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:44 [kafka-producer-network-thread | producer-104] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-104] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_53', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-12 18:37:44 [kafka-producer-network-thread | producer-104] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-104] ProducerId set to 8200 with epoch 0
2025-04-12 18:37:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_5, value: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}, offset: 70453
2025-04-12 18:37:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_5
2025-04-12 18:37:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}%n
2025-04-12 18:37:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:37:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:37:46 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-105
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:46 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:46 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-105] Instantiated an idempotent producer.
2025-04-12 18:37:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483066044
2025-04-12 18:37:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_5', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:37:46 [kafka-producer-network-thread | producer-105] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-105] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:46 [kafka-producer-network-thread | producer-105] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-105] ProducerId set to 8201 with epoch 0
2025-04-12 18:37:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:46 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_95, value: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}, offset: 70454
2025-04-12 18:37:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_95
2025-04-12 18:37:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}%n
2025-04-12 18:37:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:47 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-106
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:47 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:47 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-106] Instantiated an idempotent producer.
2025-04-12 18:37:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483067514
2025-04-12 18:37:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_95', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:47 [kafka-producer-network-thread | producer-106] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-106] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:47 [kafka-producer-network-thread | producer-106] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-106] ProducerId set to 8202 with epoch 0
2025-04-12 18:37:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:47 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_16, value: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}, offset: 70455
2025-04-12 18:37:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_16
2025-04-12 18:37:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}%n
2025-04-12 18:37:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:37:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:37:48 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-107
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:48 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:48 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-107] Instantiated an idempotent producer.
2025-04-12 18:37:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483068999
2025-04-12 18:37:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_16', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:37:49 [kafka-producer-network-thread | producer-107] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-107] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:49 [kafka-producer-network-thread | producer-107] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-107] ProducerId set to 8203 with epoch 0
2025-04-12 18:37:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:49 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 70456
2025-04-12 18:37:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-12 18:37:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}%n
2025-04-12 18:37:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:37:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:37:50 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-108
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:50 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:50 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-108] Instantiated an idempotent producer.
2025-04-12 18:37:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483070522
2025-04-12 18:37:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_91', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:37:50 [kafka-producer-network-thread | producer-108] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-108] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:50 [kafka-producer-network-thread | producer-108] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-108] ProducerId set to 8204 with epoch 0
2025-04-12 18:37:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:50 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_14, value: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}, offset: 70457
2025-04-12 18:37:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_14
2025-04-12 18:37:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}%n
2025-04-12 18:37:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:37:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:37:52 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-109
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:52 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:52 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-109] Instantiated an idempotent producer.
2025-04-12 18:37:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483072074
2025-04-12 18:37:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_14', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:37:52 [kafka-producer-network-thread | producer-109] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-109] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:52 [kafka-producer-network-thread | producer-109] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-109] ProducerId set to 8205 with epoch 0
2025-04-12 18:37:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:52 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_61, value: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}, offset: 70458
2025-04-12 18:37:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_61
2025-04-12 18:37:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}%n
2025-04-12 18:37:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:53 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-110
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:53 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:53 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-110] Instantiated an idempotent producer.
2025-04-12 18:37:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483073595
2025-04-12 18:37:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_61', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:53 [kafka-producer-network-thread | producer-110] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-110] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:53 [kafka-producer-network-thread | producer-110] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-110] ProducerId set to 8206 with epoch 0
2025-04-12 18:37:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:53 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_20, value: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}, offset: 70459
2025-04-12 18:37:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_20
2025-04-12 18:37:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}%n
2025-04-12 18:37:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-12 18:37:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-12 18:37:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-111
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-111] Instantiated an idempotent producer.
2025-04-12 18:37:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483075094
2025-04-12 18:37:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_20', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-12 18:37:55 [kafka-producer-network-thread | producer-111] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-111] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:55 [kafka-producer-network-thread | producer-111] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-111] ProducerId set to 8207 with epoch 0
2025-04-12 18:37:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_60, value: {"_id": {"$oid": "67f10b97558ab0f6396b1446"}, "review": "Not satisfied at all.", "authId": "user_60", "datetime": "2025-03-10T13:21:03.219Z", "sentiment": null}, offset: 70460
2025-04-12 18:37:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_60
2025-04-12 18:37:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1446"}, "review": "Not satisfied at all.", "authId": "user_60", "datetime": "2025-03-10T13:21:03.219Z", "sentiment": null}%n
2025-04-12 18:37:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:56 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-112
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:56 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:56 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-112] Instantiated an idempotent producer.
2025-04-12 18:37:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483076627
2025-04-12 18:37:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_60', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:56 [kafka-producer-network-thread | producer-112] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-112] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:56 [kafka-producer-network-thread | producer-112] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-112] ProducerId set to 8208 with epoch 0
2025-04-12 18:37:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:56 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_82, value: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}, offset: 70461
2025-04-12 18:37:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_82
2025-04-12 18:37:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}%n
2025-04-12 18:37:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:37:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:37:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-113
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-113] Instantiated an idempotent producer.
2025-04-12 18:37:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483078129
2025-04-12 18:37:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_82', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:37:58 [kafka-producer-network-thread | producer-113] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-113] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:58 [kafka-producer-network-thread | producer-113] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-113] ProducerId set to 8209 with epoch 0
2025-04-12 18:37:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_30, value: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}, offset: 70462
2025-04-12 18:37:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_30
2025-04-12 18:37:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}%n
2025-04-12 18:37:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:37:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:37:59 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-114
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:37:59 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:37:59 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-114] Instantiated an idempotent producer.
2025-04-12 18:37:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:37:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:37:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483079647
2025-04-12 18:37:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:37:59 [kafka-producer-network-thread | producer-114] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-114] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:37:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_30', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:37:59 [kafka-producer-network-thread | producer-114] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-114] ProducerId set to 8210 with epoch 0
2025-04-12 18:37:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:37:59 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_83, value: {"_id": {"$oid": "67f10b97558ab0f6396b145d"}, "review": "Fantastic performance and very durable.", "authId": "user_83", "datetime": "2025-01-10T22:14:10.352Z", "sentiment": null}, offset: 70463
2025-04-12 18:37:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_83
2025-04-12 18:38:00 [Thread-6] INFO  o.s.t.kafka.consumer.ConsumerManager - Shutting down consumers...
2025-04-12 18:38:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145d"}, "review": "Fantastic performance and very durable.", "authId": "user_83", "datetime": "2025-01-10T22:14:10.352Z", "sentiment": null}%n
2025-04-12 18:38:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 18:38:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 18:38:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-115
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-115] Instantiated an idempotent producer.
2025-04-12 18:38:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483081148
2025-04-12 18:38:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:01 [kafka-producer-network-thread | producer-115] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-115] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_83', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 18:38:01 [kafka-producer-network-thread | producer-115] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-115] ProducerId set to 8211 with epoch 0
2025-04-12 18:38:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:01 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_78, value: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}, offset: 70464
2025-04-12 18:38:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_78
2025-04-12 18:38:01 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:01 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:01 [Worker-2] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:01 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Member consumer-contacts-test-group-2-5fbccb31-aadd-4938-aefc-39e8da5797e2 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 18:38:01 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 18:38:01 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 18:38:01 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 18:38:01 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 18:38:01 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 18:38:01 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 18:38:01 [Worker-2] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-2 unregistered
2025-04-12 18:38:01 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 18:38:01 [Worker-4] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:01 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Member consumer-contacts-test-group-4-e333f896-36b8-4b40-9ce1-5eb23f1bb4f1 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 18:38:01 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 18:38:01 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 18:38:01 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 18:38:01 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 18:38:01 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 18:38:01 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 18:38:01 [Worker-4] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-4 unregistered
2025-04-12 18:38:01 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 18:38:01 [Worker-3] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:01 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Member consumer-contacts-test-group-3-19f9bbe2-5dc7-4e90-a4b3-e89874657593 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 18:38:01 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 18:38:01 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 18:38:01 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 18:38:01 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 18:38:01 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 18:38:01 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 18:38:01 [Worker-5] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:01 [Worker-3] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-3 unregistered
2025-04-12 18:38:01 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Member consumer-contacts-test-group-5-72b8d544-a6a3-4489-af52-681377f702f6 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 18:38:01 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 18:38:01 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 18:38:01 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 18:38:01 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 18:38:01 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 18:38:01 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 18:38:01 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 18:38:01 [Worker-5] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-5 unregistered
2025-04-12 18:38:01 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 18:38:01 [Worker-6] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:01 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Member consumer-contacts-test-group-6-790cca21-3c86-4e2b-bae9-2788b6b0983e sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 18:38:01 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 18:38:01 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 18:38:01 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 18:38:01 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 18:38:01 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 18:38:01 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 18:38:01 [Worker-6] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-6 unregistered
2025-04-12 18:38:01 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 18:38:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}%n
2025-04-12 18:38:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:38:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:38:02 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-116
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:02 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:02 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-116] Instantiated an idempotent producer.
2025-04-12 18:38:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483082669
2025-04-12 18:38:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:02 [kafka-producer-network-thread | producer-116] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-116] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_78', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:38:02 [kafka-producer-network-thread | producer-116] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-116] ProducerId set to 8212 with epoch 0
2025-04-12 18:38:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:02 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_9, value: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}, offset: 70465
2025-04-12 18:38:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_9
2025-04-12 18:38:02 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:02 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:03 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 18:38:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}%n
2025-04-12 18:38:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-12 18:38:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-12 18:38:04 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-117
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:04 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:04 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-117] Instantiated an idempotent producer.
2025-04-12 18:38:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483084224
2025-04-12 18:38:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_9', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-12 18:38:04 [kafka-producer-network-thread | producer-117] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-117] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:04 [kafka-producer-network-thread | producer-117] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-117] ProducerId set to 8213 with epoch 0
2025-04-12 18:38:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:04 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 70466
2025-04-12 18:38:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-12 18:38:04 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:04 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-12 18:38:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:38:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:38:05 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-118
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:05 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:05 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-118] Instantiated an idempotent producer.
2025-04-12 18:38:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483085758
2025-04-12 18:38:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:38:05 [kafka-producer-network-thread | producer-118] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-118] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:05 [kafka-producer-network-thread | producer-118] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-118] ProducerId set to 8214 with epoch 0
2025-04-12 18:38:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:05 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_94, value: {"_id": {"$oid": "67f10b97558ab0f6396b1468"}, "review": "Great quality, will buy again.", "authId": "user_94", "datetime": "2025-03-23T03:36:43.648Z", "sentiment": null}, offset: 70467
2025-04-12 18:38:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_94
2025-04-12 18:38:05 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:05 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:06 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 18:38:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1468"}, "review": "Great quality, will buy again.", "authId": "user_94", "datetime": "2025-03-23T03:36:43.648Z", "sentiment": null}%n
2025-04-12 18:38:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: 0.1

2025-04-12 18:38:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: 0.1

2025-04-12 18:38:07 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-119
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:07 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:07 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-119] Instantiated an idempotent producer.
2025-04-12 18:38:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483087395
2025-04-12 18:38:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_94', sentiment=magnitude: 0.1
score: 0.1
}
2025-04-12 18:38:07 [kafka-producer-network-thread | producer-119] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-119] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:07 [kafka-producer-network-thread | producer-119] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-119] ProducerId set to 8215 with epoch 0
2025-04-12 18:38:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:07 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_54, value: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}, offset: 70468
2025-04-12 18:38:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_54
2025-04-12 18:38:07 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:07 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}%n
2025-04-12 18:38:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-12 18:38:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-12 18:38:08 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-120
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:08 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:08 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-120] Instantiated an idempotent producer.
2025-04-12 18:38:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483088916
2025-04-12 18:38:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_54', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-12 18:38:08 [kafka-producer-network-thread | producer-120] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-120] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:08 [kafka-producer-network-thread | producer-120] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-120] ProducerId set to 8216 with epoch 0
2025-04-12 18:38:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:08 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_73, value: {"_id": {"$oid": "67f10b97558ab0f6396b1453"}, "review": "Not satisfied at all.", "authId": "user_73", "datetime": "2025-03-31T12:24:38.653Z", "sentiment": null}, offset: 70469
2025-04-12 18:38:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_73
2025-04-12 18:38:08 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:08 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:09 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1453"}, "review": "Not satisfied at all.", "authId": "user_73", "datetime": "2025-03-31T12:24:38.653Z", "sentiment": null}%n
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:38:09 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-121
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:09 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:09 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-121] Instantiated an idempotent producer.
2025-04-12 18:38:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483089437
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_73', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:38:09 [kafka-producer-network-thread | producer-121] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-121] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:09 [kafka-producer-network-thread | producer-121] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-121] ProducerId set to 8217 with epoch 0
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_41, value: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}, offset: 70470
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_41
2025-04-12 18:38:09 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:09 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}%n
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:38:09 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-122
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:09 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:09 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-122] Instantiated an idempotent producer.
2025-04-12 18:38:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483089961
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_41', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:38:09 [kafka-producer-network-thread | producer-122] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-122] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:09 [kafka-producer-network-thread | producer-122] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-122] ProducerId set to 8218 with epoch 0
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_68, value: {"_id": {"$oid": "67f10b97558ab0f6396b144e"}, "review": "Poor build quality, not reliable.", "authId": "user_68", "datetime": "2025-01-15T21:21:03.997Z", "sentiment": null}, offset: 70471
2025-04-12 18:38:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_68
2025-04-12 18:38:09 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:09 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144e"}, "review": "Poor build quality, not reliable.", "authId": "user_68", "datetime": "2025-01-15T21:21:03.997Z", "sentiment": null}%n
2025-04-12 18:38:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:38:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:38:11 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-123
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:11 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:11 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-123] Instantiated an idempotent producer.
2025-04-12 18:38:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483091504
2025-04-12 18:38:11 [kafka-producer-network-thread | producer-123] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-123] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:11 [kafka-producer-network-thread | producer-123] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-123] ProducerId set to 8219 with epoch 0
2025-04-12 18:38:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_68', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:38:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:11 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_92, value: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}, offset: 70472
2025-04-12 18:38:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_92
2025-04-12 18:38:11 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:11 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:12 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 18:38:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}%n
2025-04-12 18:38:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:38:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:38:13 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-124
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:13 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:13 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-124] Instantiated an idempotent producer.
2025-04-12 18:38:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483093048
2025-04-12 18:38:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:13 [kafka-producer-network-thread | producer-124] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-124] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_92', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:38:13 [kafka-producer-network-thread | producer-124] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-124] ProducerId set to 8220 with epoch 0
2025-04-12 18:38:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:13 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_61, value: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}, offset: 70473
2025-04-12 18:38:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_61
2025-04-12 18:38:13 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:13 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}%n
2025-04-12 18:38:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:38:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:38:14 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-125
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:14 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:14 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-125] Instantiated an idempotent producer.
2025-04-12 18:38:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483094575
2025-04-12 18:38:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:14 [kafka-producer-network-thread | producer-125] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-125] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_61', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:38:14 [kafka-producer-network-thread | producer-125] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-125] ProducerId set to 8221 with epoch 0
2025-04-12 18:38:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:14 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_36, value: {"_id": {"$oid": "67f10b97558ab0f6396b142e"}, "review": "Works like a charm, perfect!", "authId": "user_36", "datetime": "2024-12-07T01:28:05.981Z", "sentiment": null}, offset: 70474
2025-04-12 18:38:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_36
2025-04-12 18:38:14 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:14 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142e"}, "review": "Works like a charm, perfect!", "authId": "user_36", "datetime": "2024-12-07T01:28:05.981Z", "sentiment": null}%n
2025-04-12 18:38:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:38:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:38:15 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-126
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:15 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:15 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-126] Instantiated an idempotent producer.
2025-04-12 18:38:15 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 18:38:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483095116
2025-04-12 18:38:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_36', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:38:15 [kafka-producer-network-thread | producer-126] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-126] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:15 [kafka-producer-network-thread | producer-126] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-126] ProducerId set to 8222 with epoch 0
2025-04-12 18:38:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:15 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_99, value: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}, offset: 70475
2025-04-12 18:38:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_99
2025-04-12 18:38:15 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:15 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}%n
2025-04-12 18:38:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-12 18:38:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-12 18:38:16 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-127
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:16 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:16 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-127] Instantiated an idempotent producer.
2025-04-12 18:38:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483096636
2025-04-12 18:38:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_99', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-12 18:38:16 [kafka-producer-network-thread | producer-127] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-127] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:16 [kafka-producer-network-thread | producer-127] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-127] ProducerId set to 8223 with epoch 0
2025-04-12 18:38:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:16 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_34, value: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}, offset: 70476
2025-04-12 18:38:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_34
2025-04-12 18:38:16 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:16 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}%n
2025-04-12 18:38:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:38:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:38:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-128
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-128] Instantiated an idempotent producer.
2025-04-12 18:38:18 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 18:38:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483098130
2025-04-12 18:38:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_34', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:38:18 [kafka-producer-network-thread | producer-128] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-128] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:18 [kafka-producer-network-thread | producer-128] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-128] ProducerId set to 8224 with epoch 0
2025-04-12 18:38:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_21, value: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}, offset: 70477
2025-04-12 18:38:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_21
2025-04-12 18:38:18 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:18 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}%n
2025-04-12 18:38:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:38:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:38:19 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-129
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:38:19 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:38:19 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-129] Instantiated an idempotent producer.
2025-04-12 18:38:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:38:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:38:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483099618
2025-04-12 18:38:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:38:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_21', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:38:19 [kafka-producer-network-thread | producer-129] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-129] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:38:19 [kafka-producer-network-thread | producer-129] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-129] ProducerId set to 8225 with epoch 0
2025-04-12 18:38:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:38:19 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_95, value: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}, offset: 70478
2025-04-12 18:38:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_95
2025-04-12 18:38:19 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:38:19 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:48:41 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Starting 6 consumers for the 'contacts' topic...
2025-04-12 18:48:41 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:48:42 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483722586
2025-04-12 18:48:42 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:48:42 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:48:42 [Worker-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:48:42 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483722634
2025-04-12 18:48:42 [Worker-2] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:48:42 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:48:42 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:48:42 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483722662
2025-04-12 18:48:42 [Worker-3] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:48:42 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:48:42 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:48:42 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483722686
2025-04-12 18:48:42 [Worker-4] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:48:42 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:48:42 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:48:42 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483722707
2025-04-12 18:48:42 [Worker-5] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:48:42 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:48:42 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:48:42 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:48:42 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483722725
2025-04-12 18:48:42 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Started 6 consumers for the 'contacts' topic.
2025-04-12 18:48:42 [Worker-6] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:48:42 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:48:43 [Worker-3] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:48:43 [Worker-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:48:43 [Worker-4] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:48:43 [Worker-2] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:48:43 [Worker-6] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:48:43 [Worker-5] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:48:43 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:48:43 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:48:43 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:48:43 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:48:43 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:48:43 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:48:43 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:48:43 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:48:43 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:48:43 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:48:43 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-3-9b866bc5-4472-4f8f-8bcd-cabfdd8b0d1d
2025-04-12 18:48:43 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-1-dcee0245-9ad1-427e-ab1f-84d3f9a52079
2025-04-12 18:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-2-03b48651-3813-4bf4-9285-869c03edfca5
2025-04-12 18:48:43 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-5-b3c47e21-1657-467f-847e-0c57ce85ac2f
2025-04-12 18:48:43 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-6-a157c31b-581e-4b15-9d03-9045c98d2f8b
2025-04-12 18:48:43 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:48:43 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-4-eabfb956-1199-4022-aa7a-ca9e0fc5c1f7
2025-04-12 18:48:43 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:48:43 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:48:43 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:48:43 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:48:46 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=99, memberId='consumer-contacts-test-group-1-dcee0245-9ad1-427e-ab1f-84d3f9a52079', protocol='range'}
2025-04-12 18:48:46 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=99, memberId='consumer-contacts-test-group-2-03b48651-3813-4bf4-9285-869c03edfca5', protocol='range'}
2025-04-12 18:48:46 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=99, memberId='consumer-contacts-test-group-3-9b866bc5-4472-4f8f-8bcd-cabfdd8b0d1d', protocol='range'}
2025-04-12 18:48:46 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=99, memberId='consumer-contacts-test-group-4-eabfb956-1199-4022-aa7a-ca9e0fc5c1f7', protocol='range'}
2025-04-12 18:48:46 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Finished assignment for group at generation 99: {consumer-contacts-test-group-2-03b48651-3813-4bf4-9285-869c03edfca5=Assignment(partitions=[]), consumer-contacts-test-group-6-a157c31b-581e-4b15-9d03-9045c98d2f8b=Assignment(partitions=[]), consumer-contacts-test-group-4-eabfb956-1199-4022-aa7a-ca9e0fc5c1f7=Assignment(partitions=[]), consumer-contacts-test-group-5-b3c47e21-1657-467f-847e-0c57ce85ac2f=Assignment(partitions=[]), consumer-contacts-test-group-3-9b866bc5-4472-4f8f-8bcd-cabfdd8b0d1d=Assignment(partitions=[]), consumer-contacts-test-group-1-dcee0245-9ad1-427e-ab1f-84d3f9a52079=Assignment(partitions=[test-topic-0])}
2025-04-12 18:48:46 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=99, memberId='consumer-contacts-test-group-4-eabfb956-1199-4022-aa7a-ca9e0fc5c1f7', protocol='range'}
2025-04-12 18:48:46 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=99, memberId='consumer-contacts-test-group-3-9b866bc5-4472-4f8f-8bcd-cabfdd8b0d1d', protocol='range'}
2025-04-12 18:48:46 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=99, memberId='consumer-contacts-test-group-2-03b48651-3813-4bf4-9285-869c03edfca5', protocol='range'}
2025-04-12 18:48:46 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=99, memberId='consumer-contacts-test-group-1-dcee0245-9ad1-427e-ab1f-84d3f9a52079', protocol='range'}
2025-04-12 18:48:46 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=99, memberId='consumer-contacts-test-group-5-b3c47e21-1657-467f-847e-0c57ce85ac2f', protocol='range'}
2025-04-12 18:48:46 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:48:46 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:48:46 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-12 18:48:46 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:48:46 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=99, memberId='consumer-contacts-test-group-5-b3c47e21-1657-467f-847e-0c57ce85ac2f', protocol='range'}
2025-04-12 18:48:46 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:48:46 [Worker-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:48:46 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:48:46 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:48:46 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:48:46 [Worker-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-12 18:48:46 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=99, memberId='consumer-contacts-test-group-6-a157c31b-581e-4b15-9d03-9045c98d2f8b', protocol='range'}
2025-04-12 18:48:46 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=99, memberId='consumer-contacts-test-group-6-a157c31b-581e-4b15-9d03-9045c98d2f8b', protocol='range'}
2025-04-12 18:48:46 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:48:46 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:48:46 [Worker-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=70449, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=14}}
2025-04-12 18:48:46 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70449
2025-04-12 18:48:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-12 18:48:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}%n
2025-04-12 18:48:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:48:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:48:51 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:48:51 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:48:51 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-04-12 18:48:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:48:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:48:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483731509
2025-04-12 18:48:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:48:51 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:48:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_50', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:48:51 [kafka-producer-network-thread | producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 8226 with epoch 0
2025-04-12 18:48:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:48:51 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_93, value: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}, offset: 70450
2025-04-12 18:48:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_93
2025-04-12 18:48:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}%n
2025-04-12 18:48:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:48:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:48:53 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:48:53 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:48:53 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-2] Instantiated an idempotent producer.
2025-04-12 18:48:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:48:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:48:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483733513
2025-04-12 18:48:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:48:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_93', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:48:53 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-2] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:48:53 [kafka-producer-network-thread | producer-2] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-2] ProducerId set to 8227 with epoch 0
2025-04-12 18:48:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:48:53 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_21, value: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}, offset: 70451
2025-04-12 18:48:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_21
2025-04-12 18:48:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}%n
2025-04-12 18:48:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:48:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:48:54 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:48:54 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:48:54 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-3] Instantiated an idempotent producer.
2025-04-12 18:48:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:48:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:48:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483734109
2025-04-12 18:48:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:48:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_21', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:48:54 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-3] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:48:54 [kafka-producer-network-thread | producer-3] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-3] ProducerId set to 8228 with epoch 0
2025-04-12 18:48:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:48:54 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_53, value: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}, offset: 70452
2025-04-12 18:48:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_53
2025-04-12 18:48:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}%n
2025-04-12 18:48:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-12 18:48:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-12 18:48:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:48:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:48:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-4] Instantiated an idempotent producer.
2025-04-12 18:48:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:48:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:48:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483735627
2025-04-12 18:48:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:48:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_53', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-12 18:48:55 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-4] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:48:55 [kafka-producer-network-thread | producer-4] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-4] ProducerId set to 8229 with epoch 0
2025-04-12 18:48:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:48:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_5, value: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}, offset: 70453
2025-04-12 18:48:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_5
2025-04-12 18:48:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}%n
2025-04-12 18:48:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:48:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:48:57 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:48:57 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:48:57 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-5] Instantiated an idempotent producer.
2025-04-12 18:48:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:48:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:48:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483737128
2025-04-12 18:48:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:48:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_5', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:48:57 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-5] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:48:57 [kafka-producer-network-thread | producer-5] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-5] ProducerId set to 8230 with epoch 0
2025-04-12 18:48:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:48:57 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_95, value: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}, offset: 70454
2025-04-12 18:48:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_95
2025-04-12 18:48:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}%n
2025-04-12 18:48:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:48:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:48:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:48:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:48:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-6] Instantiated an idempotent producer.
2025-04-12 18:48:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:48:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:48:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483738717
2025-04-12 18:48:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:48:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_95', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:48:58 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-6] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:48:58 [kafka-producer-network-thread | producer-6] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-6] ProducerId set to 8231 with epoch 0
2025-04-12 18:48:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:48:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_16, value: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}, offset: 70455
2025-04-12 18:48:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_16
2025-04-12 18:49:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}%n
2025-04-12 18:49:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:49:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:49:00 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:00 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:00 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-7] Instantiated an idempotent producer.
2025-04-12 18:49:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483740261
2025-04-12 18:49:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_16', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:49:00 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-7] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:00 [kafka-producer-network-thread | producer-7] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-7] ProducerId set to 8232 with epoch 0
2025-04-12 18:49:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:00 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 70456
2025-04-12 18:49:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-12 18:49:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}%n
2025-04-12 18:49:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:49:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:49:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-8] Instantiated an idempotent producer.
2025-04-12 18:49:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483741793
2025-04-12 18:49:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_91', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:49:01 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-8] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:01 [kafka-producer-network-thread | producer-8] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-8] ProducerId set to 8233 with epoch 0
2025-04-12 18:49:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:01 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_14, value: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}, offset: 70457
2025-04-12 18:49:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_14
2025-04-12 18:49:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}%n
2025-04-12 18:49:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:49:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:49:03 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:03 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:03 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-9] Instantiated an idempotent producer.
2025-04-12 18:49:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483743342
2025-04-12 18:49:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_14', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:49:03 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-9] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:03 [kafka-producer-network-thread | producer-9] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-9] ProducerId set to 8234 with epoch 0
2025-04-12 18:49:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:03 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_61, value: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}, offset: 70458
2025-04-12 18:49:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_61
2025-04-12 18:49:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}%n
2025-04-12 18:49:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:49:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:49:04 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:04 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:04 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-10] Instantiated an idempotent producer.
2025-04-12 18:49:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483744845
2025-04-12 18:49:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_61', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:49:04 [kafka-producer-network-thread | producer-10] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-10] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:04 [kafka-producer-network-thread | producer-10] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-10] ProducerId set to 8235 with epoch 0
2025-04-12 18:49:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:04 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_20, value: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}, offset: 70459
2025-04-12 18:49:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_20
2025-04-12 18:49:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}%n
2025-04-12 18:49:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-12 18:49:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-12 18:49:05 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:05 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:05 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-11] Instantiated an idempotent producer.
2025-04-12 18:49:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483745408
2025-04-12 18:49:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_20', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-12 18:49:05 [kafka-producer-network-thread | producer-11] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-11] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:05 [kafka-producer-network-thread | producer-11] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-11] ProducerId set to 8236 with epoch 0
2025-04-12 18:49:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:05 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_60, value: {"_id": {"$oid": "67f10b97558ab0f6396b1446"}, "review": "Not satisfied at all.", "authId": "user_60", "datetime": "2025-03-10T13:21:03.219Z", "sentiment": null}, offset: 70460
2025-04-12 18:49:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_60
2025-04-12 18:49:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1446"}, "review": "Not satisfied at all.", "authId": "user_60", "datetime": "2025-03-10T13:21:03.219Z", "sentiment": null}%n
2025-04-12 18:49:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:49:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:49:06 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:06 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:06 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-12] Instantiated an idempotent producer.
2025-04-12 18:49:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483746933
2025-04-12 18:49:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_60', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:49:06 [kafka-producer-network-thread | producer-12] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-12] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:06 [kafka-producer-network-thread | producer-12] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-12] ProducerId set to 8237 with epoch 0
2025-04-12 18:49:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:06 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_82, value: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}, offset: 70461
2025-04-12 18:49:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_82
2025-04-12 18:49:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}%n
2025-04-12 18:49:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:49:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:49:07 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:07 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:07 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-13] Instantiated an idempotent producer.
2025-04-12 18:49:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483747551
2025-04-12 18:49:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_82', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:49:07 [kafka-producer-network-thread | producer-13] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-13] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:07 [kafka-producer-network-thread | producer-13] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-13] ProducerId set to 8238 with epoch 0
2025-04-12 18:49:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:07 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_30, value: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}, offset: 70462
2025-04-12 18:49:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_30
2025-04-12 18:49:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}%n
2025-04-12 18:49:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:49:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:49:09 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:09 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:09 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-14] Instantiated an idempotent producer.
2025-04-12 18:49:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483749084
2025-04-12 18:49:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_30', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:49:09 [kafka-producer-network-thread | producer-14] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-14] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:09 [kafka-producer-network-thread | producer-14] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-14] ProducerId set to 8239 with epoch 0
2025-04-12 18:49:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:09 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_83, value: {"_id": {"$oid": "67f10b97558ab0f6396b145d"}, "review": "Fantastic performance and very durable.", "authId": "user_83", "datetime": "2025-01-10T22:14:10.352Z", "sentiment": null}, offset: 70463
2025-04-12 18:49:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_83
2025-04-12 18:49:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145d"}, "review": "Fantastic performance and very durable.", "authId": "user_83", "datetime": "2025-01-10T22:14:10.352Z", "sentiment": null}%n
2025-04-12 18:49:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 18:49:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 18:49:10 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:10 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:10 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-15] Instantiated an idempotent producer.
2025-04-12 18:49:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483750638
2025-04-12 18:49:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_83', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 18:49:10 [kafka-producer-network-thread | producer-15] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-15] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:10 [kafka-producer-network-thread | producer-15] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-15] ProducerId set to 8240 with epoch 0
2025-04-12 18:49:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:10 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_78, value: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}, offset: 70464
2025-04-12 18:49:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_78
2025-04-12 18:49:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}%n
2025-04-12 18:49:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:49:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:49:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:12 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-16] Instantiated an idempotent producer.
2025-04-12 18:49:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483752185
2025-04-12 18:49:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_78', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:49:12 [kafka-producer-network-thread | producer-16] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-16] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:12 [kafka-producer-network-thread | producer-16] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-16] ProducerId set to 8241 with epoch 0
2025-04-12 18:49:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:12 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_9, value: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}, offset: 70465
2025-04-12 18:49:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_9
2025-04-12 18:49:12 [Worker-5] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:12 [Worker-6] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:12 [Worker-5] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:12 [Worker-3] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:12 [Worker-6] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:12 [Worker-3] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:12 [Thread-6] INFO  o.s.t.kafka.consumer.ConsumerManager - Shutting down consumers...
2025-04-12 18:49:12 [Worker-2] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:12 [Worker-4] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:12 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Member consumer-contacts-test-group-5-b3c47e21-1657-467f-847e-0c57ce85ac2f sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 18:49:12 [Worker-2] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:12 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Member consumer-contacts-test-group-3-9b866bc5-4472-4f8f-8bcd-cabfdd8b0d1d sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 18:49:12 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Member consumer-contacts-test-group-6-a157c31b-581e-4b15-9d03-9045c98d2f8b sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 18:49:12 [Worker-4] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:12 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Member consumer-contacts-test-group-2-03b48651-3813-4bf4-9285-869c03edfca5 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 18:49:12 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 18:49:12 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 18:49:12 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 18:49:12 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Member consumer-contacts-test-group-4-eabfb956-1199-4022-aa7a-ca9e0fc5c1f7 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 18:49:12 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 18:49:12 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 18:49:12 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 18:49:12 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 18:49:12 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 18:49:12 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 18:49:12 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 18:49:12 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 18:49:12 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 18:49:12 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 18:49:12 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 18:49:12 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 18:49:12 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 18:49:12 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 18:49:12 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 18:49:12 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 18:49:12 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 18:49:12 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 18:49:12 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 18:49:12 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 18:49:12 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 18:49:12 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 18:49:12 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 18:49:12 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 18:49:12 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 18:49:12 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 18:49:12 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 18:49:12 [Worker-5] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-5 unregistered
2025-04-12 18:49:12 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 18:49:12 [Worker-6] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-6 unregistered
2025-04-12 18:49:12 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 18:49:12 [Worker-4] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-4 unregistered
2025-04-12 18:49:12 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 18:49:12 [Worker-2] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-2 unregistered
2025-04-12 18:49:12 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 18:49:12 [Worker-3] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-3 unregistered
2025-04-12 18:49:12 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 18:49:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}%n
2025-04-12 18:49:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-12 18:49:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-12 18:49:13 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:13 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:13 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-17] Instantiated an idempotent producer.
2025-04-12 18:49:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483753724
2025-04-12 18:49:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_9', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-12 18:49:13 [kafka-producer-network-thread | producer-17] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-17] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:13 [kafka-producer-network-thread | producer-17] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-17] ProducerId set to 8242 with epoch 0
2025-04-12 18:49:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:13 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 70466
2025-04-12 18:49:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-12 18:49:13 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:13 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:13 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 18:49:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-12 18:49:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:49:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:49:14 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:14 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:14 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-18] Instantiated an idempotent producer.
2025-04-12 18:49:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483754278
2025-04-12 18:49:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:49:14 [kafka-producer-network-thread | producer-18] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-18] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:14 [kafka-producer-network-thread | producer-18] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-18] ProducerId set to 8243 with epoch 0
2025-04-12 18:49:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:14 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_94, value: {"_id": {"$oid": "67f10b97558ab0f6396b1468"}, "review": "Great quality, will buy again.", "authId": "user_94", "datetime": "2025-03-23T03:36:43.648Z", "sentiment": null}, offset: 70467
2025-04-12 18:49:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_94
2025-04-12 18:49:14 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:14 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1468"}, "review": "Great quality, will buy again.", "authId": "user_94", "datetime": "2025-03-23T03:36:43.648Z", "sentiment": null}%n
2025-04-12 18:49:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: 0.1

2025-04-12 18:49:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: 0.1

2025-04-12 18:49:15 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:15 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:15 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-19] Instantiated an idempotent producer.
2025-04-12 18:49:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483755816
2025-04-12 18:49:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_94', sentiment=magnitude: 0.1
score: 0.1
}
2025-04-12 18:49:15 [kafka-producer-network-thread | producer-19] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-19] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:15 [kafka-producer-network-thread | producer-19] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-19] ProducerId set to 8244 with epoch 0
2025-04-12 18:49:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:15 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_54, value: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}, offset: 70468
2025-04-12 18:49:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_54
2025-04-12 18:49:15 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:15 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:16 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 18:49:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}%n
2025-04-12 18:49:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-12 18:49:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-12 18:49:17 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:17 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:17 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-20] Instantiated an idempotent producer.
2025-04-12 18:49:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483757321
2025-04-12 18:49:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_54', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-12 18:49:17 [kafka-producer-network-thread | producer-20] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-20] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:17 [kafka-producer-network-thread | producer-20] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-20] ProducerId set to 8245 with epoch 0
2025-04-12 18:49:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:17 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_73, value: {"_id": {"$oid": "67f10b97558ab0f6396b1453"}, "review": "Not satisfied at all.", "authId": "user_73", "datetime": "2025-03-31T12:24:38.653Z", "sentiment": null}, offset: 70469
2025-04-12 18:49:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_73
2025-04-12 18:49:17 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:17 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1453"}, "review": "Not satisfied at all.", "authId": "user_73", "datetime": "2025-03-31T12:24:38.653Z", "sentiment": null}%n
2025-04-12 18:49:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:49:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:49:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-21] Instantiated an idempotent producer.
2025-04-12 18:49:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483758817
2025-04-12 18:49:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_73', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:49:18 [kafka-producer-network-thread | producer-21] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-21] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:18 [kafka-producer-network-thread | producer-21] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-21] ProducerId set to 8246 with epoch 0
2025-04-12 18:49:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_41, value: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}, offset: 70470
2025-04-12 18:49:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_41
2025-04-12 18:49:18 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:18 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:19 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 18:49:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}%n
2025-04-12 18:49:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:49:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:49:20 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:20 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:20 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-22] Instantiated an idempotent producer.
2025-04-12 18:49:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483760359
2025-04-12 18:49:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:20 [kafka-producer-network-thread | producer-22] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-22] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_41', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:49:20 [kafka-producer-network-thread | producer-22] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-22] ProducerId set to 8247 with epoch 0
2025-04-12 18:49:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:20 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_68, value: {"_id": {"$oid": "67f10b97558ab0f6396b144e"}, "review": "Poor build quality, not reliable.", "authId": "user_68", "datetime": "2025-01-15T21:21:03.997Z", "sentiment": null}, offset: 70471
2025-04-12 18:49:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_68
2025-04-12 18:49:20 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:20 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144e"}, "review": "Poor build quality, not reliable.", "authId": "user_68", "datetime": "2025-01-15T21:21:03.997Z", "sentiment": null}%n
2025-04-12 18:49:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:49:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:49:21 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-23
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:21 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:21 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-23] Instantiated an idempotent producer.
2025-04-12 18:49:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483761962
2025-04-12 18:49:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_68', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:49:21 [kafka-producer-network-thread | producer-23] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-23] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:21 [kafka-producer-network-thread | producer-23] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-23] ProducerId set to 8248 with epoch 0
2025-04-12 18:49:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:21 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_92, value: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}, offset: 70472
2025-04-12 18:49:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_92
2025-04-12 18:49:21 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:21 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:22 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 18:49:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}%n
2025-04-12 18:49:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:49:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:49:23 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-24
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:23 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:23 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-24] Instantiated an idempotent producer.
2025-04-12 18:49:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483763477
2025-04-12 18:49:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_92', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:49:23 [kafka-producer-network-thread | producer-24] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-24] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:23 [kafka-producer-network-thread | producer-24] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-24] ProducerId set to 8249 with epoch 0
2025-04-12 18:49:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:23 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_61, value: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}, offset: 70473
2025-04-12 18:49:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_61
2025-04-12 18:49:23 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:23 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}%n
2025-04-12 18:49:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:49:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:49:24 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-25
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:49:24 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:49:24 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-25] Instantiated an idempotent producer.
2025-04-12 18:49:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:49:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:49:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744483764036
2025-04-12 18:49:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:49:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_61', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:49:24 [kafka-producer-network-thread | producer-25] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-25] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:49:24 [kafka-producer-network-thread | producer-25] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-25] ProducerId set to 8250 with epoch 0
2025-04-12 18:49:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:49:24 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_36, value: {"_id": {"$oid": "67f10b97558ab0f6396b142e"}, "review": "Works like a charm, perfect!", "authId": "user_36", "datetime": "2024-12-07T01:28:05.981Z", "sentiment": null}, offset: 70474
2025-04-12 18:49:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_36
2025-04-12 18:49:24 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:49:24 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 18:59:33 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Starting 6 consumers for the 'contacts' topic...
2025-04-12 18:59:33 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:59:33 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484374429
2025-04-12 18:59:34 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:59:34 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:34 [Worker-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:59:34 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484374465
2025-04-12 18:59:34 [Worker-2] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:59:34 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:59:34 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:59:34 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484374486
2025-04-12 18:59:34 [Worker-3] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:59:34 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:59:34 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:59:34 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484374503
2025-04-12 18:59:34 [Worker-4] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:59:34 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:59:34 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:59:34 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484374535
2025-04-12 18:59:34 [Worker-5] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:59:34 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 18:59:34 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:59:34 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:34 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484374554
2025-04-12 18:59:34 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Started 6 consumers for the 'contacts' topic.
2025-04-12 18:59:34 [Worker-6] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 18:59:34 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 18:59:35 [Worker-5] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:35 [Worker-2] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:35 [Worker-4] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:35 [Worker-3] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:35 [Worker-6] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:35 [Worker-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:35 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:59:35 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:59:35 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:59:35 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:59:35 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:59:35 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 18:59:35 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:59:35 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:59:35 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:59:35 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:59:35 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:59:35 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:59:35 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-4-e359a335-17c0-409c-a369-9b1dd86b5668
2025-04-12 18:59:35 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-2-708a9dc1-39d6-44a5-b5ca-1661b99887f4
2025-04-12 18:59:35 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-6-386a5e5a-65e0-4869-9e31-f6474fbc0bc8
2025-04-12 18:59:35 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:59:35 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:59:35 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-1-12ba69be-412f-44d6-9533-16405b95610c
2025-04-12 18:59:35 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-3-7ed59560-5fc5-4589-85d2-6e2314454d2a
2025-04-12 18:59:35 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-5-a3503ab8-ceef-4521-96aa-81b483426e69
2025-04-12 18:59:35 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:59:35 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:59:35 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:59:35 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-12 18:59:38 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=101, memberId='consumer-contacts-test-group-1-12ba69be-412f-44d6-9533-16405b95610c', protocol='range'}
2025-04-12 18:59:38 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=101, memberId='consumer-contacts-test-group-2-708a9dc1-39d6-44a5-b5ca-1661b99887f4', protocol='range'}
2025-04-12 18:59:38 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=101, memberId='consumer-contacts-test-group-3-7ed59560-5fc5-4589-85d2-6e2314454d2a', protocol='range'}
2025-04-12 18:59:38 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=101, memberId='consumer-contacts-test-group-4-e359a335-17c0-409c-a369-9b1dd86b5668', protocol='range'}
2025-04-12 18:59:38 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Finished assignment for group at generation 101: {consumer-contacts-test-group-3-7ed59560-5fc5-4589-85d2-6e2314454d2a=Assignment(partitions=[]), consumer-contacts-test-group-4-e359a335-17c0-409c-a369-9b1dd86b5668=Assignment(partitions=[]), consumer-contacts-test-group-2-708a9dc1-39d6-44a5-b5ca-1661b99887f4=Assignment(partitions=[]), consumer-contacts-test-group-5-a3503ab8-ceef-4521-96aa-81b483426e69=Assignment(partitions=[]), consumer-contacts-test-group-6-386a5e5a-65e0-4869-9e31-f6474fbc0bc8=Assignment(partitions=[]), consumer-contacts-test-group-1-12ba69be-412f-44d6-9533-16405b95610c=Assignment(partitions=[test-topic-0])}
2025-04-12 18:59:38 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=101, memberId='consumer-contacts-test-group-1-12ba69be-412f-44d6-9533-16405b95610c', protocol='range'}
2025-04-12 18:59:38 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=101, memberId='consumer-contacts-test-group-4-e359a335-17c0-409c-a369-9b1dd86b5668', protocol='range'}
2025-04-12 18:59:38 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=101, memberId='consumer-contacts-test-group-3-7ed59560-5fc5-4589-85d2-6e2314454d2a', protocol='range'}
2025-04-12 18:59:38 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=101, memberId='consumer-contacts-test-group-2-708a9dc1-39d6-44a5-b5ca-1661b99887f4', protocol='range'}
2025-04-12 18:59:38 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:59:38 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-12 18:59:38 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:59:38 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:59:38 [Worker-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:59:38 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:59:38 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:59:38 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=101, memberId='consumer-contacts-test-group-5-a3503ab8-ceef-4521-96aa-81b483426e69', protocol='range'}
2025-04-12 18:59:38 [Worker-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-12 18:59:38 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=101, memberId='consumer-contacts-test-group-5-a3503ab8-ceef-4521-96aa-81b483426e69', protocol='range'}
2025-04-12 18:59:38 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:59:38 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:59:38 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=101, memberId='consumer-contacts-test-group-6-386a5e5a-65e0-4869-9e31-f6474fbc0bc8', protocol='range'}
2025-04-12 18:59:38 [Worker-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=70449, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=14}}
2025-04-12 18:59:38 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=101, memberId='consumer-contacts-test-group-6-386a5e5a-65e0-4869-9e31-f6474fbc0bc8', protocol='range'}
2025-04-12 18:59:38 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 18:59:38 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 18:59:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70449
2025-04-12 18:59:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-12 18:59:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}%n
2025-04-12 18:59:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:59:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:59:43 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:59:43 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:43 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-04-12 18:59:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484383295
2025-04-12 18:59:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:59:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_50', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:59:43 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:43 [kafka-producer-network-thread | producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 8251 with epoch 0
2025-04-12 18:59:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:59:43 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_93, value: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}, offset: 70450
2025-04-12 18:59:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_93
2025-04-12 18:59:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}%n
2025-04-12 18:59:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:59:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:59:44 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:59:44 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:44 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-2] Instantiated an idempotent producer.
2025-04-12 18:59:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484384367
2025-04-12 18:59:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:59:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_93', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:59:44 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-2] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:44 [kafka-producer-network-thread | producer-2] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-2] ProducerId set to 8252 with epoch 0
2025-04-12 18:59:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:59:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_21, value: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}, offset: 70451
2025-04-12 18:59:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_21
2025-04-12 18:59:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}%n
2025-04-12 18:59:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:59:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:59:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:59:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-3] Instantiated an idempotent producer.
2025-04-12 18:59:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484385977
2025-04-12 18:59:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:59:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_21', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:59:45 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-3] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:45 [kafka-producer-network-thread | producer-3] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-3] ProducerId set to 8253 with epoch 0
2025-04-12 18:59:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:59:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_53, value: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}, offset: 70452
2025-04-12 18:59:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_53
2025-04-12 18:59:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}%n
2025-04-12 18:59:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-12 18:59:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-12 18:59:47 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:59:47 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:47 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-4] Instantiated an idempotent producer.
2025-04-12 18:59:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484387543
2025-04-12 18:59:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:59:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_53', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-12 18:59:47 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-4] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:47 [kafka-producer-network-thread | producer-4] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-4] ProducerId set to 8254 with epoch 0
2025-04-12 18:59:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:59:47 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_5, value: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}, offset: 70453
2025-04-12 18:59:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_5
2025-04-12 18:59:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}%n
2025-04-12 18:59:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:59:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:59:49 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:59:49 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:49 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-5] Instantiated an idempotent producer.
2025-04-12 18:59:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484389064
2025-04-12 18:59:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:59:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_5', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:59:49 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-5] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:49 [kafka-producer-network-thread | producer-5] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-5] ProducerId set to 8255 with epoch 0
2025-04-12 18:59:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:59:49 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_95, value: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}, offset: 70454
2025-04-12 18:59:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_95
2025-04-12 18:59:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}%n
2025-04-12 18:59:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:59:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:59:50 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:59:50 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:50 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-6] Instantiated an idempotent producer.
2025-04-12 18:59:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484390624
2025-04-12 18:59:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:59:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_95', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:59:50 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-6] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:50 [kafka-producer-network-thread | producer-6] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-6] ProducerId set to 8256 with epoch 0
2025-04-12 18:59:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:59:50 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_16, value: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}, offset: 70455
2025-04-12 18:59:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_16
2025-04-12 18:59:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}%n
2025-04-12 18:59:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 18:59:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 18:59:52 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:59:52 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:52 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-7] Instantiated an idempotent producer.
2025-04-12 18:59:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484392166
2025-04-12 18:59:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:59:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_16', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 18:59:52 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-7] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:52 [kafka-producer-network-thread | producer-7] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-7] ProducerId set to 8257 with epoch 0
2025-04-12 18:59:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:59:52 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 70456
2025-04-12 18:59:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-12 18:59:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}%n
2025-04-12 18:59:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 18:59:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 18:59:53 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:59:53 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:53 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-8] Instantiated an idempotent producer.
2025-04-12 18:59:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484393708
2025-04-12 18:59:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:59:53 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-8] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_91', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 18:59:53 [kafka-producer-network-thread | producer-8] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-8] ProducerId set to 8258 with epoch 0
2025-04-12 18:59:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:59:53 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_14, value: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}, offset: 70457
2025-04-12 18:59:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_14
2025-04-12 18:59:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}%n
2025-04-12 18:59:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 18:59:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 18:59:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:59:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-9] Instantiated an idempotent producer.
2025-04-12 18:59:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484395219
2025-04-12 18:59:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:59:55 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-9] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_14', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 18:59:55 [kafka-producer-network-thread | producer-9] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-9] ProducerId set to 8259 with epoch 0
2025-04-12 18:59:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:59:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_61, value: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}, offset: 70458
2025-04-12 18:59:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_61
2025-04-12 18:59:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}%n
2025-04-12 18:59:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:59:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:59:56 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:59:56 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:56 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-10] Instantiated an idempotent producer.
2025-04-12 18:59:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484396757
2025-04-12 18:59:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:59:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_61', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:59:56 [kafka-producer-network-thread | producer-10] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-10] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:56 [kafka-producer-network-thread | producer-10] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-10] ProducerId set to 8260 with epoch 0
2025-04-12 18:59:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:59:56 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_20, value: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}, offset: 70459
2025-04-12 18:59:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_20
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}%n
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-12 18:59:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:59:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-11] Instantiated an idempotent producer.
2025-04-12 18:59:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484398308
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_20', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-12 18:59:58 [kafka-producer-network-thread | producer-11] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-11] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:58 [kafka-producer-network-thread | producer-11] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-11] ProducerId set to 8261 with epoch 0
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_60, value: {"_id": {"$oid": "67f10b97558ab0f6396b1446"}, "review": "Not satisfied at all.", "authId": "user_60", "datetime": "2025-03-10T13:21:03.219Z", "sentiment": null}, offset: 70460
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_60
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1446"}, "review": "Not satisfied at all.", "authId": "user_60", "datetime": "2025-03-10T13:21:03.219Z", "sentiment": null}%n
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 18:59:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 18:59:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 18:59:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-12] Instantiated an idempotent producer.
2025-04-12 18:59:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 18:59:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 18:59:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744484398882
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_60', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 18:59:58 [kafka-producer-network-thread | producer-12] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-12] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 18:59:58 [kafka-producer-network-thread | producer-12] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-12] ProducerId set to 8262 with epoch 0
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_82, value: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}, offset: 70461
2025-04-12 18:59:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_82
