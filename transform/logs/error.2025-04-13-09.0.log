2025-04-13 09:09:53 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Starting 6 consumers for the 'contacts' topic...
2025-04-13 09:09:53 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-13 09:09:53 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:09:53 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:09:53 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:09:53 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535393943
2025-04-13 09:09:53 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-13 09:09:53 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:09:53 [Worker-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-13 09:09:53 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-13 09:09:53 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:09:53 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:09:53 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535393984
2025-04-13 09:09:53 [Worker-2] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-13 09:09:53 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-13 09:09:54 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-13 09:09:54 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:09:54 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:09:54 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:09:54 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535394043
2025-04-13 09:09:54 [Worker-3] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-13 09:09:54 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-13 09:09:54 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-13 09:09:54 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:09:54 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:09:54 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:09:54 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535394065
2025-04-13 09:09:54 [Worker-4] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-13 09:09:54 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-13 09:09:54 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-13 09:09:54 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:09:54 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:09:54 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:09:54 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535394092
2025-04-13 09:09:54 [Worker-5] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-13 09:09:54 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-13 09:09:54 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-13 09:09:54 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:09:54 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:09:54 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:09:54 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535394113
2025-04-13 09:09:54 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Started 6 consumers for the 'contacts' topic.
2025-04-13 09:09:54 [Worker-6] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-13 09:09:54 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-13 09:09:54 [Worker-6] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:09:54 [Worker-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:09:54 [Worker-5] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:09:54 [Worker-2] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:09:54 [Worker-3] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:09:54 [Worker-4] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:09:54 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-13 09:09:54 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-13 09:09:54 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-13 09:09:54 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-13 09:09:54 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-13 09:09:54 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-13 09:09:54 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-13 09:09:54 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-13 09:09:54 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-13 09:09:54 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-13 09:09:54 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-13 09:09:54 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-13 09:09:55 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-4-824bd2ff-c9ed-445b-b988-e6d39c49abbc
2025-04-13 09:09:55 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-6-d1b429d0-45bd-4f10-9b2b-13cad52d8a32
2025-04-13 09:09:55 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-13 09:09:55 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-13 09:09:55 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-5-831392e3-2b17-4620-b548-44b3e5131cbe
2025-04-13 09:09:55 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-13 09:09:56 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-2-6eacb86f-110c-46ee-a37d-7e45b8cc5b44
2025-04-13 09:09:56 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-1-2b9697ac-9d85-45b3-91d7-3fa2fde2d9c7
2025-04-13 09:09:56 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-13 09:09:56 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-13 09:09:56 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-3-10de9477-eb27-416c-a1ab-41346555e3d0
2025-04-13 09:09:56 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-13 09:09:58 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=115, memberId='consumer-contacts-test-group-3-10de9477-eb27-416c-a1ab-41346555e3d0', protocol='range'}
2025-04-13 09:09:58 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=115, memberId='consumer-contacts-test-group-2-6eacb86f-110c-46ee-a37d-7e45b8cc5b44', protocol='range'}
2025-04-13 09:09:58 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=115, memberId='consumer-contacts-test-group-5-831392e3-2b17-4620-b548-44b3e5131cbe', protocol='range'}
2025-04-13 09:09:58 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=115, memberId='consumer-contacts-test-group-4-824bd2ff-c9ed-445b-b988-e6d39c49abbc', protocol='range'}
2025-04-13 09:09:58 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=115, memberId='consumer-contacts-test-group-1-2b9697ac-9d85-45b3-91d7-3fa2fde2d9c7', protocol='range'}
2025-04-13 09:09:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=115, memberId='consumer-contacts-test-group-6-d1b429d0-45bd-4f10-9b2b-13cad52d8a32', protocol='range'}
2025-04-13 09:09:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Finished assignment for group at generation 115: {consumer-contacts-test-group-1-2b9697ac-9d85-45b3-91d7-3fa2fde2d9c7=Assignment(partitions=[test-topic-0]), consumer-contacts-test-group-5-831392e3-2b17-4620-b548-44b3e5131cbe=Assignment(partitions=[]), consumer-contacts-test-group-6-d1b429d0-45bd-4f10-9b2b-13cad52d8a32=Assignment(partitions=[]), consumer-contacts-test-group-4-824bd2ff-c9ed-445b-b988-e6d39c49abbc=Assignment(partitions=[]), consumer-contacts-test-group-2-6eacb86f-110c-46ee-a37d-7e45b8cc5b44=Assignment(partitions=[]), consumer-contacts-test-group-3-10de9477-eb27-416c-a1ab-41346555e3d0=Assignment(partitions=[])}
2025-04-13 09:09:58 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=115, memberId='consumer-contacts-test-group-4-824bd2ff-c9ed-445b-b988-e6d39c49abbc', protocol='range'}
2025-04-13 09:09:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=115, memberId='consumer-contacts-test-group-6-d1b429d0-45bd-4f10-9b2b-13cad52d8a32', protocol='range'}
2025-04-13 09:09:58 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=115, memberId='consumer-contacts-test-group-5-831392e3-2b17-4620-b548-44b3e5131cbe', protocol='range'}
2025-04-13 09:09:58 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=115, memberId='consumer-contacts-test-group-1-2b9697ac-9d85-45b3-91d7-3fa2fde2d9c7', protocol='range'}
2025-04-13 09:09:58 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 09:09:58 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=115, memberId='consumer-contacts-test-group-3-10de9477-eb27-416c-a1ab-41346555e3d0', protocol='range'}
2025-04-13 09:09:58 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=115, memberId='consumer-contacts-test-group-2-6eacb86f-110c-46ee-a37d-7e45b8cc5b44', protocol='range'}
2025-04-13 09:09:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 09:09:58 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 09:09:58 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 09:09:58 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-13 09:09:58 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 09:09:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 09:09:58 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 09:09:58 [Worker-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 09:09:58 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 09:09:58 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 09:09:58 [Worker-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-13 09:09:58 [Worker-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=70729, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=16}}
2025-04-13 09:09:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_42, value: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}, offset: 70729
2025-04-13 09:09:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_42
2025-04-13 09:10:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}%n
2025-04-13 09:10:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 09:10:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 09:10:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-04-13 09:10:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535401585
2025-04-13 09:10:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_42', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 09:10:01 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:01 [kafka-producer-network-thread | producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 9140 with epoch 0
2025-04-13 09:10:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:02 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_52, value: {"_id": {"$oid": "67f10b97558ab0f6396b143e"}, "review": "This changed my life!", "authId": "user_52", "datetime": "2025-01-10T06:57:54.745Z", "sentiment": null}, offset: 70730
2025-04-13 09:10:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_52
2025-04-13 09:10:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143e"}, "review": "This changed my life!", "authId": "user_52", "datetime": "2025-01-10T06:57:54.745Z", "sentiment": null}%n
2025-04-13 09:10:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 09:10:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 09:10:03 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:03 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:03 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-2] Instantiated an idempotent producer.
2025-04-13 09:10:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535403577
2025-04-13 09:10:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_52', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 09:10:03 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-2] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:03 [kafka-producer-network-thread | producer-2] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-2] ProducerId set to 9141 with epoch 0
2025-04-13 09:10:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:03 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_56, value: {"_id": {"$oid": "67f10b97558ab0f6396b1442"}, "review": "It's okay, does the job.", "authId": "user_56", "datetime": "2024-12-14T20:56:35.004Z", "sentiment": null}, offset: 70731
2025-04-13 09:10:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_56
2025-04-13 09:10:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1442"}, "review": "It's okay, does the job.", "authId": "user_56", "datetime": "2024-12-14T20:56:35.004Z", "sentiment": null}%n
2025-04-13 09:10:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-13 09:10:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-13 09:10:05 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:05 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:05 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-3] Instantiated an idempotent producer.
2025-04-13 09:10:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535405071
2025-04-13 09:10:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_56', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-13 09:10:05 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-3] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:05 [kafka-producer-network-thread | producer-3] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-3] ProducerId set to 9142 with epoch 0
2025-04-13 09:10:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:05 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_39, value: {"_id": {"$oid": "67f10b97558ab0f6396b1431"}, "review": "Some features are good, others not so much.", "authId": "user_39", "datetime": "2025-01-13T19:31:50.928Z", "sentiment": null}, offset: 70732
2025-04-13 09:10:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_39
2025-04-13 09:10:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1431"}, "review": "Some features are good, others not so much.", "authId": "user_39", "datetime": "2025-01-13T19:31:50.928Z", "sentiment": null}%n
2025-04-13 09:10:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 09:10:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 09:10:06 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:06 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:06 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-4] Instantiated an idempotent producer.
2025-04-13 09:10:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535406541
2025-04-13 09:10:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_39', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 09:10:06 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-4] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:06 [kafka-producer-network-thread | producer-4] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-4] ProducerId set to 9143 with epoch 0
2025-04-13 09:10:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:06 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_53, value: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}, offset: 70733
2025-04-13 09:10:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_53
2025-04-13 09:10:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}%n
2025-04-13 09:10:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 09:10:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 09:10:07 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:07 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:07 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-5] Instantiated an idempotent producer.
2025-04-13 09:10:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:08 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-5] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535407997
2025-04-13 09:10:08 [kafka-producer-network-thread | producer-5] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-5] ProducerId set to 9144 with epoch 0
2025-04-13 09:10:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_53', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 09:10:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:08 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_39, value: {"_id": {"$oid": "67f10b97558ab0f6396b1431"}, "review": "Some features are good, others not so much.", "authId": "user_39", "datetime": "2025-01-13T19:31:50.928Z", "sentiment": null}, offset: 70734
2025-04-13 09:10:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_39
2025-04-13 09:10:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1431"}, "review": "Some features are good, others not so much.", "authId": "user_39", "datetime": "2025-01-13T19:31:50.928Z", "sentiment": null}%n
2025-04-13 09:10:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 09:10:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 09:10:09 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:09 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:09 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-6] Instantiated an idempotent producer.
2025-04-13 09:10:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535409485
2025-04-13 09:10:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_39', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 09:10:09 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-6] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:09 [kafka-producer-network-thread | producer-6] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-6] ProducerId set to 9145 with epoch 0
2025-04-13 09:10:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:09 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_84, value: {"_id": {"$oid": "67f10b97558ab0f6396b145e"}, "review": "Terrible experience, very disappointed.", "authId": "user_84", "datetime": "2025-03-09T23:08:10.325Z", "sentiment": null}, offset: 70735
2025-04-13 09:10:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_84
2025-04-13 09:10:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145e"}, "review": "Terrible experience, very disappointed.", "authId": "user_84", "datetime": "2025-03-09T23:08:10.325Z", "sentiment": null}%n
2025-04-13 09:10:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 09:10:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 09:10:10 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:10 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:10 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-7] Instantiated an idempotent producer.
2025-04-13 09:10:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535410975
2025-04-13 09:10:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_84', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 09:10:10 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-7] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:10 [kafka-producer-network-thread | producer-7] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-7] ProducerId set to 9146 with epoch 0
2025-04-13 09:10:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:10 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_53, value: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}, offset: 70736
2025-04-13 09:10:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_53
2025-04-13 09:10:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}%n
2025-04-13 09:10:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 09:10:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 09:10:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:12 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-8] Instantiated an idempotent producer.
2025-04-13 09:10:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535412458
2025-04-13 09:10:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_53', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 09:10:12 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-8] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:12 [kafka-producer-network-thread | producer-8] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-8] ProducerId set to 9147 with epoch 0
2025-04-13 09:10:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:12 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_53, value: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}, offset: 70737
2025-04-13 09:10:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_53
2025-04-13 09:10:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}%n
2025-04-13 09:10:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 09:10:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 09:10:13 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:13 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:13 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-9] Instantiated an idempotent producer.
2025-04-13 09:10:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535413985
2025-04-13 09:10:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_53', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 09:10:13 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-9] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:13 [kafka-producer-network-thread | producer-9] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-9] ProducerId set to 9148 with epoch 0
2025-04-13 09:10:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:13 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_30, value: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}, offset: 70738
2025-04-13 09:10:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_30
2025-04-13 09:10:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}%n
2025-04-13 09:10:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 09:10:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 09:10:15 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:15 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:15 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-10] Instantiated an idempotent producer.
2025-04-13 09:10:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535415480
2025-04-13 09:10:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:15 [kafka-producer-network-thread | producer-10] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-10] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_30', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 09:10:15 [kafka-producer-network-thread | producer-10] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-10] ProducerId set to 9149 with epoch 0
2025-04-13 09:10:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:16 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_90, value: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}, offset: 70739
2025-04-13 09:10:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_90
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}%n
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: -0.1

2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: -0.1

2025-04-13 09:10:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-11] Instantiated an idempotent producer.
2025-04-13 09:10:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535418186
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_90', sentiment=magnitude: 0.1
score: -0.1
}
2025-04-13 09:10:18 [kafka-producer-network-thread | producer-11] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-11] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:18 [kafka-producer-network-thread | producer-11] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-11] ProducerId set to 9150 with epoch 0
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "I’m very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 70740
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "I’m very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}%n
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 09:10:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-12] Instantiated an idempotent producer.
2025-04-13 09:10:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535418757
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_91', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 09:10:18 [kafka-producer-network-thread | producer-12] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-12] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:18 [kafka-producer-network-thread | producer-12] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-12] ProducerId set to 9151 with epoch 0
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_84, value: {"_id": {"$oid": "67f10b97558ab0f6396b145e"}, "review": "Terrible experience, very disappointed.", "authId": "user_84", "datetime": "2025-03-09T23:08:10.325Z", "sentiment": null}, offset: 70741
2025-04-13 09:10:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_84
2025-04-13 09:10:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145e"}, "review": "Terrible experience, very disappointed.", "authId": "user_84", "datetime": "2025-03-09T23:08:10.325Z", "sentiment": null}%n
2025-04-13 09:10:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 09:10:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 09:10:19 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:19 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:19 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-13] Instantiated an idempotent producer.
2025-04-13 09:10:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535419315
2025-04-13 09:10:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_84', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 09:10:19 [kafka-producer-network-thread | producer-13] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-13] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:19 [kafka-producer-network-thread | producer-13] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-13] ProducerId set to 9152 with epoch 0
2025-04-13 09:10:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:19 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_98, value: {"_id": {"$oid": "67f10b97558ab0f6396b146c"}, "review": "Not bad, not great either.", "authId": "user_98", "datetime": "2025-03-21T23:54:21.817Z", "sentiment": null}, offset: 70742
2025-04-13 09:10:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_98
2025-04-13 09:10:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146c"}, "review": "Not bad, not great either.", "authId": "user_98", "datetime": "2025-03-21T23:54:21.817Z", "sentiment": null}%n
2025-04-13 09:10:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 09:10:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 09:10:20 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:20 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:20 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-14] Instantiated an idempotent producer.
2025-04-13 09:10:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535420862
2025-04-13 09:10:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_98', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 09:10:20 [kafka-producer-network-thread | producer-14] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-14] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:20 [kafka-producer-network-thread | producer-14] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-14] ProducerId set to 9153 with epoch 0
2025-04-13 09:10:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:20 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_56, value: {"_id": {"$oid": "67f10b97558ab0f6396b1442"}, "review": "It's okay, does the job.", "authId": "user_56", "datetime": "2024-12-14T20:56:35.004Z", "sentiment": null}, offset: 70743
2025-04-13 09:10:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_56
2025-04-13 09:10:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1442"}, "review": "It's okay, does the job.", "authId": "user_56", "datetime": "2024-12-14T20:56:35.004Z", "sentiment": null}%n
2025-04-13 09:10:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-13 09:10:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-13 09:10:21 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:21 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:21 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-15] Instantiated an idempotent producer.
2025-04-13 09:10:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535421406
2025-04-13 09:10:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_56', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-13 09:10:21 [kafka-producer-network-thread | producer-15] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-15] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:21 [kafka-producer-network-thread | producer-15] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-15] ProducerId set to 9154 with epoch 0
2025-04-13 09:10:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:21 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_96, value: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}, offset: 70744
2025-04-13 09:10:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_96
2025-04-13 09:10:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}%n
2025-04-13 09:10:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 09:10:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 09:10:22 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:22 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:22 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-16] Instantiated an idempotent producer.
2025-04-13 09:10:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535422902
2025-04-13 09:10:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_96', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 09:10:22 [kafka-producer-network-thread | producer-16] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-16] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:22 [kafka-producer-network-thread | producer-16] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-16] ProducerId set to 9155 with epoch 0
2025-04-13 09:10:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:22 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_42, value: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}, offset: 70745
2025-04-13 09:10:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_42
2025-04-13 09:10:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}%n
2025-04-13 09:10:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 09:10:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 09:10:24 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:24 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:24 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-17] Instantiated an idempotent producer.
2025-04-13 09:10:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535424352
2025-04-13 09:10:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_42', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 09:10:24 [kafka-producer-network-thread | producer-17] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-17] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:24 [kafka-producer-network-thread | producer-17] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-17] ProducerId set to 9156 with epoch 0
2025-04-13 09:10:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:24 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_75, value: {"_id": {"$oid": "67f10b97558ab0f6396b1455"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_75", "datetime": "2025-02-20T20:29:36.163Z", "sentiment": null}, offset: 70746
2025-04-13 09:10:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_75
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1455"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_75", "datetime": "2025-02-20T20:29:36.163Z", "sentiment": null}%n
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.3
score: 0.6

2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.3
score: 0.6

2025-04-13 09:10:25 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:25 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:25 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-18] Instantiated an idempotent producer.
2025-04-13 09:10:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535425044
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_75', sentiment=magnitude: 1.3
score: 0.6
}
2025-04-13 09:10:25 [kafka-producer-network-thread | producer-18] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-18] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:25 [kafka-producer-network-thread | producer-18] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-18] ProducerId set to 9157 with epoch 0
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_49, value: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "It’s alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}, offset: 70747
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_49
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "It’s alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}%n
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-13 09:10:25 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:25 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:25 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-19] Instantiated an idempotent producer.
2025-04-13 09:10:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535425545
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_49', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-13 09:10:25 [kafka-producer-network-thread | producer-19] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-19] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:25 [kafka-producer-network-thread | producer-19] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-19] ProducerId set to 9158 with epoch 0
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_36, value: {"_id": {"$oid": "67f10b97558ab0f6396b142e"}, "review": "Works like a charm, perfect!", "authId": "user_36", "datetime": "2024-12-07T01:28:05.981Z", "sentiment": null}, offset: 70748
2025-04-13 09:10:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_36
2025-04-13 09:10:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142e"}, "review": "Works like a charm, perfect!", "authId": "user_36", "datetime": "2024-12-07T01:28:05.981Z", "sentiment": null}%n
2025-04-13 09:10:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 09:10:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 09:10:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-20] Instantiated an idempotent producer.
2025-04-13 09:10:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535426084
2025-04-13 09:10:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_36', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 09:10:26 [kafka-producer-network-thread | producer-20] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-20] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:26 [kafka-producer-network-thread | producer-20] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-20] ProducerId set to 9159 with epoch 0
2025-04-13 09:10:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:27 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_21, value: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}, offset: 70749
2025-04-13 09:10:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_21
2025-04-13 09:10:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}%n
2025-04-13 09:10:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 09:10:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 09:10:28 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:28 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:28 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-21] Instantiated an idempotent producer.
2025-04-13 09:10:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535428602
2025-04-13 09:10:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_21', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 09:10:28 [kafka-producer-network-thread | producer-21] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-21] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:28 [kafka-producer-network-thread | producer-21] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-21] ProducerId set to 9160 with epoch 0
2025-04-13 09:10:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:28 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_53, value: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}, offset: 70750
2025-04-13 09:10:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_53
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}%n
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 09:10:30 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:30 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:30 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-22] Instantiated an idempotent producer.
2025-04-13 09:10:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535430079
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_53', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 09:10:30 [kafka-producer-network-thread | producer-22] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-22] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:30 [kafka-producer-network-thread | producer-22] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-22] ProducerId set to 9161 with epoch 0
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_85, value: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}, offset: 70751
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_85
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}%n
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 09:10:30 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-23
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:30 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:30 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-23] Instantiated an idempotent producer.
2025-04-13 09:10:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535430601
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_85', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 09:10:30 [kafka-producer-network-thread | producer-23] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-23] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:30 [kafka-producer-network-thread | producer-23] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-23] ProducerId set to 9162 with epoch 0
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_5, value: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}, offset: 70752
2025-04-13 09:10:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_5
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}%n
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 09:10:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-24
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-24] Instantiated an idempotent producer.
2025-04-13 09:10:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535432066
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_5', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 09:10:32 [kafka-producer-network-thread | producer-24] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-24] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:32 [kafka-producer-network-thread | producer-24] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-24] ProducerId set to 9163 with epoch 0
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_16, value: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}, offset: 70753
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_16
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}%n
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 09:10:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-25
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-25] Instantiated an idempotent producer.
2025-04-13 09:10:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535432586
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_16', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 09:10:32 [kafka-producer-network-thread | producer-25] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-25] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:32 [kafka-producer-network-thread | producer-25] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-25] ProducerId set to 9164 with epoch 0
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_27, value: {"_id": {"$oid": "67f10b97558ab0f6396b1425"}, "review": "This changed my life!", "authId": "user_27", "datetime": "2024-12-10T09:12:46.567Z", "sentiment": null}, offset: 70754
2025-04-13 09:10:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_27
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1425"}, "review": "This changed my life!", "authId": "user_27", "datetime": "2024-12-10T09:12:46.567Z", "sentiment": null}%n
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 09:10:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-26
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-26] Instantiated an idempotent producer.
2025-04-13 09:10:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535433155
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_27', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 09:10:33 [kafka-producer-network-thread | producer-26] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-26] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:33 [kafka-producer-network-thread | producer-26] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-26] ProducerId set to 9165 with epoch 0
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_32, value: {"_id": {"$oid": "67f10b97558ab0f6396b142a"}, "review": "Fantastic performance and very durable.", "authId": "user_32", "datetime": "2025-01-03T02:26:59.600Z", "sentiment": null}, offset: 70755
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_32
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142a"}, "review": "Fantastic performance and very durable.", "authId": "user_32", "datetime": "2025-01-03T02:26:59.600Z", "sentiment": null}%n
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 09:10:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-27
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-27] Instantiated an idempotent producer.
2025-04-13 09:10:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535433679
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_32', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 09:10:33 [kafka-producer-network-thread | producer-27] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-27] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:33 [kafka-producer-network-thread | producer-27] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-27] ProducerId set to 9166 with epoch 0
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_61, value: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}, offset: 70756
2025-04-13 09:10:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_61
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}%n
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 09:10:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-28
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-28] Instantiated an idempotent producer.
2025-04-13 09:10:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535435147
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_61', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 09:10:35 [kafka-producer-network-thread | producer-28] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-28] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:35 [kafka-producer-network-thread | producer-28] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-28] ProducerId set to 9167 with epoch 0
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_69, value: {"_id": {"$oid": "67f10b97558ab0f6396b144f"}, "review": "Decent, but not amazing.", "authId": "user_69", "datetime": "2024-12-28T22:45:48.069Z", "sentiment": null}, offset: 70757
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_69
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144f"}, "review": "Decent, but not amazing.", "authId": "user_69", "datetime": "2024-12-28T22:45:48.069Z", "sentiment": null}%n
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 09:10:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-29
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-29] Instantiated an idempotent producer.
2025-04-13 09:10:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535435684
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_69', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 09:10:35 [kafka-producer-network-thread | producer-29] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-29] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:35 [kafka-producer-network-thread | producer-29] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-29] ProducerId set to 9168 with epoch 0
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_98, value: {"_id": {"$oid": "67f10b97558ab0f6396b146c"}, "review": "Not bad, not great either.", "authId": "user_98", "datetime": "2025-03-21T23:54:21.817Z", "sentiment": null}, offset: 70758
2025-04-13 09:10:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_98
2025-04-13 09:10:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146c"}, "review": "Not bad, not great either.", "authId": "user_98", "datetime": "2025-03-21T23:54:21.817Z", "sentiment": null}%n
2025-04-13 09:10:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 09:10:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 09:10:36 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-30
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:36 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:36 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-30] Instantiated an idempotent producer.
2025-04-13 09:10:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535436198
2025-04-13 09:10:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_98', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 09:10:36 [kafka-producer-network-thread | producer-30] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-30] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:36 [kafka-producer-network-thread | producer-30] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-30] ProducerId set to 9169 with epoch 0
2025-04-13 09:10:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:37 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70759
2025-04-13 09:10:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-13 09:10:38 [Thread-6] INFO  o.s.t.kafka.consumer.ConsumerManager - Shutting down consumers...
2025-04-13 09:10:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}%n
2025-04-13 09:10:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 09:10:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 09:10:38 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-31
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:38 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:38 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-31] Instantiated an idempotent producer.
2025-04-13 09:10:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535438755
2025-04-13 09:10:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_50', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 09:10:38 [kafka-producer-network-thread | producer-31] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-31] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:38 [kafka-producer-network-thread | producer-31] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-31] ProducerId set to 9170 with epoch 0
2025-04-13 09:10:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "I’m very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 70760
2025-04-13 09:10:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-13 09:10:38 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:38 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:38 [Worker-3] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:38 [Worker-2] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:38 [Worker-5] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:38 [Worker-4] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:38 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Member consumer-contacts-test-group-3-10de9477-eb27-416c-a1ab-41346555e3d0 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-13 09:10:38 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Member consumer-contacts-test-group-5-831392e3-2b17-4620-b548-44b3e5131cbe sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-13 09:10:38 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Member consumer-contacts-test-group-4-824bd2ff-c9ed-445b-b988-e6d39c49abbc sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-13 09:10:38 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Member consumer-contacts-test-group-2-6eacb86f-110c-46ee-a37d-7e45b8cc5b44 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-13 09:10:38 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-13 09:10:38 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-13 09:10:38 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-13 09:10:38 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-13 09:10:38 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-13 09:10:38 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-13 09:10:38 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-13 09:10:38 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-13 09:10:38 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-13 09:10:38 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-13 09:10:38 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-13 09:10:38 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-13 09:10:38 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-13 09:10:38 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-13 09:10:38 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-13 09:10:38 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-13 09:10:38 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-13 09:10:38 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-13 09:10:38 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-13 09:10:38 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-13 09:10:38 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-13 09:10:38 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-13 09:10:38 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-13 09:10:38 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-13 09:10:38 [Worker-5] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-5 unregistered
2025-04-13 09:10:38 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-13 09:10:38 [Worker-6] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:38 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Member consumer-contacts-test-group-6-d1b429d0-45bd-4f10-9b2b-13cad52d8a32 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-13 09:10:38 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-13 09:10:38 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-13 09:10:38 [Worker-2] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-2 unregistered
2025-04-13 09:10:38 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-13 09:10:38 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-13 09:10:38 [Worker-4] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-4 unregistered
2025-04-13 09:10:38 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-13 09:10:38 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-13 09:10:38 [Worker-3] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-3 unregistered
2025-04-13 09:10:38 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-13 09:10:39 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-13 09:10:39 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-13 09:10:39 [Worker-6] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-6 unregistered
2025-04-13 09:10:39 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "I’m very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}%n
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 09:10:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-32
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-32] Instantiated an idempotent producer.
2025-04-13 09:10:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535439302
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_91', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 09:10:39 [kafka-producer-network-thread | producer-32] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-32] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:39 [kafka-producer-network-thread | producer-32] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-32] ProducerId set to 9171 with epoch 0
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_6, value: {"_id": {"$oid": "67f10b97558ab0f6396b1410"}, "review": "I’m very happy with this purchase!", "authId": "user_6", "datetime": "2025-01-21T18:03:15.045Z", "sentiment": null}, offset: 70761
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_6
2025-04-13 09:10:39 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:39 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1410"}, "review": "I’m very happy with this purchase!", "authId": "user_6", "datetime": "2025-01-21T18:03:15.045Z", "sentiment": null}%n
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 09:10:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-33
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-33] Instantiated an idempotent producer.
2025-04-13 09:10:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535439819
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_6', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 09:10:39 [kafka-producer-network-thread | producer-33] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-33] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:39 [kafka-producer-network-thread | producer-33] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-33] ProducerId set to 9172 with epoch 0
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_92, value: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}, offset: 70762
2025-04-13 09:10:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_92
2025-04-13 09:10:39 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:39 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:40 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 09:10:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}%n
2025-04-13 09:10:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 09:10:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 09:10:41 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-34
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:41 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:41 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-34] Instantiated an idempotent producer.
2025-04-13 09:10:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535441283
2025-04-13 09:10:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:41 [kafka-producer-network-thread | producer-34] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-34] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_92', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 09:10:41 [kafka-producer-network-thread | producer-34] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-34] ProducerId set to 9173 with epoch 0
2025-04-13 09:10:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:41 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_52, value: {"_id": {"$oid": "67f10b97558ab0f6396b143e"}, "review": "This changed my life!", "authId": "user_52", "datetime": "2025-01-10T06:57:54.745Z", "sentiment": null}, offset: 70763
2025-04-13 09:10:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_52
2025-04-13 09:10:41 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:41 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143e"}, "review": "This changed my life!", "authId": "user_52", "datetime": "2025-01-10T06:57:54.745Z", "sentiment": null}%n
2025-04-13 09:10:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 09:10:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 09:10:42 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-35
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:42 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:42 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-35] Instantiated an idempotent producer.
2025-04-13 09:10:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535442785
2025-04-13 09:10:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_52', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 09:10:42 [kafka-producer-network-thread | producer-35] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-35] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:42 [kafka-producer-network-thread | producer-35] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-35] ProducerId set to 9174 with epoch 0
2025-04-13 09:10:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:42 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_96, value: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}, offset: 70764
2025-04-13 09:10:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_96
2025-04-13 09:10:42 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:42 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}%n
2025-04-13 09:10:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 09:10:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 09:10:43 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-36
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:43 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:43 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-36] Instantiated an idempotent producer.
2025-04-13 09:10:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535443317
2025-04-13 09:10:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:43 [kafka-producer-network-thread | producer-36] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-36] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_96', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 09:10:43 [kafka-producer-network-thread | producer-36] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-36] ProducerId set to 9175 with epoch 0
2025-04-13 09:10:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:43 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_99, value: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}, offset: 70765
2025-04-13 09:10:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_99
2025-04-13 09:10:43 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:43 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:43 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 09:10:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}%n
2025-04-13 09:10:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 09:10:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 09:10:44 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-37
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:44 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:44 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-37] Instantiated an idempotent producer.
2025-04-13 09:10:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535444799
2025-04-13 09:10:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_99', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 09:10:44 [kafka-producer-network-thread | producer-37] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-37] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:44 [kafka-producer-network-thread | producer-37] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-37] ProducerId set to 9176 with epoch 0
2025-04-13 09:10:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_93, value: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}, offset: 70766
2025-04-13 09:10:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_93
2025-04-13 09:10:44 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:44 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}%n
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 09:10:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-38
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-38] Instantiated an idempotent producer.
2025-04-13 09:10:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535445408
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:45 [kafka-producer-network-thread | producer-38] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-38] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_93', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 09:10:45 [kafka-producer-network-thread | producer-38] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-38] ProducerId set to 9177 with epoch 0
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_47, value: {"_id": {"$oid": "67f10b97558ab0f6396b1439"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_47", "datetime": "2025-01-08T17:35:19.231Z", "sentiment": null}, offset: 70767
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_47
2025-04-13 09:10:45 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:45 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1439"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_47", "datetime": "2025-01-08T17:35:19.231Z", "sentiment": null}%n
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.8
score: 0.4

2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.8
score: 0.4

2025-04-13 09:10:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-39
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-39] Instantiated an idempotent producer.
2025-04-13 09:10:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535445976
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_47', sentiment=magnitude: 0.8
score: 0.4
}
2025-04-13 09:10:45 [kafka-producer-network-thread | producer-39] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-39] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:45 [kafka-producer-network-thread | producer-39] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-39] ProducerId set to 9178 with epoch 0
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_5, value: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}, offset: 70768
2025-04-13 09:10:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_5
2025-04-13 09:10:45 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:45 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:46 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 09:10:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}%n
2025-04-13 09:10:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 09:10:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 09:10:47 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-40
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 09:10:47 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 09:10:47 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-40] Instantiated an idempotent producer.
2025-04-13 09:10:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 09:10:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 09:10:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744535447482
2025-04-13 09:10:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 09:10:47 [kafka-producer-network-thread | producer-40] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-40] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 09:10:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_5', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 09:10:47 [kafka-producer-network-thread | producer-40] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-40] ProducerId set to 9179 with epoch 0
2025-04-13 09:10:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 09:10:47 [Worker-1] WARN  o.s.t.k.consumer.ConsumerTemplate - Failed to commit offsets, retry 1/3
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:231)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:215)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:1116)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.commitSync(LegacyKafkaConsumer.java:719)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.commitSync(LegacyKafkaConsumer.java:704)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.commitSync(LegacyKafkaConsumer.java:699)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:918)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.commitOffsets(ConsumerTemplate.java:106)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:71)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:49 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 09:10:49 [Worker-1] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-13 09:10:49 [Worker-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Revoke previously assigned partitions test-topic-0
2025-04-13 09:10:49 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Member consumer-contacts-test-group-1-2b9697ac-9d85-45b3-91d7-3fa2fde2d9c7 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-13 09:10:49 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-13 09:10:49 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-13 09:10:49 [Worker-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-13 09:10:49 [Worker-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-13 09:10:49 [Worker-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-13 09:10:49 [Worker-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-13 09:10:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-1 unregistered
2025-04-13 09:10:49 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-13 09:10:49 [Thread-6] INFO  o.s.t.kafka.consumer.ConsumerManager - All consumers have been shut down.
