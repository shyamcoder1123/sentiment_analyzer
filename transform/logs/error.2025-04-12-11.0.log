2025-04-12 11:17:08 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Starting 6 consumers for the 'contacts' topic...
2025-04-12 11:17:08 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:17:08 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:08 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:08 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:08 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456628856
2025-04-12 11:17:08 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:17:08 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:08 [Worker-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:17:08 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:17:08 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:08 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:08 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456628903
2025-04-12 11:17:08 [Worker-2] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:17:08 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:17:08 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:17:08 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:08 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:08 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:08 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456628942
2025-04-12 11:17:08 [Worker-3] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:17:08 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:17:08 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:17:08 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:08 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:08 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:08 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456628976
2025-04-12 11:17:08 [Worker-4] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:17:08 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:17:08 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:17:08 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:09 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:09 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:09 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456629002
2025-04-12 11:17:09 [Worker-5] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:17:09 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:17:09 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:17:09 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:09 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:09 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:09 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456629022
2025-04-12 11:17:09 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Started 6 consumers for the 'contacts' topic.
2025-04-12 11:17:09 [Worker-6] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:17:09 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:17:09 [Worker-4] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:09 [Worker-5] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:09 [Worker-6] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:09 [Worker-3] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:09 [Worker-2] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:09 [Worker-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:09 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:17:09 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:17:09 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:17:09 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:17:09 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:17:09 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:17:09 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:17:09 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:17:09 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:17:09 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:17:09 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:17:09 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:17:09 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-3-f496c32f-8a57-4b3b-8a86-9164593b9c4e
2025-04-12 11:17:09 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-5-4aea7b81-204e-4045-8576-0ef93f339537
2025-04-12 11:17:09 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-1-0c520021-cc13-44c5-97ab-7bae172dd05c
2025-04-12 11:17:09 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-4-f9b52196-08f9-4fc5-9a47-b914ee566dbc
2025-04-12 11:17:09 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-6-7df5e43d-7730-4da4-9707-3b991b17c038
2025-04-12 11:17:09 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:17:09 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:17:09 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:17:09 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:17:09 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:17:09 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-2-db660109-d8a4-4d4c-8053-19d63a8f8cba
2025-04-12 11:17:09 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:17:12 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=85, memberId='consumer-contacts-test-group-1-0c520021-cc13-44c5-97ab-7bae172dd05c', protocol='range'}
2025-04-12 11:17:12 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=85, memberId='consumer-contacts-test-group-2-db660109-d8a4-4d4c-8053-19d63a8f8cba', protocol='range'}
2025-04-12 11:17:12 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=85, memberId='consumer-contacts-test-group-3-f496c32f-8a57-4b3b-8a86-9164593b9c4e', protocol='range'}
2025-04-12 11:17:12 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=85, memberId='consumer-contacts-test-group-4-f9b52196-08f9-4fc5-9a47-b914ee566dbc', protocol='range'}
2025-04-12 11:17:13 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=85, memberId='consumer-contacts-test-group-5-4aea7b81-204e-4045-8576-0ef93f339537', protocol='range'}
2025-04-12 11:17:13 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Finished assignment for group at generation 85: {consumer-contacts-test-group-6-7df5e43d-7730-4da4-9707-3b991b17c038=Assignment(partitions=[]), consumer-contacts-test-group-1-0c520021-cc13-44c5-97ab-7bae172dd05c=Assignment(partitions=[test-topic-0]), consumer-contacts-test-group-5-4aea7b81-204e-4045-8576-0ef93f339537=Assignment(partitions=[]), consumer-contacts-test-group-3-f496c32f-8a57-4b3b-8a86-9164593b9c4e=Assignment(partitions=[]), consumer-contacts-test-group-4-f9b52196-08f9-4fc5-9a47-b914ee566dbc=Assignment(partitions=[]), consumer-contacts-test-group-2-db660109-d8a4-4d4c-8053-19d63a8f8cba=Assignment(partitions=[])}
2025-04-12 11:17:13 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=85, memberId='consumer-contacts-test-group-6-7df5e43d-7730-4da4-9707-3b991b17c038', protocol='range'}
2025-04-12 11:17:13 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=85, memberId='consumer-contacts-test-group-1-0c520021-cc13-44c5-97ab-7bae172dd05c', protocol='range'}
2025-04-12 11:17:13 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=85, memberId='consumer-contacts-test-group-5-4aea7b81-204e-4045-8576-0ef93f339537', protocol='range'}
2025-04-12 11:17:13 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=85, memberId='consumer-contacts-test-group-4-f9b52196-08f9-4fc5-9a47-b914ee566dbc', protocol='range'}
2025-04-12 11:17:13 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=85, memberId='consumer-contacts-test-group-6-7df5e43d-7730-4da4-9707-3b991b17c038', protocol='range'}
2025-04-12 11:17:13 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=85, memberId='consumer-contacts-test-group-3-f496c32f-8a57-4b3b-8a86-9164593b9c4e', protocol='range'}
2025-04-12 11:17:13 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=85, memberId='consumer-contacts-test-group-2-db660109-d8a4-4d4c-8053-19d63a8f8cba', protocol='range'}
2025-04-12 11:17:13 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-12 11:17:13 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:17:13 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:17:13 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:17:13 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:17:13 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:17:13 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:17:13 [Worker-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:17:13 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:17:13 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:17:13 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:17:13 [Worker-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-12 11:17:13 [Worker-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=70349, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=12}}
2025-04-12 11:17:13 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_70, value: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}, offset: 70349
2025-04-12 11:17:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_70
2025-04-12 11:17:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}%n
2025-04-12 11:17:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 11:17:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 11:17:16 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:16 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:16 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-04-12 11:17:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456637022
2025-04-12 11:17:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_70', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 11:17:17 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:17 [kafka-producer-network-thread | producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 7060 with epoch 0
2025-04-12 11:17:17 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:17 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_81, value: {"_id": {"$oid": "67f10b97558ab0f6396b145b"}, "review": "Not bad, not great either.", "authId": "user_81", "datetime": "2025-03-11T12:19:55.240Z", "sentiment": null}, offset: 70350
2025-04-12 11:17:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_81
2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145b"}, "review": "Not bad, not great either.", "authId": "user_81", "datetime": "2025-03-11T12:19:55.240Z", "sentiment": null}%n
2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 11:17:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-2] Instantiated an idempotent producer.
2025-04-12 11:17:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456638107
2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_81', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 11:17:18 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-2] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:18 [kafka-producer-network-thread | producer-2] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-2] ProducerId set to 7061 with epoch 0
2025-04-12 11:17:18 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_24, value: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}, offset: 70351
2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_24
2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}%n
2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-12 11:17:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-3] Instantiated an idempotent producer.
2025-04-12 11:17:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456638762
2025-04-12 11:17:18 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-3] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:18 [kafka-producer-network-thread | producer-3] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-3] ProducerId set to 7062 with epoch 0
2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_24', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-12 11:17:18 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 70352
2025-04-12 11:17:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-12 11:17:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-12 11:17:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 11:17:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 11:17:19 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:19 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:19 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-4] Instantiated an idempotent producer.
2025-04-12 11:17:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456639420
2025-04-12 11:17:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 11:17:19 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-4] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:19 [kafka-producer-network-thread | producer-4] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-4] ProducerId set to 7063 with epoch 0
2025-04-12 11:17:19 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:19 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_48, value: {"_id": {"$oid": "67f10b97558ab0f6396b143a"}, "review": "Terrible experience, very disappointed.", "authId": "user_48", "datetime": "2025-01-15T02:57:46.191Z", "sentiment": null}, offset: 70353
2025-04-12 11:17:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_48
2025-04-12 11:17:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143a"}, "review": "Terrible experience, very disappointed.", "authId": "user_48", "datetime": "2025-01-15T02:57:46.191Z", "sentiment": null}%n
2025-04-12 11:17:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:21 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:21 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:21 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-5] Instantiated an idempotent producer.
2025-04-12 11:17:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456641151
2025-04-12 11:17:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_48', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:21 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-5] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:21 [kafka-producer-network-thread | producer-5] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-5] ProducerId set to 7064 with epoch 0
2025-04-12 11:17:21 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:21 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "I’m very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 70354
2025-04-12 11:17:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-12 11:17:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "I’m very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}%n
2025-04-12 11:17:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 11:17:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 11:17:22 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:22 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:22 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-6] Instantiated an idempotent producer.
2025-04-12 11:17:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456642702
2025-04-12 11:17:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_91', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 11:17:22 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-6] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:22 [kafka-producer-network-thread | producer-6] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-6] ProducerId set to 7065 with epoch 0
2025-04-12 11:17:22 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:22 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_1, value: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}, offset: 70355
2025-04-12 11:17:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_1
2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}%n
2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:23 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:23 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:23 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-7] Instantiated an idempotent producer.
2025-04-12 11:17:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456643307
2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_1', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:23 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-7] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:23 [kafka-producer-network-thread | producer-7] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-7] ProducerId set to 7066 with epoch 0
2025-04-12 11:17:23 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_78, value: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}, offset: 70356
2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_78
2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}%n
2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 11:17:23 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:23 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:23 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-8] Instantiated an idempotent producer.
2025-04-12 11:17:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456643898
2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_78', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 11:17:23 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-8] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:23 [kafka-producer-network-thread | producer-8] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-8] ProducerId set to 7067 with epoch 0
2025-04-12 11:17:23 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_19, value: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}, offset: 70357
2025-04-12 11:17:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_19
2025-04-12 11:17:25 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}%n
2025-04-12 11:17:25 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-12 11:17:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-12 11:17:25 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:25 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:25 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-9] Instantiated an idempotent producer.
2025-04-12 11:17:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456645484
2025-04-12 11:17:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_19', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-12 11:17:25 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-9] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:25 [kafka-producer-network-thread | producer-9] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-9] ProducerId set to 7068 with epoch 0
2025-04-12 11:17:25 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:25 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 70358
2025-04-12 11:17:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-10] Instantiated an idempotent producer.
2025-04-12 11:17:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456646052
2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:26 [kafka-producer-network-thread | producer-10] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-10] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:26 [kafka-producer-network-thread | producer-10] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-10] ProducerId set to 7069 with epoch 0
2025-04-12 11:17:26 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_97, value: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}, offset: 70359
2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_97
2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}%n
2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-11] Instantiated an idempotent producer.
2025-04-12 11:17:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456646684
2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_97', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:26 [kafka-producer-network-thread | producer-11] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-11] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:26 [kafka-producer-network-thread | producer-11] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-11] ProducerId set to 7070 with epoch 0
2025-04-12 11:17:26 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_42, value: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}, offset: 70360
2025-04-12 11:17:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_42
2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}%n
2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:28 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:28 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:28 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-12] Instantiated an idempotent producer.
2025-04-12 11:17:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456648265
2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_42', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:28 [kafka-producer-network-thread | producer-12] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-12] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:28 [kafka-producer-network-thread | producer-12] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-12] ProducerId set to 7071 with epoch 0
2025-04-12 11:17:28 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_34, value: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}, offset: 70361
2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_34
2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}%n
2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 11:17:28 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:28 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:28 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-13] Instantiated an idempotent producer.
2025-04-12 11:17:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456648826
2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_34', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 11:17:28 [kafka-producer-network-thread | producer-13] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-13] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:28 [kafka-producer-network-thread | producer-13] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-13] ProducerId set to 7072 with epoch 0
2025-04-12 11:17:28 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_92, value: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}, offset: 70362
2025-04-12 11:17:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_92
2025-04-12 11:17:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}%n
2025-04-12 11:17:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 11:17:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 11:17:30 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:30 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:30 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-14] Instantiated an idempotent producer.
2025-04-12 11:17:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456650383
2025-04-12 11:17:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_92', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 11:17:30 [kafka-producer-network-thread | producer-14] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-14] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:30 [kafka-producer-network-thread | producer-14] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-14] ProducerId set to 7073 with epoch 0
2025-04-12 11:17:30 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:30 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_64, value: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}, offset: 70363
2025-04-12 11:17:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_64
2025-04-12 11:17:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}%n
2025-04-12 11:17:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-15] Instantiated an idempotent producer.
2025-04-12 11:17:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456652038
2025-04-12 11:17:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_64', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:32 [kafka-producer-network-thread | producer-15] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-15] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:32 [kafka-producer-network-thread | producer-15] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-15] ProducerId set to 7074 with epoch 0
2025-04-12 11:17:32 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_3, value: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}, offset: 70364
2025-04-12 11:17:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_3
2025-04-12 11:17:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}%n
2025-04-12 11:17:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 11:17:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 11:17:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-16] Instantiated an idempotent producer.
2025-04-12 11:17:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456653652
2025-04-12 11:17:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_3', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 11:17:33 [kafka-producer-network-thread | producer-16] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-16] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:33 [kafka-producer-network-thread | producer-16] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-16] ProducerId set to 7075 with epoch 0
2025-04-12 11:17:33 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_64, value: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}, offset: 70365
2025-04-12 11:17:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_64
2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}%n
2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-17] Instantiated an idempotent producer.
2025-04-12 11:17:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456655165
2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_64', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:35 [kafka-producer-network-thread | producer-17] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-17] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:35 [kafka-producer-network-thread | producer-17] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-17] ProducerId set to 7076 with epoch 0
2025-04-12 11:17:35 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_56, value: {"_id": {"$oid": "67f10b97558ab0f6396b1442"}, "review": "It's okay, does the job.", "authId": "user_56", "datetime": "2024-12-14T20:56:35.004Z", "sentiment": null}, offset: 70366
2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_56
2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1442"}, "review": "It's okay, does the job.", "authId": "user_56", "datetime": "2024-12-14T20:56:35.004Z", "sentiment": null}%n
2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-12 11:17:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-18] Instantiated an idempotent producer.
2025-04-12 11:17:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456655704
2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_56', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-12 11:17:35 [kafka-producer-network-thread | producer-18] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-18] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:35 [kafka-producer-network-thread | producer-18] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-18] ProducerId set to 7077 with epoch 0
2025-04-12 11:17:35 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_41, value: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}, offset: 70367
2025-04-12 11:17:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_41
2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}%n
2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:37 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:37 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:37 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-19] Instantiated an idempotent producer.
2025-04-12 11:17:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456657234
2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_41', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:37 [kafka-producer-network-thread | producer-19] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-19] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:37 [kafka-producer-network-thread | producer-19] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-19] ProducerId set to 7078 with epoch 0
2025-04-12 11:17:37 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_11, value: {"_id": {"$oid": "67f10b97558ab0f6396b1415"}, "review": "Would not recommend to anyone.", "authId": "user_11", "datetime": "2024-12-10T23:20:11.570Z", "sentiment": null}, offset: 70368
2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_11
2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1415"}, "review": "Would not recommend to anyone.", "authId": "user_11", "datetime": "2024-12-10T23:20:11.570Z", "sentiment": null}%n
2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:37 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:37 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:37 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-20] Instantiated an idempotent producer.
2025-04-12 11:17:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456657821
2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_11', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:37 [kafka-producer-network-thread | producer-20] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-20] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:37 [kafka-producer-network-thread | producer-20] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-20] ProducerId set to 7079 with epoch 0
2025-04-12 11:17:37 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 70369
2025-04-12 11:17:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 11:17:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-21] Instantiated an idempotent producer.
2025-04-12 11:17:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456659407
2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 11:17:39 [kafka-producer-network-thread | producer-21] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-21] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:39 [kafka-producer-network-thread | producer-21] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-21] ProducerId set to 7080 with epoch 0
2025-04-12 11:17:39 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_66, value: {"_id": {"$oid": "67f10b97558ab0f6396b144c"}, "review": "Great quality, will buy again.", "authId": "user_66", "datetime": "2025-02-01T17:37:18.002Z", "sentiment": null}, offset: 70370
2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_66
2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144c"}, "review": "Great quality, will buy again.", "authId": "user_66", "datetime": "2025-02-01T17:37:18.002Z", "sentiment": null}%n
2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: 0.1

2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: 0.1

2025-04-12 11:17:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-22] Instantiated an idempotent producer.
2025-04-12 11:17:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456659978
2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_66', sentiment=magnitude: 0.1
score: 0.1
}
2025-04-12 11:17:39 [kafka-producer-network-thread | producer-22] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-22] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:39 [kafka-producer-network-thread | producer-22] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-22] ProducerId set to 7081 with epoch 0
2025-04-12 11:17:39 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_90, value: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}, offset: 70371
2025-04-12 11:17:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_90
2025-04-12 11:17:41 [Worker-5] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:41 [Worker-5] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:41 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Member consumer-contacts-test-group-5-4aea7b81-204e-4045-8576-0ef93f339537 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:17:41 [Worker-2] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:41 [Worker-3] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:41 [Worker-2] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:41 [Worker-3] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:41 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:17:41 [Thread-6] INFO  o.s.t.kafka.consumer.ConsumerManager - Shutting down consumers...
2025-04-12 11:17:41 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Member consumer-contacts-test-group-2-db660109-d8a4-4d4c-8053-19d63a8f8cba sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:17:41 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Member consumer-contacts-test-group-3-f496c32f-8a57-4b3b-8a86-9164593b9c4e sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:17:41 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:17:41 [Worker-6] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:41 [Worker-4] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:41 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:17:41 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:17:41 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:17:41 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:17:41 [Worker-6] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:41 [Worker-4] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:41 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:17:41 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:17:41 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Member consumer-contacts-test-group-6-7df5e43d-7730-4da4-9707-3b991b17c038 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:17:41 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:17:41 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Member consumer-contacts-test-group-4-f9b52196-08f9-4fc5-9a47-b914ee566dbc sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:17:41 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:17:41 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:17:41 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:17:41 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:17:41 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:17:41 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:17:41 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:17:41 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:17:41 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:17:41 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:17:41 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:17:41 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:17:41 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:17:41 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:17:41 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:17:41 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:17:41 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:17:41 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:17:41 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:17:41 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:17:41 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:17:41 [Worker-3] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-3 unregistered
2025-04-12 11:17:41 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:17:41 [Worker-5] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-5 unregistered
2025-04-12 11:17:41 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:17:41 [Worker-2] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-2 unregistered
2025-04-12 11:17:41 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:17:41 [Worker-4] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-4 unregistered
2025-04-12 11:17:41 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:17:41 [Worker-6] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-6 unregistered
2025-04-12 11:17:41 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:17:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}%n
2025-04-12 11:17:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: -0.1

2025-04-12 11:17:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: -0.1

2025-04-12 11:17:41 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-23
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:41 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:41 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-23] Instantiated an idempotent producer.
2025-04-12 11:17:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456661513
2025-04-12 11:17:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_90', sentiment=magnitude: 0.1
score: -0.1
}
2025-04-12 11:17:41 [kafka-producer-network-thread | producer-23] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-23] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:41 [kafka-producer-network-thread | producer-23] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-23] ProducerId set to 7082 with epoch 0
2025-04-12 11:17:41 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:41 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70372
2025-04-12 11:17:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-12 11:17:41 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:41 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}%n
2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 11:17:43 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-24
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:43 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:43 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-24] Instantiated an idempotent producer.
2025-04-12 11:17:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456663053
2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_50', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 11:17:43 [kafka-producer-network-thread | producer-24] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-24] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:43 [kafka-producer-network-thread | producer-24] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-24] ProducerId set to 7083 with epoch 0
2025-04-12 11:17:43 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_93, value: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}, offset: 70373
2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_93
2025-04-12 11:17:43 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:43 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:43 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}%n
2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:43 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-25
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:43 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:43 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-25] Instantiated an idempotent producer.
2025-04-12 11:17:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456663622
2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_93', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:43 [kafka-producer-network-thread | producer-25] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-25] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:43 [kafka-producer-network-thread | producer-25] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-25] ProducerId set to 7084 with epoch 0
2025-04-12 11:17:43 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_63, value: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}, offset: 70374
2025-04-12 11:17:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_63
2025-04-12 11:17:43 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:43 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}%n
2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 11:17:44 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-26
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:44 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:44 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-26] Instantiated an idempotent producer.
2025-04-12 11:17:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456664186
2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_63', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 11:17:44 [kafka-producer-network-thread | producer-26] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-26] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:44 [kafka-producer-network-thread | producer-26] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-26] ProducerId set to 7085 with epoch 0
2025-04-12 11:17:44 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_34, value: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}, offset: 70375
2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_34
2025-04-12 11:17:44 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:44 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}%n
2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 11:17:44 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-27
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:44 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:44 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-27] Instantiated an idempotent producer.
2025-04-12 11:17:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456664752
2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_34', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 11:17:44 [kafka-producer-network-thread | producer-27] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-27] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:44 [kafka-producer-network-thread | producer-27] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-27] ProducerId set to 7086 with epoch 0
2025-04-12 11:17:44 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_98, value: {"_id": {"$oid": "67f10b97558ab0f6396b146c"}, "review": "Not bad, not great either.", "authId": "user_98", "datetime": "2025-03-21T23:54:21.817Z", "sentiment": null}, offset: 70376
2025-04-12 11:17:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_98
2025-04-12 11:17:44 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:44 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:46 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146c"}, "review": "Not bad, not great either.", "authId": "user_98", "datetime": "2025-03-21T23:54:21.817Z", "sentiment": null}%n
2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 11:17:46 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-28
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:46 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:46 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-28] Instantiated an idempotent producer.
2025-04-12 11:17:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456666307
2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_98', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 11:17:46 [kafka-producer-network-thread | producer-28] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-28] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:46 [kafka-producer-network-thread | producer-28] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-28] ProducerId set to 7087 with epoch 0
2025-04-12 11:17:46 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_64, value: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}, offset: 70377
2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_64
2025-04-12 11:17:46 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:46 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}%n
2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:46 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-29
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:46 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:46 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-29] Instantiated an idempotent producer.
2025-04-12 11:17:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456666888
2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_64', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:46 [kafka-producer-network-thread | producer-29] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-29] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:46 [kafka-producer-network-thread | producer-29] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-29] ProducerId set to 7088 with epoch 0
2025-04-12 11:17:46 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_37, value: {"_id": {"$oid": "67f10b97558ab0f6396b142f"}, "review": "Mediocre quality, nothing special.", "authId": "user_37", "datetime": "2024-12-22T11:29:54.974Z", "sentiment": null}, offset: 70378
2025-04-12 11:17:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_37
2025-04-12 11:17:46 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:46 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142f"}, "review": "Mediocre quality, nothing special.", "authId": "user_37", "datetime": "2024-12-22T11:29:54.974Z", "sentiment": null}%n
2025-04-12 11:17:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:48 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-30
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:48 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:48 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-30] Instantiated an idempotent producer.
2025-04-12 11:17:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:48 [kafka-producer-network-thread | producer-30] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-30] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456668440
2025-04-12 11:17:48 [kafka-producer-network-thread | producer-30] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-30] ProducerId set to 7089 with epoch 0
2025-04-12 11:17:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_37', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:48 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:48 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_32, value: {"_id": {"$oid": "67f10b97558ab0f6396b142a"}, "review": "Fantastic performance and very durable.", "authId": "user_32", "datetime": "2025-01-03T02:26:59.600Z", "sentiment": null}, offset: 70379
2025-04-12 11:17:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_32
2025-04-12 11:17:48 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:48 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:49 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 11:17:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142a"}, "review": "Fantastic performance and very durable.", "authId": "user_32", "datetime": "2025-01-03T02:26:59.600Z", "sentiment": null}%n
2025-04-12 11:17:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 11:17:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 11:17:50 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-31
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:50 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:50 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-31] Instantiated an idempotent producer.
2025-04-12 11:17:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456670064
2025-04-12 11:17:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_32', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 11:17:50 [kafka-producer-network-thread | producer-31] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-31] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:50 [kafka-producer-network-thread | producer-31] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-31] ProducerId set to 7090 with epoch 0
2025-04-12 11:17:50 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:50 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_93, value: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}, offset: 70380
2025-04-12 11:17:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_93
2025-04-12 11:17:50 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:50 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1467"}, "review": "Doesn't work as advertised.", "authId": "user_93", "datetime": "2025-01-02T18:13:11.127Z", "sentiment": null}%n
2025-04-12 11:17:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:17:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:17:51 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-32
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:51 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:51 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-32] Instantiated an idempotent producer.
2025-04-12 11:17:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456671678
2025-04-12 11:17:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_93', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:17:51 [kafka-producer-network-thread | producer-32] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-32] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:51 [kafka-producer-network-thread | producer-32] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-32] ProducerId set to 7091 with epoch 0
2025-04-12 11:17:51 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:51 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_67, value: {"_id": {"$oid": "67f10b97558ab0f6396b144d"}, "review": "Works like a charm, perfect!", "authId": "user_67", "datetime": "2025-02-10T15:03:00.523Z", "sentiment": null}, offset: 70381
2025-04-12 11:17:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_67
2025-04-12 11:17:51 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:51 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:52 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 11:17:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144d"}, "review": "Works like a charm, perfect!", "authId": "user_67", "datetime": "2025-02-10T15:03:00.523Z", "sentiment": null}%n
2025-04-12 11:17:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 11:17:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 11:17:53 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-33
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:17:53 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:17:53 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-33] Instantiated an idempotent producer.
2025-04-12 11:17:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:17:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:17:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744456673265
2025-04-12 11:17:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:17:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_67', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 11:17:53 [kafka-producer-network-thread | producer-33] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-33] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:17:53 [kafka-producer-network-thread | producer-33] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-33] ProducerId set to 7092 with epoch 0
2025-04-12 11:17:53 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.google.protobuf.UnknownFieldSet$Parser and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"]->com.google.cloud.language.v1.Sentiment["unknownFields"]->com.google.protobuf.UnknownFieldSet["parserForType"])
	at com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:77)
	at com.fasterxml.jackson.databind.SerializerProvider.reportBadDefinition(SerializerProvider.java:1330)
	at com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:53)
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:30)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:53 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_79, value: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}, offset: 70382
2025-04-12 11:17:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_79
2025-04-12 11:17:53 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:17:53 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:39 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Starting 6 consumers for the 'contacts' topic...
2025-04-12 11:35:39 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:35:39 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:39 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:39 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:39 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457739985
2025-04-12 11:35:39 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:35:40 [Worker-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:35:40 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:40 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457740041
2025-04-12 11:35:40 [Worker-2] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:35:40 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:35:40 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:35:40 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457740072
2025-04-12 11:35:40 [Worker-3] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:35:40 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:35:40 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:35:40 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457740106
2025-04-12 11:35:40 [Worker-4] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:35:40 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:35:40 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:35:40 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457740133
2025-04-12 11:35:40 [Worker-5] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:35:40 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:35:40 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:35:40 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:40 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457740159
2025-04-12 11:35:40 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Started 6 consumers for the 'contacts' topic.
2025-04-12 11:35:40 [Worker-6] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:35:40 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:35:40 [Worker-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:40 [Worker-2] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:40 [Worker-5] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:40 [Worker-3] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:40 [Worker-6] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:40 [Worker-4] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:40 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:35:40 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:35:40 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:35:40 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:35:40 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:35:40 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:35:40 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:35:40 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:35:40 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:35:40 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:35:40 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:35:40 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:35:40 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-4-e48f7b63-1505-4094-bfcd-323b2844bca6
2025-04-12 11:35:40 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-3-59740a32-dc5e-41df-9684-c904e25fe112
2025-04-12 11:35:40 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-5-4e9179ec-36ff-468a-892c-a3d0a571d9a6
2025-04-12 11:35:40 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-1-3c26ebe9-640c-4fbd-a0a8-84410d52b97b
2025-04-12 11:35:40 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-6-82e7fe3c-51d5-48f3-bb71-a266f16108a6
2025-04-12 11:35:40 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:35:40 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-2-c620e547-bd88-420a-86a0-a9c1e9336d15
2025-04-12 11:35:40 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:35:40 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:35:40 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:35:40 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:35:40 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:35:44 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=87, memberId='consumer-contacts-test-group-1-3c26ebe9-640c-4fbd-a0a8-84410d52b97b', protocol='range'}
2025-04-12 11:35:44 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=87, memberId='consumer-contacts-test-group-2-c620e547-bd88-420a-86a0-a9c1e9336d15', protocol='range'}
2025-04-12 11:35:44 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=87, memberId='consumer-contacts-test-group-3-59740a32-dc5e-41df-9684-c904e25fe112', protocol='range'}
2025-04-12 11:35:44 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=87, memberId='consumer-contacts-test-group-4-e48f7b63-1505-4094-bfcd-323b2844bca6', protocol='range'}
2025-04-12 11:35:44 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=87, memberId='consumer-contacts-test-group-5-4e9179ec-36ff-468a-892c-a3d0a571d9a6', protocol='range'}
2025-04-12 11:35:44 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Finished assignment for group at generation 87: {consumer-contacts-test-group-6-82e7fe3c-51d5-48f3-bb71-a266f16108a6=Assignment(partitions=[]), consumer-contacts-test-group-5-4e9179ec-36ff-468a-892c-a3d0a571d9a6=Assignment(partitions=[]), consumer-contacts-test-group-3-59740a32-dc5e-41df-9684-c904e25fe112=Assignment(partitions=[]), consumer-contacts-test-group-2-c620e547-bd88-420a-86a0-a9c1e9336d15=Assignment(partitions=[]), consumer-contacts-test-group-4-e48f7b63-1505-4094-bfcd-323b2844bca6=Assignment(partitions=[]), consumer-contacts-test-group-1-3c26ebe9-640c-4fbd-a0a8-84410d52b97b=Assignment(partitions=[test-topic-0])}
2025-04-12 11:35:44 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=87, memberId='consumer-contacts-test-group-3-59740a32-dc5e-41df-9684-c904e25fe112', protocol='range'}
2025-04-12 11:35:44 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=87, memberId='consumer-contacts-test-group-1-3c26ebe9-640c-4fbd-a0a8-84410d52b97b', protocol='range'}
2025-04-12 11:35:44 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=87, memberId='consumer-contacts-test-group-2-c620e547-bd88-420a-86a0-a9c1e9336d15', protocol='range'}
2025-04-12 11:35:44 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=87, memberId='consumer-contacts-test-group-4-e48f7b63-1505-4094-bfcd-323b2844bca6', protocol='range'}
2025-04-12 11:35:44 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:35:44 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=87, memberId='consumer-contacts-test-group-5-4e9179ec-36ff-468a-892c-a3d0a571d9a6', protocol='range'}
2025-04-12 11:35:44 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-12 11:35:44 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:35:44 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:35:44 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:35:44 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=87, memberId='consumer-contacts-test-group-6-82e7fe3c-51d5-48f3-bb71-a266f16108a6', protocol='range'}
2025-04-12 11:35:44 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:35:44 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:35:44 [Worker-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:35:44 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:35:44 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=87, memberId='consumer-contacts-test-group-6-82e7fe3c-51d5-48f3-bb71-a266f16108a6', protocol='range'}
2025-04-12 11:35:44 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:35:44 [Worker-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-12 11:35:44 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:35:44 [Worker-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=70349, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=12}}
2025-04-12 11:35:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_70, value: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}, offset: 70349
2025-04-12 11:35:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_70
2025-04-12 11:35:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}%n
2025-04-12 11:35:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 11:35:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 11:35:48 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:35:48 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:48 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-04-12 11:35:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457748641
2025-04-12 11:35:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:35:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_70', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 11:35:48 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:48 [kafka-producer-network-thread | producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 7093 with epoch 0
2025-04-12 11:35:49 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.JsonMappingException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app') (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:402)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:361)
	at com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:323)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:778)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.ClassCastException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app')
	at org.shyam.transform.kafka.model.SentimentSerializer.serialize(SentimentSerializer.java:10)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	... 13 common frames omitted
2025-04-12 11:35:49 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_81, value: {"_id": {"$oid": "67f10b97558ab0f6396b145b"}, "review": "Not bad, not great either.", "authId": "user_81", "datetime": "2025-03-11T12:19:55.240Z", "sentiment": null}, offset: 70350
2025-04-12 11:35:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_81
2025-04-12 11:35:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145b"}, "review": "Not bad, not great either.", "authId": "user_81", "datetime": "2025-03-11T12:19:55.240Z", "sentiment": null}%n
2025-04-12 11:35:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 11:35:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 11:35:50 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:35:50 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:50 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-2] Instantiated an idempotent producer.
2025-04-12 11:35:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457750611
2025-04-12 11:35:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:35:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_81', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 11:35:50 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-2] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:50 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.JsonMappingException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app') (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:402)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:361)
	at com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:323)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:778)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.ClassCastException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app')
	at org.shyam.transform.kafka.model.SentimentSerializer.serialize(SentimentSerializer.java:10)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	... 13 common frames omitted
2025-04-12 11:35:50 [kafka-producer-network-thread | producer-2] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-2] ProducerId set to 7094 with epoch 0
2025-04-12 11:35:50 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_24, value: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}, offset: 70351
2025-04-12 11:35:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_24
2025-04-12 11:35:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}%n
2025-04-12 11:35:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-12 11:35:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-12 11:35:52 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:35:52 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:52 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-3] Instantiated an idempotent producer.
2025-04-12 11:35:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457752197
2025-04-12 11:35:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:35:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_24', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-12 11:35:52 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-3] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:52 [kafka-producer-network-thread | producer-3] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-3] ProducerId set to 7095 with epoch 0
2025-04-12 11:35:52 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.JsonMappingException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app') (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:402)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:361)
	at com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:323)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:778)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.ClassCastException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app')
	at org.shyam.transform.kafka.model.SentimentSerializer.serialize(SentimentSerializer.java:10)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	... 13 common frames omitted
2025-04-12 11:35:52 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 70352
2025-04-12 11:35:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-12 11:35:52 [Worker-6] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:52 [Worker-4] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:52 [Worker-2] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:52 [Worker-6] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:52 [Worker-4] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:52 [Worker-2] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:52 [Thread-6] INFO  o.s.t.kafka.consumer.ConsumerManager - Shutting down consumers...
2025-04-12 11:35:52 [Worker-5] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:52 [Worker-3] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:52 [Worker-5] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:52 [Worker-3] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:52 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Member consumer-contacts-test-group-2-c620e547-bd88-420a-86a0-a9c1e9336d15 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:35:52 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Member consumer-contacts-test-group-5-4e9179ec-36ff-468a-892c-a3d0a571d9a6 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:35:52 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Member consumer-contacts-test-group-4-e48f7b63-1505-4094-bfcd-323b2844bca6 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:35:52 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Member consumer-contacts-test-group-6-82e7fe3c-51d5-48f3-bb71-a266f16108a6 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:35:52 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Member consumer-contacts-test-group-3-59740a32-dc5e-41df-9684-c904e25fe112 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:35:52 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:35:52 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:35:52 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:35:52 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:35:52 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:35:52 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:35:52 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:35:52 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:35:52 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:35:52 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:35:52 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:35:52 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:35:52 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:35:52 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:35:52 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:35:52 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:35:52 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:35:52 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:35:52 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:35:52 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:35:52 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:35:52 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:35:52 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:35:52 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:35:52 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:35:52 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:35:52 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:35:52 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:35:52 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:35:52 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:35:52 [Worker-4] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-4 unregistered
2025-04-12 11:35:52 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:35:52 [Worker-5] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-5 unregistered
2025-04-12 11:35:52 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:35:52 [Worker-3] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-3 unregistered
2025-04-12 11:35:52 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:35:52 [Worker-2] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-2 unregistered
2025-04-12 11:35:52 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:35:52 [Worker-6] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-6 unregistered
2025-04-12 11:35:52 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:35:53 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 11:35:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-12 11:35:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 11:35:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 11:35:53 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:35:53 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:53 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-4] Instantiated an idempotent producer.
2025-04-12 11:35:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457753793
2025-04-12 11:35:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:35:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 11:35:53 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.JsonMappingException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app') (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:402)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:361)
	at com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:323)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:778)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.ClassCastException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app')
	at org.shyam.transform.kafka.model.SentimentSerializer.serialize(SentimentSerializer.java:10)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	... 13 common frames omitted
2025-04-12 11:35:53 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-4] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:53 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_48, value: {"_id": {"$oid": "67f10b97558ab0f6396b143a"}, "review": "Terrible experience, very disappointed.", "authId": "user_48", "datetime": "2025-01-15T02:57:46.191Z", "sentiment": null}, offset: 70353
2025-04-12 11:35:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_48
2025-04-12 11:35:53 [kafka-producer-network-thread | producer-4] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-4] ProducerId set to 7096 with epoch 0
2025-04-12 11:35:53 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:53 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143a"}, "review": "Terrible experience, very disappointed.", "authId": "user_48", "datetime": "2025-01-15T02:57:46.191Z", "sentiment": null}%n
2025-04-12 11:35:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:35:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:35:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:35:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-5] Instantiated an idempotent producer.
2025-04-12 11:35:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457755343
2025-04-12 11:35:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:35:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_48', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:35:55 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.JsonMappingException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app') (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:402)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:361)
	at com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:323)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:778)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.ClassCastException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app')
	at org.shyam.transform.kafka.model.SentimentSerializer.serialize(SentimentSerializer.java:10)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	... 13 common frames omitted
2025-04-12 11:35:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "I’m very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 70354
2025-04-12 11:35:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-12 11:35:55 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:55 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-5] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:55 [kafka-producer-network-thread | producer-5] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-5] ProducerId set to 7097 with epoch 0
2025-04-12 11:35:55 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:56 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 11:35:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "I’m very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}%n
2025-04-12 11:35:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 11:35:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 11:35:56 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:35:56 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:56 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-6] Instantiated an idempotent producer.
2025-04-12 11:35:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457756867
2025-04-12 11:35:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:35:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_91', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 11:35:56 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-6] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:56 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.JsonMappingException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app') (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:402)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:361)
	at com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:323)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:778)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.ClassCastException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app')
	at org.shyam.transform.kafka.model.SentimentSerializer.serialize(SentimentSerializer.java:10)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	... 13 common frames omitted
2025-04-12 11:35:56 [kafka-producer-network-thread | producer-6] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-6] ProducerId set to 7098 with epoch 0
2025-04-12 11:35:56 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_1, value: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}, offset: 70355
2025-04-12 11:35:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_1
2025-04-12 11:35:56 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:56 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}%n
2025-04-12 11:35:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:35:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:35:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:35:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-7] Instantiated an idempotent producer.
2025-04-12 11:35:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457758414
2025-04-12 11:35:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:35:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_1', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:35:58 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.JsonMappingException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app') (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:402)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:361)
	at com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:323)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:778)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.ClassCastException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app')
	at org.shyam.transform.kafka.model.SentimentSerializer.serialize(SentimentSerializer.java:10)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	... 13 common frames omitted
2025-04-12 11:35:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_78, value: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}, offset: 70356
2025-04-12 11:35:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_78
2025-04-12 11:35:58 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:58 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:58 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-7] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:58 [kafka-producer-network-thread | producer-7] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-7] ProducerId set to 7099 with epoch 0
2025-04-12 11:35:59 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 11:35:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}%n
2025-04-12 11:35:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 11:35:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 11:35:59 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:35:59 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:35:59 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-8] Instantiated an idempotent producer.
2025-04-12 11:35:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:35:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:35:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457759949
2025-04-12 11:35:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:35:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_78', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 11:35:59 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-8] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:35:59 [kafka-producer-network-thread | producer-8] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-8] ProducerId set to 7100 with epoch 0
2025-04-12 11:35:59 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.JsonMappingException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app') (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:402)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:361)
	at com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:323)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:778)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.ClassCastException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app')
	at org.shyam.transform.kafka.model.SentimentSerializer.serialize(SentimentSerializer.java:10)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	... 13 common frames omitted
2025-04-12 11:35:59 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_19, value: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}, offset: 70357
2025-04-12 11:35:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_19
2025-04-12 11:35:59 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:35:59 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:36:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}%n
2025-04-12 11:36:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-12 11:36:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-12 11:36:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:36:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:36:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-9] Instantiated an idempotent producer.
2025-04-12 11:36:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:36:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:36:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457761500
2025-04-12 11:36:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:36:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_19', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-12 11:36:01 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-9] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:36:01 [kafka-producer-network-thread | producer-9] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-9] ProducerId set to 7101 with epoch 0
2025-04-12 11:36:01 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.JsonMappingException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app') (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:402)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:361)
	at com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:323)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:778)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.ClassCastException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app')
	at org.shyam.transform.kafka.model.SentimentSerializer.serialize(SentimentSerializer.java:10)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	... 13 common frames omitted
2025-04-12 11:36:01 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 70358
2025-04-12 11:36:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-12 11:36:01 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:36:01 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:36:02 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 11:36:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-12 11:36:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:36:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:36:03 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:36:03 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:36:03 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-10] Instantiated an idempotent producer.
2025-04-12 11:36:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:36:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:36:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457763077
2025-04-12 11:36:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:36:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:36:03 [kafka-producer-network-thread | producer-10] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-10] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:36:03 [kafka-producer-network-thread | producer-10] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-10] ProducerId set to 7102 with epoch 0
2025-04-12 11:36:03 [Worker-1] ERROR o.s.t.kafka.producer.Producer - Failed to send ProcessedMessage to Kafka
com.fasterxml.jackson.databind.JsonMappingException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app') (through reference chain: org.shyam.transform.kafka.model.ProcessedMessage["sentiment"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:402)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:361)
	at com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:323)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:778)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:183)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:502)
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:341)
	at com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4793)
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:4038)
	at org.shyam.transform.kafka.producer.Producer.produce(Producer.java:49)
	at org.shyam.transform.kafka.producer.Producer.sentToTopic(Producer.java:32)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToKafka(ReviewsConsumer.java:47)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:42)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.lang.ClassCastException: class com.google.cloud.language.v1.Sentiment cannot be cast to class com.google.cloud.language.v2.Sentiment (com.google.cloud.language.v1.Sentiment and com.google.cloud.language.v2.Sentiment are in unnamed module of loader 'app')
	at org.shyam.transform.kafka.model.SentimentSerializer.serialize(SentimentSerializer.java:10)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:732)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:770)
	... 13 common frames omitted
2025-04-12 11:36:03 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_97, value: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}, offset: 70359
2025-04-12 11:36:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_97
2025-04-12 11:36:03 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:36:03 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:18 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Starting 6 consumers for the 'contacts' topic...
2025-04-12 11:38:18 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:38:18 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457898723
2025-04-12 11:38:18 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:38:18 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:18 [Worker-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:38:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457898765
2025-04-12 11:38:18 [Worker-2] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:38:18 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:38:18 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:38:18 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457898799
2025-04-12 11:38:18 [Worker-3] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:38:18 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:38:18 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:38:18 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457898833
2025-04-12 11:38:18 [Worker-4] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:38:18 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:38:18 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:38:18 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457898870
2025-04-12 11:38:18 [Worker-5] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:38:18 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 100
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-12 11:38:18 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:38:18 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:18 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457898888
2025-04-12 11:38:18 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Started 6 consumers for the 'contacts' topic.
2025-04-12 11:38:18 [Worker-6] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-12 11:38:18 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-12 11:38:19 [Worker-6] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:19 [Worker-4] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:19 [Worker-3] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:19 [Worker-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:19 [Worker-5] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:19 [Worker-2] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:19 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:38:19 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:38:19 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:38:19 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:38:19 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:38:19 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-12 11:38:19 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:38:19 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:38:19 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:38:19 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:38:19 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:38:19 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:38:19 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-1-2caad12c-3e55-49d3-a101-874a229ded7a
2025-04-12 11:38:19 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-2-1cf42688-7793-46fc-b436-6ad66657f46f
2025-04-12 11:38:19 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-4-3b999c81-81af-47e4-a4ce-a76b733eb8dc
2025-04-12 11:38:19 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-5-6f1343de-4fff-43d7-973f-8cf7e638de3b
2025-04-12 11:38:19 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-6-0e971235-385a-4edc-b71b-c055c27d4bec
2025-04-12 11:38:19 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:38:19 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:38:19 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-3-35cabd82-6e8b-4c87-bbf4-61d8e8a9b0d9
2025-04-12 11:38:19 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:38:19 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:38:19 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:38:19 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-12 11:38:22 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=89, memberId='consumer-contacts-test-group-1-2caad12c-3e55-49d3-a101-874a229ded7a', protocol='range'}
2025-04-12 11:38:22 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=89, memberId='consumer-contacts-test-group-2-1cf42688-7793-46fc-b436-6ad66657f46f', protocol='range'}
2025-04-12 11:38:22 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=89, memberId='consumer-contacts-test-group-3-35cabd82-6e8b-4c87-bbf4-61d8e8a9b0d9', protocol='range'}
2025-04-12 11:38:22 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=89, memberId='consumer-contacts-test-group-4-3b999c81-81af-47e4-a4ce-a76b733eb8dc', protocol='range'}
2025-04-12 11:38:22 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Finished assignment for group at generation 89: {consumer-contacts-test-group-5-6f1343de-4fff-43d7-973f-8cf7e638de3b=Assignment(partitions=[]), consumer-contacts-test-group-3-35cabd82-6e8b-4c87-bbf4-61d8e8a9b0d9=Assignment(partitions=[]), consumer-contacts-test-group-4-3b999c81-81af-47e4-a4ce-a76b733eb8dc=Assignment(partitions=[]), consumer-contacts-test-group-2-1cf42688-7793-46fc-b436-6ad66657f46f=Assignment(partitions=[]), consumer-contacts-test-group-6-0e971235-385a-4edc-b71b-c055c27d4bec=Assignment(partitions=[]), consumer-contacts-test-group-1-2caad12c-3e55-49d3-a101-874a229ded7a=Assignment(partitions=[test-topic-0])}
2025-04-12 11:38:22 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=89, memberId='consumer-contacts-test-group-4-3b999c81-81af-47e4-a4ce-a76b733eb8dc', protocol='range'}
2025-04-12 11:38:22 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=89, memberId='consumer-contacts-test-group-3-35cabd82-6e8b-4c87-bbf4-61d8e8a9b0d9', protocol='range'}
2025-04-12 11:38:22 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=89, memberId='consumer-contacts-test-group-2-1cf42688-7793-46fc-b436-6ad66657f46f', protocol='range'}
2025-04-12 11:38:22 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=89, memberId='consumer-contacts-test-group-1-2caad12c-3e55-49d3-a101-874a229ded7a', protocol='range'}
2025-04-12 11:38:22 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:38:22 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:38:22 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:38:22 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-12 11:38:22 [Worker-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:38:22 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:38:22 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:38:22 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=89, memberId='consumer-contacts-test-group-5-6f1343de-4fff-43d7-973f-8cf7e638de3b', protocol='range'}
2025-04-12 11:38:22 [Worker-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-12 11:38:22 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=89, memberId='consumer-contacts-test-group-5-6f1343de-4fff-43d7-973f-8cf7e638de3b', protocol='range'}
2025-04-12 11:38:22 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:38:22 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:38:22 [Worker-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=70349, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=12}}
2025-04-12 11:38:22 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=89, memberId='consumer-contacts-test-group-6-0e971235-385a-4edc-b71b-c055c27d4bec', protocol='range'}
2025-04-12 11:38:22 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=89, memberId='consumer-contacts-test-group-6-0e971235-385a-4edc-b71b-c055c27d4bec', protocol='range'}
2025-04-12 11:38:22 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-12 11:38:22 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-12 11:38:22 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_70, value: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}, offset: 70349
2025-04-12 11:38:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_70
2025-04-12 11:38:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}%n
2025-04-12 11:38:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 11:38:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 11:38:27 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:27 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:27 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-04-12 11:38:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457907199
2025-04-12 11:38:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_70', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 11:38:27 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:27 [kafka-producer-network-thread | producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 7103 with epoch 0
2025-04-12 11:38:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:27 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_81, value: {"_id": {"$oid": "67f10b97558ab0f6396b145b"}, "review": "Not bad, not great either.", "authId": "user_81", "datetime": "2025-03-11T12:19:55.240Z", "sentiment": null}, offset: 70350
2025-04-12 11:38:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_81
2025-04-12 11:38:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145b"}, "review": "Not bad, not great either.", "authId": "user_81", "datetime": "2025-03-11T12:19:55.240Z", "sentiment": null}%n
2025-04-12 11:38:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 11:38:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 11:38:29 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:29 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:29 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-2] Instantiated an idempotent producer.
2025-04-12 11:38:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457909173
2025-04-12 11:38:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_81', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 11:38:29 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-2] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:29 [kafka-producer-network-thread | producer-2] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-2] ProducerId set to 7104 with epoch 0
2025-04-12 11:38:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:29 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_24, value: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}, offset: 70351
2025-04-12 11:38:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_24
2025-04-12 11:38:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}%n
2025-04-12 11:38:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-12 11:38:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-12 11:38:30 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:30 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:30 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-3] Instantiated an idempotent producer.
2025-04-12 11:38:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457910734
2025-04-12 11:38:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_24', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-12 11:38:30 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-3] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:30 [kafka-producer-network-thread | producer-3] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-3] ProducerId set to 7105 with epoch 0
2025-04-12 11:38:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:30 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 70352
2025-04-12 11:38:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-12 11:38:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-12 11:38:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 11:38:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 11:38:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-4] Instantiated an idempotent producer.
2025-04-12 11:38:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457912312
2025-04-12 11:38:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 11:38:32 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-4] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:32 [kafka-producer-network-thread | producer-4] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-4] ProducerId set to 7106 with epoch 0
2025-04-12 11:38:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_48, value: {"_id": {"$oid": "67f10b97558ab0f6396b143a"}, "review": "Terrible experience, very disappointed.", "authId": "user_48", "datetime": "2025-01-15T02:57:46.191Z", "sentiment": null}, offset: 70353
2025-04-12 11:38:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_48
2025-04-12 11:38:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143a"}, "review": "Terrible experience, very disappointed.", "authId": "user_48", "datetime": "2025-01-15T02:57:46.191Z", "sentiment": null}%n
2025-04-12 11:38:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:38:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:38:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-5] Instantiated an idempotent producer.
2025-04-12 11:38:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457913885
2025-04-12 11:38:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_48', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:38:33 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-5] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:33 [kafka-producer-network-thread | producer-5] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-5] ProducerId set to 7107 with epoch 0
2025-04-12 11:38:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "I’m very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 70354
2025-04-12 11:38:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-12 11:38:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "I’m very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}%n
2025-04-12 11:38:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-12 11:38:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-12 11:38:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-6] Instantiated an idempotent producer.
2025-04-12 11:38:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457915456
2025-04-12 11:38:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_91', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-12 11:38:35 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-6] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:35 [kafka-producer-network-thread | producer-6] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-6] ProducerId set to 7108 with epoch 0
2025-04-12 11:38:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_1, value: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}, offset: 70355
2025-04-12 11:38:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_1
2025-04-12 11:38:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}%n
2025-04-12 11:38:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:38:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:38:36 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:36 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:36 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-7] Instantiated an idempotent producer.
2025-04-12 11:38:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457917001
2025-04-12 11:38:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:37 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-7] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_1', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:38:37 [kafka-producer-network-thread | producer-7] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-7] ProducerId set to 7109 with epoch 0
2025-04-12 11:38:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:37 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_78, value: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}, offset: 70356
2025-04-12 11:38:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_78
2025-04-12 11:38:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}%n
2025-04-12 11:38:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 11:38:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 11:38:38 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:38 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:38 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-8] Instantiated an idempotent producer.
2025-04-12 11:38:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457918602
2025-04-12 11:38:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_78', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 11:38:38 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-8] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:38 [kafka-producer-network-thread | producer-8] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-8] ProducerId set to 7110 with epoch 0
2025-04-12 11:38:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_19, value: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}, offset: 70357
2025-04-12 11:38:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_19
2025-04-12 11:38:40 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}%n
2025-04-12 11:38:40 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-12 11:38:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-12 11:38:40 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:40 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:40 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-9] Instantiated an idempotent producer.
2025-04-12 11:38:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457920188
2025-04-12 11:38:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_19', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-12 11:38:40 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-9] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:40 [kafka-producer-network-thread | producer-9] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-9] ProducerId set to 7111 with epoch 0
2025-04-12 11:38:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:40 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 70358
2025-04-12 11:38:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-12 11:38:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-12 11:38:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:38:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:38:41 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:41 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:41 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-10] Instantiated an idempotent producer.
2025-04-12 11:38:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457921779
2025-04-12 11:38:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:38:41 [kafka-producer-network-thread | producer-10] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-10] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:41 [kafka-producer-network-thread | producer-10] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-10] ProducerId set to 7112 with epoch 0
2025-04-12 11:38:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:41 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_97, value: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}, offset: 70359
2025-04-12 11:38:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_97
2025-04-12 11:38:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}%n
2025-04-12 11:38:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:38:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:38:43 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:43 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:43 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-11] Instantiated an idempotent producer.
2025-04-12 11:38:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457923328
2025-04-12 11:38:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_97', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:38:43 [kafka-producer-network-thread | producer-11] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-11] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:43 [kafka-producer-network-thread | producer-11] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-11] ProducerId set to 7113 with epoch 0
2025-04-12 11:38:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:43 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_42, value: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}, offset: 70360
2025-04-12 11:38:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_42
2025-04-12 11:38:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}%n
2025-04-12 11:38:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:38:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:38:44 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:44 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:44 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-12] Instantiated an idempotent producer.
2025-04-12 11:38:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457924880
2025-04-12 11:38:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_42', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:38:44 [kafka-producer-network-thread | producer-12] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-12] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:44 [kafka-producer-network-thread | producer-12] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-12] ProducerId set to 7114 with epoch 0
2025-04-12 11:38:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_34, value: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}, offset: 70361
2025-04-12 11:38:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_34
2025-04-12 11:38:45 [Worker-2] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:45 [Worker-3] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:45 [Worker-2] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:45 [Worker-3] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:45 [Worker-6] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:45 [Worker-5] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:45 [Worker-6] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:45 [Thread-6] INFO  o.s.t.kafka.consumer.ConsumerManager - Shutting down consumers...
2025-04-12 11:38:45 [Worker-5] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:45 [Worker-4] ERROR o.s.t.k.consumer.ConsumerTemplate - Error in consumer loop
org.apache.kafka.common.errors.WakeupException: null
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:530)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:294)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:252)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.pollForFetches(LegacyKafkaConsumer.java:687)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:618)
	at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.poll(LegacyKafkaConsumer.java:591)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:67)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:45 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Member consumer-contacts-test-group-6-0e971235-385a-4edc-b71b-c055c27d4bec sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:38:45 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Member consumer-contacts-test-group-2-1cf42688-7793-46fc-b436-6ad66657f46f sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:38:45 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Member consumer-contacts-test-group-3-35cabd82-6e8b-4c87-bbf4-61d8e8a9b0d9 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:38:45 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Member consumer-contacts-test-group-5-6f1343de-4fff-43d7-973f-8cf7e638de3b sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:38:45 [Worker-4] ERROR o.s.t.k.consumer.ConsumerTemplate - Error removing shutdown hook
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:83)
	at java.base/java.lang.Runtime.removeShutdownHook(Runtime.java:281)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.cleanup(ConsumerTemplate.java:141)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:90)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:45 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:38:45 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:38:45 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:38:45 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:38:45 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Member consumer-contacts-test-group-4-3b999c81-81af-47e4-a4ce-a76b733eb8dc sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-12 11:38:45 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:38:45 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-12 11:38:45 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:38:45 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:38:45 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:38:45 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-12 11:38:45 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:38:45 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:38:45 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:38:45 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:38:45 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:38:45 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:38:45 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-12 11:38:45 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:38:45 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:38:45 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:38:45 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:38:45 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-12 11:38:45 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:38:45 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:38:45 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:38:45 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:38:45 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-12 11:38:45 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:38:45 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:38:45 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-12 11:38:45 [Worker-4] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-4 unregistered
2025-04-12 11:38:45 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:38:45 [Worker-5] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-5 unregistered
2025-04-12 11:38:45 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:38:45 [Worker-2] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-2 unregistered
2025-04-12 11:38:45 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:38:45 [Worker-6] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-6 unregistered
2025-04-12 11:38:45 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:38:45 [Worker-3] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-3 unregistered
2025-04-12 11:38:45 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-12 11:38:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}%n
2025-04-12 11:38:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 11:38:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 11:38:46 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:46 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:46 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-13] Instantiated an idempotent producer.
2025-04-12 11:38:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457926445
2025-04-12 11:38:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_34', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 11:38:46 [kafka-producer-network-thread | producer-13] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-13] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:46 [kafka-producer-network-thread | producer-13] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-13] ProducerId set to 7115 with epoch 0
2025-04-12 11:38:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:46 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_92, value: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}, offset: 70362
2025-04-12 11:38:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_92
2025-04-12 11:38:46 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:46 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:47 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 11:38:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}%n
2025-04-12 11:38:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-12 11:38:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-12 11:38:48 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:48 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:48 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-14] Instantiated an idempotent producer.
2025-04-12 11:38:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457928016
2025-04-12 11:38:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_92', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-12 11:38:48 [kafka-producer-network-thread | producer-14] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-14] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:48 [kafka-producer-network-thread | producer-14] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-14] ProducerId set to 7116 with epoch 0
2025-04-12 11:38:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:48 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_64, value: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}, offset: 70363
2025-04-12 11:38:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_64
2025-04-12 11:38:48 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:48 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}%n
2025-04-12 11:38:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:38:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:38:49 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:49 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:49 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-15] Instantiated an idempotent producer.
2025-04-12 11:38:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457929563
2025-04-12 11:38:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_64', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:38:49 [kafka-producer-network-thread | producer-15] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-15] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:49 [kafka-producer-network-thread | producer-15] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-15] ProducerId set to 7117 with epoch 0
2025-04-12 11:38:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:49 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_3, value: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}, offset: 70364
2025-04-12 11:38:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_3
2025-04-12 11:38:49 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:49 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:50 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 11:38:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}%n
2025-04-12 11:38:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-12 11:38:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-12 11:38:51 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:51 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:51 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-16] Instantiated an idempotent producer.
2025-04-12 11:38:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457931083
2025-04-12 11:38:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:51 [kafka-producer-network-thread | producer-16] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-16] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_3', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-12 11:38:51 [kafka-producer-network-thread | producer-16] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-16] ProducerId set to 7118 with epoch 0
2025-04-12 11:38:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:51 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_64, value: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}, offset: 70365
2025-04-12 11:38:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_64
2025-04-12 11:38:51 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:51 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}%n
2025-04-12 11:38:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:38:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:38:52 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:52 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:52 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-17] Instantiated an idempotent producer.
2025-04-12 11:38:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457932598
2025-04-12 11:38:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_64', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:38:52 [kafka-producer-network-thread | producer-17] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-17] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:52 [kafka-producer-network-thread | producer-17] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-17] ProducerId set to 7119 with epoch 0
2025-04-12 11:38:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:52 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_56, value: {"_id": {"$oid": "67f10b97558ab0f6396b1442"}, "review": "It's okay, does the job.", "authId": "user_56", "datetime": "2024-12-14T20:56:35.004Z", "sentiment": null}, offset: 70366
2025-04-12 11:38:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_56
2025-04-12 11:38:52 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:52 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:53 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 11:38:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1442"}, "review": "It's okay, does the job.", "authId": "user_56", "datetime": "2024-12-14T20:56:35.004Z", "sentiment": null}%n
2025-04-12 11:38:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-12 11:38:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-12 11:38:53 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:53 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:53 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-18] Instantiated an idempotent producer.
2025-04-12 11:38:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457933123
2025-04-12 11:38:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_56', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-12 11:38:53 [kafka-producer-network-thread | producer-18] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-18] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:53 [kafka-producer-network-thread | producer-18] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-18] ProducerId set to 7120 with epoch 0
2025-04-12 11:38:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:53 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_41, value: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}, offset: 70367
2025-04-12 11:38:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_41
2025-04-12 11:38:53 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:53 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}%n
2025-04-12 11:38:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:38:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:38:54 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:54 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:54 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-19] Instantiated an idempotent producer.
2025-04-12 11:38:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457934644
2025-04-12 11:38:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_41', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:38:54 [kafka-producer-network-thread | producer-19] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-19] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:54 [kafka-producer-network-thread | producer-19] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-19] ProducerId set to 7121 with epoch 0
2025-04-12 11:38:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:54 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_11, value: {"_id": {"$oid": "67f10b97558ab0f6396b1415"}, "review": "Would not recommend to anyone.", "authId": "user_11", "datetime": "2024-12-10T23:20:11.570Z", "sentiment": null}, offset: 70368
2025-04-12 11:38:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_11
2025-04-12 11:38:54 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:54 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1415"}, "review": "Would not recommend to anyone.", "authId": "user_11", "datetime": "2024-12-10T23:20:11.570Z", "sentiment": null}%n
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-12 11:38:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-20] Instantiated an idempotent producer.
2025-04-12 11:38:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457935206
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_11', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-12 11:38:55 [kafka-producer-network-thread | producer-20] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-20] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:55 [kafka-producer-network-thread | producer-20] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-20] ProducerId set to 7122 with epoch 0
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 70369
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-12 11:38:55 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:55 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-12 11:38:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-21] Instantiated an idempotent producer.
2025-04-12 11:38:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457935770
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-12 11:38:55 [kafka-producer-network-thread | producer-21] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-21] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:55 [kafka-producer-network-thread | producer-21] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-21] ProducerId set to 7123 with epoch 0
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_66, value: {"_id": {"$oid": "67f10b97558ab0f6396b144c"}, "review": "Great quality, will buy again.", "authId": "user_66", "datetime": "2025-02-01T17:37:18.002Z", "sentiment": null}, offset: 70370
2025-04-12 11:38:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_66
2025-04-12 11:38:55 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:55 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:56 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-12 11:38:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144c"}, "review": "Great quality, will buy again.", "authId": "user_66", "datetime": "2025-02-01T17:37:18.002Z", "sentiment": null}%n
2025-04-12 11:38:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: 0.1

2025-04-12 11:38:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: 0.1

2025-04-12 11:38:57 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:57 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:57 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-22] Instantiated an idempotent producer.
2025-04-12 11:38:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457937344
2025-04-12 11:38:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_66', sentiment=magnitude: 0.1
score: 0.1
}
2025-04-12 11:38:57 [kafka-producer-network-thread | producer-22] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-22] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:57 [kafka-producer-network-thread | producer-22] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-22] ProducerId set to 7124 with epoch 0
2025-04-12 11:38:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:57 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_90, value: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}, offset: 70371
2025-04-12 11:38:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_90
2025-04-12 11:38:57 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:57 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}%n
2025-04-12 11:38:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: -0.1

2025-04-12 11:38:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: -0.1

2025-04-12 11:38:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-23
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-12 11:38:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-12 11:38:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-23] Instantiated an idempotent producer.
2025-04-12 11:38:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-12 11:38:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-12 11:38:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744457938879
2025-04-12 11:38:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-12 11:38:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_90', sentiment=magnitude: 0.1
score: -0.1
}
2025-04-12 11:38:58 [kafka-producer-network-thread | producer-23] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-23] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-12 11:38:58 [kafka-producer-network-thread | producer-23] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-23] ProducerId set to 7125 with epoch 0
2025-04-12 11:38:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-12 11:38:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70372
2025-04-12 11:38:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-12 11:38:58 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:10)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:58 [Worker-1] ERROR o.s.t.s.GoogleNaturalLanguageService - Error in creating  google client for sentiment : 
java.lang.IllegalStateException: Shutdown in progress
	at java.base/java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:67)
	at java.base/java.lang.Runtime.addShutdownHook(Runtime.java:250)
	at org.shyam.transform.service.GoogleNaturalLanguageService.registerShutdownHook(GoogleNaturalLanguageService.java:55)
	at org.shyam.transform.service.GoogleNaturalLanguageService.<init>(GoogleNaturalLanguageService.java:32)
	at org.shyam.transform.service.LanguageServiceFactory.createLanguageService(LanguageServiceFactory.java:12)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.sendToProcessing(ReviewsConsumer.java:51)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.processAndSend(ReviewsConsumer.java:40)
	at org.shyam.transform.kafka.consumer.ReviewsConsumer.consume(ReviewsConsumer.java:26)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.processRecords(ConsumerTemplate.java:46)
	at org.shyam.transform.kafka.consumer.ConsumerTemplate.startConsuming(ConsumerTemplate.java:70)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-04-12 11:38:59 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
