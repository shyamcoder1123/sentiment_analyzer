2025-04-13 11:41:49 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Starting 6 consumers for the 'contacts' topic...
2025-04-13 11:41:49 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-13 11:41:49 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544510205
2025-04-13 11:41:50 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-13 11:41:50 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:41:50 [Worker-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-13 11:41:50 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544510249
2025-04-13 11:41:50 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-13 11:41:50 [Worker-2] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-13 11:41:50 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:41:50 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544510277
2025-04-13 11:41:50 [Worker-3] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-13 11:41:50 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-13 11:41:50 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-13 11:41:50 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544510299
2025-04-13 11:41:50 [Worker-4] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-13 11:41:50 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-13 11:41:50 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-13 11:41:50 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544510320
2025-04-13 11:41:50 [Worker-5] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-13 11:41:50 [main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-contacts-test-group-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = contacts-test-group
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 10
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2025-04-13 11:41:50 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-13 11:41:50 [main] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:41:50 [main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544510336
2025-04-13 11:41:50 [main] INFO  o.s.t.kafka.consumer.ConsumerManager - Started 6 consumers for the 'contacts' topic.
2025-04-13 11:41:50 [Worker-6] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Subscribed to topic(s): test-topic
2025-04-13 11:41:50 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Consuming messages from topic: test-topic
2025-04-13 11:41:51 [Worker-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:41:51 [Worker-4] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:41:51 [Worker-5] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:41:51 [Worker-2] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:41:51 [Worker-6] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:41:51 [Worker-3] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:41:51 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-13 11:41:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-13 11:41:51 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-13 11:41:51 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-13 11:41:51 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-13 11:41:51 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
2025-04-13 11:41:51 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:41:51 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:41:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:41:51 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:41:51 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:41:51 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:41:51 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-2-d65807e0-4f2b-497b-ba07-f5030e3b20d4
2025-04-13 11:41:51 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2
2025-04-13 11:41:51 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-1-715df488-e3ea-42e4-a146-027c5bda4234
2025-04-13 11:41:51 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-3-950f0571-d3f2-40d7-b664-06529a6ce9ab
2025-04-13 11:41:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda
2025-04-13 11:41:51 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:41:51 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: need to re-join with the given member-id: consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32
2025-04-13 11:41:51 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:41:51 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:41:51 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:41:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:41:51 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:41:54 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=117, memberId='consumer-contacts-test-group-1-715df488-e3ea-42e4-a146-027c5bda4234', protocol='range'}
2025-04-13 11:41:54 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=117, memberId='consumer-contacts-test-group-2-d65807e0-4f2b-497b-ba07-f5030e3b20d4', protocol='range'}
2025-04-13 11:41:54 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Finished assignment for group at generation 117: {consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32=Assignment(partitions=[]), consumer-contacts-test-group-2-d65807e0-4f2b-497b-ba07-f5030e3b20d4=Assignment(partitions=[]), consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda=Assignment(partitions=[]), consumer-contacts-test-group-3-950f0571-d3f2-40d7-b664-06529a6ce9ab=Assignment(partitions=[]), consumer-contacts-test-group-1-715df488-e3ea-42e4-a146-027c5bda4234=Assignment(partitions=[test-topic-0]), consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2=Assignment(partitions=[])}
2025-04-13 11:41:54 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=117, memberId='consumer-contacts-test-group-3-950f0571-d3f2-40d7-b664-06529a6ce9ab', protocol='range'}
2025-04-13 11:41:54 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=117, memberId='consumer-contacts-test-group-3-950f0571-d3f2-40d7-b664-06529a6ce9ab', protocol='range'}
2025-04-13 11:41:54 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=117, memberId='consumer-contacts-test-group-2-d65807e0-4f2b-497b-ba07-f5030e3b20d4', protocol='range'}
2025-04-13 11:41:54 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=117, memberId='consumer-contacts-test-group-1-715df488-e3ea-42e4-a146-027c5bda4234', protocol='range'}
2025-04-13 11:41:54 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:41:54 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:41:54 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-13 11:41:54 [Worker-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:41:54 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:41:54 [Worker-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-13 11:41:54 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=117, memberId='consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda', protocol='range'}
2025-04-13 11:41:54 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=117, memberId='consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda', protocol='range'}
2025-04-13 11:41:54 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:41:54 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:41:54 [Worker-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=70769, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=16}}
2025-04-13 11:41:54 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=117, memberId='consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2', protocol='range'}
2025-04-13 11:41:54 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=117, memberId='consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2', protocol='range'}
2025-04-13 11:41:54 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:41:54 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:41:54 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=117, memberId='consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32', protocol='range'}
2025-04-13 11:41:54 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=117, memberId='consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32', protocol='range'}
2025-04-13 11:41:54 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:41:54 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:41:54 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_42, value: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}, offset: 70769
2025-04-13 11:41:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_42
2025-04-13 11:41:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}%n
2025-04-13 11:41:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:41:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:41:59 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:41:59 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:41:59 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
2025-04-13 11:41:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:41:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:41:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544519247
2025-04-13 11:41:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:41:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_42', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:41:59 [kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:41:59 [kafka-producer-network-thread | producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 9180 with epoch 0
2025-04-13 11:41:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:41:59 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_10, value: {"_id": {"$oid": "67f10b97558ab0f6396b1414"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_10", "datetime": "2025-03-16T01:58:57.821Z", "sentiment": null}, offset: 70770
2025-04-13 11:41:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_10
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1414"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_10", "datetime": "2025-03-16T01:58:57.821Z", "sentiment": null}%n
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.4
score: 0.7

2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.4
score: 0.7

2025-04-13 11:42:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-2] Instantiated an idempotent producer.
2025-04-13 11:42:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544521317
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_10', sentiment=magnitude: 1.4
score: 0.7
}
2025-04-13 11:42:01 [kafka-producer-network-thread | producer-2] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-2] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:01 [kafka-producer-network-thread | producer-2] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-2] ProducerId set to 9181 with epoch 0
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_99, value: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}, offset: 70771
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_99
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}%n
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:42:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-3] Instantiated an idempotent producer.
2025-04-13 11:42:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544521866
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_99', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:42:01 [kafka-producer-network-thread | producer-3] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-3] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:01 [kafka-producer-network-thread | producer-3] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-3] ProducerId set to 9182 with epoch 0
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_15, value: {"_id": {"$oid": "67f10b97558ab0f6396b1419"}, "review": "Mediocre quality, nothing special.", "authId": "user_15", "datetime": "2024-12-29T15:59:17.459Z", "sentiment": null}, offset: 70772
2025-04-13 11:42:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_15
2025-04-13 11:42:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1419"}, "review": "Mediocre quality, nothing special.", "authId": "user_15", "datetime": "2024-12-29T15:59:17.459Z", "sentiment": null}%n
2025-04-13 11:42:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:03 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-4
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:03 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:03 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-4] Instantiated an idempotent producer.
2025-04-13 11:42:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544523393
2025-04-13 11:42:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_15', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:03 [kafka-producer-network-thread | producer-4] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-4] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:03 [kafka-producer-network-thread | producer-4] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-4] ProducerId set to 9183 with epoch 0
2025-04-13 11:42:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:03 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_18, value: {"_id": {"$oid": "67f10b97558ab0f6396b141c"}, "review": "Doesn't work as advertised.", "authId": "user_18", "datetime": "2025-02-20T09:53:18.980Z", "sentiment": null}, offset: 70773
2025-04-13 11:42:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_18
2025-04-13 11:42:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141c"}, "review": "Doesn't work as advertised.", "authId": "user_18", "datetime": "2025-02-20T09:53:18.980Z", "sentiment": null}%n
2025-04-13 11:42:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:04 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-5
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:04 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:04 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-5] Instantiated an idempotent producer.
2025-04-13 11:42:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544524862
2025-04-13 11:42:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_18', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:04 [kafka-producer-network-thread | producer-5] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-5] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:04 [kafka-producer-network-thread | producer-5] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-5] ProducerId set to 9184 with epoch 0
2025-04-13 11:42:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:04 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_31, value: {"_id": {"$oid": "67f10b97558ab0f6396b1429"}, "review": "Very bad quality, feels cheap.", "authId": "user_31", "datetime": "2025-02-24T23:38:18.245Z", "sentiment": null}, offset: 70774
2025-04-13 11:42:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_31
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1429"}, "review": "Very bad quality, feels cheap.", "authId": "user_31", "datetime": "2025-02-24T23:38:18.245Z", "sentiment": null}%n
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:06 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:06 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:06 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-6] Instantiated an idempotent producer.
2025-04-13 11:42:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544526350
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_31', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:06 [kafka-producer-network-thread | producer-6] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-6] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:06 [kafka-producer-network-thread | producer-6] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-6] ProducerId set to 9185 with epoch 0
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_62, value: {"_id": {"$oid": "67f10b97558ab0f6396b1448"}, "review": "Poor build quality, not reliable.", "authId": "user_62", "datetime": "2025-03-04T10:01:05.882Z", "sentiment": null}, offset: 70775
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_62
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1448"}, "review": "Poor build quality, not reliable.", "authId": "user_62", "datetime": "2025-03-04T10:01:05.882Z", "sentiment": null}%n
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:06 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-7
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:06 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:06 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-7] Instantiated an idempotent producer.
2025-04-13 11:42:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544526930
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_62', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:06 [kafka-producer-network-thread | producer-7] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-7] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:06 [kafka-producer-network-thread | producer-7] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-7] ProducerId set to 9186 with epoch 0
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_88, value: {"_id": {"$oid": "67f10b97558ab0f6396b1462"}, "review": "Decent, but not amazing.", "authId": "user_88", "datetime": "2025-03-21T21:19:57.894Z", "sentiment": null}, offset: 70776
2025-04-13 11:42:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_88
2025-04-13 11:42:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1462"}, "review": "Decent, but not amazing.", "authId": "user_88", "datetime": "2025-03-21T21:19:57.894Z", "sentiment": null}%n
2025-04-13 11:42:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:42:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:42:07 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-8
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:07 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:07 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-8] Instantiated an idempotent producer.
2025-04-13 11:42:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544527516
2025-04-13 11:42:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_88', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:42:07 [kafka-producer-network-thread | producer-8] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-8] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:07 [kafka-producer-network-thread | producer-8] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-8] ProducerId set to 9187 with epoch 0
2025-04-13 11:42:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:07 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_42, value: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}, offset: 70777
2025-04-13 11:42:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_42
2025-04-13 11:42:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}%n
2025-04-13 11:42:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:08 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-9
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:08 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:08 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-9] Instantiated an idempotent producer.
2025-04-13 11:42:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544528990
2025-04-13 11:42:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_42', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:08 [kafka-producer-network-thread | producer-9] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-9] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:08 [kafka-producer-network-thread | producer-9] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-9] ProducerId set to 9188 with epoch 0
2025-04-13 11:42:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:09 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_99, value: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}, offset: 70778
2025-04-13 11:42:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_99
2025-04-13 11:42:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}%n
2025-04-13 11:42:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:42:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:42:10 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-10
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:10 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:10 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-10] Instantiated an idempotent producer.
2025-04-13 11:42:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544530514
2025-04-13 11:42:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:10 [kafka-producer-network-thread | producer-10] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-10] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_99', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:42:10 [kafka-producer-network-thread | producer-10] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-10] ProducerId set to 9189 with epoch 0
2025-04-13 11:42:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:11 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_2, value: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}, offset: 70779
2025-04-13 11:42:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_2
2025-04-13 11:42:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}%n
2025-04-13 11:42:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:42:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:42:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-11
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:13 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-11] Instantiated an idempotent producer.
2025-04-13 11:42:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544533007
2025-04-13 11:42:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_2', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:42:13 [kafka-producer-network-thread | producer-11] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-11] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:13 [kafka-producer-network-thread | producer-11] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-11] ProducerId set to 9190 with epoch 0
2025-04-13 11:42:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:13 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_54, value: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}, offset: 70780
2025-04-13 11:42:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_54
2025-04-13 11:42:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}%n
2025-04-13 11:42:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:42:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:42:14 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-12
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:14 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:14 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-12] Instantiated an idempotent producer.
2025-04-13 11:42:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544534483
2025-04-13 11:42:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_54', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:42:14 [kafka-producer-network-thread | producer-12] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-12] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:14 [kafka-producer-network-thread | producer-12] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-12] ProducerId set to 9191 with epoch 0
2025-04-13 11:42:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:14 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_9, value: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}, offset: 70781
2025-04-13 11:42:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_9
2025-04-13 11:42:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}%n
2025-04-13 11:42:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:42:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:42:15 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-13
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:15 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:15 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-13] Instantiated an idempotent producer.
2025-04-13 11:42:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544535011
2025-04-13 11:42:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_9', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:42:15 [kafka-producer-network-thread | producer-13] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-13] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:15 [kafka-producer-network-thread | producer-13] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-13] ProducerId set to 9192 with epoch 0
2025-04-13 11:42:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:15 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70782
2025-04-13 11:42:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-13 11:42:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}%n
2025-04-13 11:42:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:42:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:42:16 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-14
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:16 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:16 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-14] Instantiated an idempotent producer.
2025-04-13 11:42:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544536494
2025-04-13 11:42:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_50', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:42:16 [kafka-producer-network-thread | producer-14] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-14] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:16 [kafka-producer-network-thread | producer-14] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-14] ProducerId set to 9193 with epoch 0
2025-04-13 11:42:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:16 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_59, value: {"_id": {"$oid": "67f10b97558ab0f6396b1445"}, "review": "It's okay, does the job.", "authId": "user_59", "datetime": "2024-12-03T13:32:51.362Z", "sentiment": null}, offset: 70783
2025-04-13 11:42:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_59
2025-04-13 11:42:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1445"}, "review": "It's okay, does the job.", "authId": "user_59", "datetime": "2024-12-03T13:32:51.362Z", "sentiment": null}%n
2025-04-13 11:42:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-13 11:42:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-13 11:42:19 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-15
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:19 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:19 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-15] Instantiated an idempotent producer.
2025-04-13 11:42:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544539159
2025-04-13 11:42:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_59', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-13 11:42:19 [kafka-producer-network-thread | producer-15] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-15] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:19 [kafka-producer-network-thread | producer-15] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-15] ProducerId set to 9194 with epoch 0
2025-04-13 11:42:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:19 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_54, value: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}, offset: 70784
2025-04-13 11:42:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_54
2025-04-13 11:42:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}%n
2025-04-13 11:42:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:42:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:42:20 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-16
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:20 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:20 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-16] Instantiated an idempotent producer.
2025-04-13 11:42:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544540660
2025-04-13 11:42:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_54', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:42:20 [kafka-producer-network-thread | producer-16] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-16] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:20 [kafka-producer-network-thread | producer-16] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-16] ProducerId set to 9195 with epoch 0
2025-04-13 11:42:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:20 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_46, value: {"_id": {"$oid": "67f10b97558ab0f6396b1438"}, "review": "Not bad, not great either.", "authId": "user_46", "datetime": "2025-03-08T03:51:19.059Z", "sentiment": null}, offset: 70785
2025-04-13 11:42:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_46
2025-04-13 11:42:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1438"}, "review": "Not bad, not great either.", "authId": "user_46", "datetime": "2025-03-08T03:51:19.059Z", "sentiment": null}%n
2025-04-13 11:42:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:42:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:42:22 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-17
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:22 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:22 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-17] Instantiated an idempotent producer.
2025-04-13 11:42:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544542150
2025-04-13 11:42:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_46', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:42:22 [kafka-producer-network-thread | producer-17] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-17] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:22 [kafka-producer-network-thread | producer-17] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-17] ProducerId set to 9196 with epoch 0
2025-04-13 11:42:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:22 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_76, value: {"_id": {"$oid": "67f10b97558ab0f6396b1456"}, "review": "Great quality, will buy again.", "authId": "user_76", "datetime": "2025-01-27T21:25:36.715Z", "sentiment": null}, offset: 70786
2025-04-13 11:42:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_76
2025-04-13 11:42:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1456"}, "review": "Great quality, will buy again.", "authId": "user_76", "datetime": "2025-01-27T21:25:36.715Z", "sentiment": null}%n
2025-04-13 11:42:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: 0.1

2025-04-13 11:42:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: 0.1

2025-04-13 11:42:23 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-18
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:23 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:23 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-18] Instantiated an idempotent producer.
2025-04-13 11:42:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544543624
2025-04-13 11:42:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_76', sentiment=magnitude: 0.1
score: 0.1
}
2025-04-13 11:42:23 [kafka-producer-network-thread | producer-18] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-18] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:23 [kafka-producer-network-thread | producer-18] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-18] ProducerId set to 9197 with epoch 0
2025-04-13 11:42:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:23 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_50, value: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}, offset: 70787
2025-04-13 11:42:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_50
2025-04-13 11:42:25 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143c"}, "review": "Works like a charm, perfect!", "authId": "user_50", "datetime": "2025-03-28T22:16:25.554Z", "sentiment": null}%n
2025-04-13 11:42:25 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:42:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:42:25 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-19
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:25 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:25 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-19] Instantiated an idempotent producer.
2025-04-13 11:42:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544545120
2025-04-13 11:42:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_50', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:42:25 [kafka-producer-network-thread | producer-19] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-19] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:25 [kafka-producer-network-thread | producer-19] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-19] ProducerId set to 9198 with epoch 0
2025-04-13 11:42:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:25 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_82, value: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}, offset: 70788
2025-04-13 11:42:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_82
2025-04-13 11:42:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}%n
2025-04-13 11:42:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-20
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-20] Instantiated an idempotent producer.
2025-04-13 11:42:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544546553
2025-04-13 11:42:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_82', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:26 [kafka-producer-network-thread | producer-20] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-20] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:26 [kafka-producer-network-thread | producer-20] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-20] ProducerId set to 9199 with epoch 0
2025-04-13 11:42:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:27 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_7, value: {"_id": {"$oid": "67f10b97558ab0f6396b1411"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_7", "datetime": "2025-03-02T02:14:03.248Z", "sentiment": null}, offset: 70789
2025-04-13 11:42:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_7
2025-04-13 11:42:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1411"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_7", "datetime": "2025-03-02T02:14:03.248Z", "sentiment": null}%n
2025-04-13 11:42:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:42:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:42:29 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-21
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:29 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:29 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-21] Instantiated an idempotent producer.
2025-04-13 11:42:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544549033
2025-04-13 11:42:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:29 [kafka-producer-network-thread | producer-21] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-21] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_7', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:42:29 [kafka-producer-network-thread | producer-21] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-21] ProducerId set to 9201 with epoch 0
2025-04-13 11:42:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:29 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_7, value: {"_id": {"$oid": "67f10b97558ab0f6396b1411"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_7", "datetime": "2025-03-02T02:14:03.248Z", "sentiment": null}, offset: 70790
2025-04-13 11:42:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_7
2025-04-13 11:42:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1411"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_7", "datetime": "2025-03-02T02:14:03.248Z", "sentiment": null}%n
2025-04-13 11:42:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:42:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:42:30 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-22
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:30 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:30 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-22] Instantiated an idempotent producer.
2025-04-13 11:42:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:30 [kafka-producer-network-thread | producer-22] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-22] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544550511
2025-04-13 11:42:30 [kafka-producer-network-thread | producer-22] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-22] ProducerId set to 9202 with epoch 0
2025-04-13 11:42:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_7', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:42:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:30 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_97, value: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}, offset: 70791
2025-04-13 11:42:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_97
2025-04-13 11:42:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}%n
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-23
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-23] Instantiated an idempotent producer.
2025-04-13 11:42:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544552011
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:32 [kafka-producer-network-thread | producer-23] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-23] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_97', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:32 [kafka-producer-network-thread | producer-23] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-23] ProducerId set to 9203 with epoch 0
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_26, value: {"_id": {"$oid": "67f10b97558ab0f6396b1424"}, "review": "Doesn't work as advertised.", "authId": "user_26", "datetime": "2025-02-15T00:52:47.341Z", "sentiment": null}, offset: 70792
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_26
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1424"}, "review": "Doesn't work as advertised.", "authId": "user_26", "datetime": "2025-02-15T00:52:47.341Z", "sentiment": null}%n
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-24
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-24] Instantiated an idempotent producer.
2025-04-13 11:42:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544552507
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_26', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:32 [kafka-producer-network-thread | producer-24] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-24] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:32 [kafka-producer-network-thread | producer-24] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-24] ProducerId set to 9204 with epoch 0
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_49, value: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "Its alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}, offset: 70793
2025-04-13 11:42:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_49
2025-04-13 11:42:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "Its alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}%n
2025-04-13 11:42:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-13 11:42:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-13 11:42:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-25
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-25] Instantiated an idempotent producer.
2025-04-13 11:42:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544553986
2025-04-13 11:42:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_49', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-13 11:42:33 [kafka-producer-network-thread | producer-25] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-25] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:33 [kafka-producer-network-thread | producer-25] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-25] ProducerId set to 9205 with epoch 0
2025-04-13 11:42:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_40, value: {"_id": {"$oid": "67f10b97558ab0f6396b1432"}, "review": "This changed my life!", "authId": "user_40", "datetime": "2025-03-22T15:49:19.758Z", "sentiment": null}, offset: 70794
2025-04-13 11:42:34 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_40
2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1432"}, "review": "This changed my life!", "authId": "user_40", "datetime": "2025-03-22T15:49:19.758Z", "sentiment": null}%n
2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:42:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-26
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-26] Instantiated an idempotent producer.
2025-04-13 11:42:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544555459
2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_40', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:42:35 [kafka-producer-network-thread | producer-26] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-26] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:35 [kafka-producer-network-thread | producer-26] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-26] ProducerId set to 9206 with epoch 0
2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_7, value: {"_id": {"$oid": "67f10b97558ab0f6396b1411"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_7", "datetime": "2025-03-02T02:14:03.248Z", "sentiment": null}, offset: 70795
2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_7
2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1411"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_7", "datetime": "2025-03-02T02:14:03.248Z", "sentiment": null}%n
2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:42:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-27
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-27] Instantiated an idempotent producer.
2025-04-13 11:42:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544555992
2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_7', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:42:35 [kafka-producer-network-thread | producer-27] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-27] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:35 [kafka-producer-network-thread | producer-27] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-27] ProducerId set to 9207 with epoch 0
2025-04-13 11:42:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:36 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_19, value: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}, offset: 70796
2025-04-13 11:42:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_19
2025-04-13 11:42:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}%n
2025-04-13 11:42:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:42:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:42:36 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-28
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:36 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:36 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-28] Instantiated an idempotent producer.
2025-04-13 11:42:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544556521
2025-04-13 11:42:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_19', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:42:36 [kafka-producer-network-thread | producer-28] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-28] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:36 [kafka-producer-network-thread | producer-28] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-28] ProducerId set to 9208 with epoch 0
2025-04-13 11:42:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:36 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_96, value: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}, offset: 70797
2025-04-13 11:42:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_96
2025-04-13 11:42:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}%n
2025-04-13 11:42:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:42:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:42:37 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-29
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:37 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:37 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-29] Instantiated an idempotent producer.
2025-04-13 11:42:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544557998
2025-04-13 11:42:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_96', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:42:38 [kafka-producer-network-thread | producer-29] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-29] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:38 [kafka-producer-network-thread | producer-29] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-29] ProducerId set to 9209 with epoch 0
2025-04-13 11:42:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_73, value: {"_id": {"$oid": "67f10b97558ab0f6396b1453"}, "review": "Not satisfied at all.", "authId": "user_73", "datetime": "2025-03-31T12:24:38.653Z", "sentiment": null}, offset: 70798
2025-04-13 11:42:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_73
2025-04-13 11:42:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1453"}, "review": "Not satisfied at all.", "authId": "user_73", "datetime": "2025-03-31T12:24:38.653Z", "sentiment": null}%n
2025-04-13 11:42:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-30
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-30] Instantiated an idempotent producer.
2025-04-13 11:42:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544559495
2025-04-13 11:42:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_73', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:39 [kafka-producer-network-thread | producer-30] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-30] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:39 [kafka-producer-network-thread | producer-30] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-30] ProducerId set to 9210 with epoch 0
2025-04-13 11:42:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:40 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_62, value: {"_id": {"$oid": "67f10b97558ab0f6396b1448"}, "review": "Poor build quality, not reliable.", "authId": "user_62", "datetime": "2025-03-04T10:01:05.882Z", "sentiment": null}, offset: 70799
2025-04-13 11:42:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_62
2025-04-13 11:42:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1448"}, "review": "Poor build quality, not reliable.", "authId": "user_62", "datetime": "2025-03-04T10:01:05.882Z", "sentiment": null}%n
2025-04-13 11:42:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:42 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-31
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:42 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:42 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-31] Instantiated an idempotent producer.
2025-04-13 11:42:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544562054
2025-04-13 11:42:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_62', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:42 [kafka-producer-network-thread | producer-31] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-31] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:42 [kafka-producer-network-thread | producer-31] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-31] ProducerId set to 9211 with epoch 0
2025-04-13 11:42:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:42 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_74, value: {"_id": {"$oid": "67f10b97558ab0f6396b1454"}, "review": "Works like a charm, perfect!", "authId": "user_74", "datetime": "2024-12-02T13:29:55.232Z", "sentiment": null}, offset: 70800
2025-04-13 11:42:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_74
2025-04-13 11:42:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1454"}, "review": "Works like a charm, perfect!", "authId": "user_74", "datetime": "2024-12-02T13:29:55.232Z", "sentiment": null}%n
2025-04-13 11:42:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:42:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:42:43 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-32
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:43 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:43 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-32] Instantiated an idempotent producer.
2025-04-13 11:42:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544563557
2025-04-13 11:42:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_74', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:42:43 [kafka-producer-network-thread | producer-32] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-32] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:43 [kafka-producer-network-thread | producer-32] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-32] ProducerId set to 9212 with epoch 0
2025-04-13 11:42:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:43 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_16, value: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}, offset: 70801
2025-04-13 11:42:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_16
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}%n
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:42:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-33
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-33] Instantiated an idempotent producer.
2025-04-13 11:42:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544565054
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_16', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:42:45 [kafka-producer-network-thread | producer-33] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-33] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:45 [kafka-producer-network-thread | producer-33] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-33] ProducerId set to 9213 with epoch 0
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_55, value: {"_id": {"$oid": "67f10b97558ab0f6396b1441"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_55", "datetime": "2025-02-18T10:02:17.105Z", "sentiment": null}, offset: 70802
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_55
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1441"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_55", "datetime": "2025-02-18T10:02:17.105Z", "sentiment": null}%n
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.3
score: 0.6

2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.3
score: 0.6

2025-04-13 11:42:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-34
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-34] Instantiated an idempotent producer.
2025-04-13 11:42:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544565593
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:45 [kafka-producer-network-thread | producer-34] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-34] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_55', sentiment=magnitude: 1.3
score: 0.6
}
2025-04-13 11:42:45 [kafka-producer-network-thread | producer-34] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-34] ProducerId set to 9214 with epoch 0
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_54, value: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}, offset: 70803
2025-04-13 11:42:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_54
2025-04-13 11:42:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}%n
2025-04-13 11:42:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:42:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:42:47 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-35
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:47 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:47 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-35] Instantiated an idempotent producer.
2025-04-13 11:42:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544567074
2025-04-13 11:42:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_54', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:42:47 [kafka-producer-network-thread | producer-35] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-35] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:47 [kafka-producer-network-thread | producer-35] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-35] ProducerId set to 9215 with epoch 0
2025-04-13 11:42:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:47 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_17, value: {"_id": {"$oid": "67f10b97558ab0f6396b141b"}, "review": "Terrible experience, very disappointed.", "authId": "user_17", "datetime": "2025-03-01T00:30:53.030Z", "sentiment": null}, offset: 70804
2025-04-13 11:42:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_17
2025-04-13 11:42:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141b"}, "review": "Terrible experience, very disappointed.", "authId": "user_17", "datetime": "2025-03-01T00:30:53.030Z", "sentiment": null}%n
2025-04-13 11:42:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:48 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-36
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:48 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:48 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-36] Instantiated an idempotent producer.
2025-04-13 11:42:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544568553
2025-04-13 11:42:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_17', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:48 [kafka-producer-network-thread | producer-36] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-36] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:48 [kafka-producer-network-thread | producer-36] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-36] ProducerId set to 9216 with epoch 0
2025-04-13 11:42:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:48 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_3, value: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}, offset: 70805
2025-04-13 11:42:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_3
2025-04-13 11:42:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}%n
2025-04-13 11:42:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 11:42:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 11:42:49 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-37
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:49 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:49 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-37] Instantiated an idempotent producer.
2025-04-13 11:42:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544569088
2025-04-13 11:42:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:49 [kafka-producer-network-thread | producer-37] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-37] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_3', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 11:42:49 [kafka-producer-network-thread | producer-37] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-37] ProducerId set to 9217 with epoch 0
2025-04-13 11:42:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:49 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_100, value: {"_id": {"$oid": "67f10b97558ab0f6396b146e"}, "review": "Terrible experience, very disappointed.", "authId": "user_100", "datetime": "2025-02-03T10:22:13.147Z", "sentiment": null}, offset: 70806
2025-04-13 11:42:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_100
2025-04-13 11:42:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146e"}, "review": "Terrible experience, very disappointed.", "authId": "user_100", "datetime": "2025-02-03T10:22:13.147Z", "sentiment": null}%n
2025-04-13 11:42:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:51 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-38
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:51 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:51 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-38] Instantiated an idempotent producer.
2025-04-13 11:42:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544571723
2025-04-13 11:42:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_100', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:51 [kafka-producer-network-thread | producer-38] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-38] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:51 [kafka-producer-network-thread | producer-38] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-38] ProducerId set to 9218 with epoch 0
2025-04-13 11:42:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:51 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_10, value: {"_id": {"$oid": "67f10b97558ab0f6396b1414"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_10", "datetime": "2025-03-16T01:58:57.821Z", "sentiment": null}, offset: 70807
2025-04-13 11:42:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_10
2025-04-13 11:42:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1414"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_10", "datetime": "2025-03-16T01:58:57.821Z", "sentiment": null}%n
2025-04-13 11:42:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.4
score: 0.7

2025-04-13 11:42:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.4
score: 0.7

2025-04-13 11:42:53 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-39
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:53 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:53 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-39] Instantiated an idempotent producer.
2025-04-13 11:42:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544573288
2025-04-13 11:42:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_10', sentiment=magnitude: 1.4
score: 0.7
}
2025-04-13 11:42:53 [kafka-producer-network-thread | producer-39] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-39] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:53 [kafka-producer-network-thread | producer-39] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-39] ProducerId set to 9219 with epoch 0
2025-04-13 11:42:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:53 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_78, value: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}, offset: 70808
2025-04-13 11:42:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_78
2025-04-13 11:42:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}%n
2025-04-13 11:42:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:42:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:42:54 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-40
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:54 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:54 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-40] Instantiated an idempotent producer.
2025-04-13 11:42:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544574844
2025-04-13 11:42:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_78', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:42:54 [kafka-producer-network-thread | producer-40] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-40] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:54 [kafka-producer-network-thread | producer-40] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-40] ProducerId set to 9220 with epoch 0
2025-04-13 11:42:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_75, value: {"_id": {"$oid": "67f10b97558ab0f6396b1455"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_75", "datetime": "2025-02-20T20:29:36.163Z", "sentiment": null}, offset: 70809
2025-04-13 11:42:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_75
2025-04-13 11:42:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1455"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_75", "datetime": "2025-02-20T20:29:36.163Z", "sentiment": null}%n
2025-04-13 11:42:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.3
score: 0.6

2025-04-13 11:42:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.3
score: 0.6

2025-04-13 11:42:57 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-41
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:57 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:57 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-41] Instantiated an idempotent producer.
2025-04-13 11:42:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544577362
2025-04-13 11:42:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:57 [kafka-producer-network-thread | producer-41] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-41] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_75', sentiment=magnitude: 1.3
score: 0.6
}
2025-04-13 11:42:57 [kafka-producer-network-thread | producer-41] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-41] ProducerId set to 9221 with epoch 0
2025-04-13 11:42:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:57 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_33, value: {"_id": {"$oid": "67f10b97558ab0f6396b142b"}, "review": "Terrible experience, very disappointed.", "authId": "user_33", "datetime": "2024-12-24T07:32:50.779Z", "sentiment": null}, offset: 70810
2025-04-13 11:42:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_33
2025-04-13 11:42:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142b"}, "review": "Terrible experience, very disappointed.", "authId": "user_33", "datetime": "2024-12-24T07:32:50.779Z", "sentiment": null}%n
2025-04-13 11:42:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-42
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-42] Instantiated an idempotent producer.
2025-04-13 11:42:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544578762
2025-04-13 11:42:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_33', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:58 [kafka-producer-network-thread | producer-42] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-42] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:58 [kafka-producer-network-thread | producer-42] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-42] ProducerId set to 9222 with epoch 0
2025-04-13 11:42:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_84, value: {"_id": {"$oid": "67f10b97558ab0f6396b145e"}, "review": "Terrible experience, very disappointed.", "authId": "user_84", "datetime": "2025-03-09T23:08:10.325Z", "sentiment": null}, offset: 70811
2025-04-13 11:42:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_84
2025-04-13 11:42:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145e"}, "review": "Terrible experience, very disappointed.", "authId": "user_84", "datetime": "2025-03-09T23:08:10.325Z", "sentiment": null}%n
2025-04-13 11:42:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:42:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:42:59 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-43
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:42:59 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:42:59 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-43] Instantiated an idempotent producer.
2025-04-13 11:42:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:42:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:42:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544579323
2025-04-13 11:42:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:42:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_84', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:42:59 [kafka-producer-network-thread | producer-43] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-43] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:42:59 [kafka-producer-network-thread | producer-43] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-43] ProducerId set to 9223 with epoch 0
2025-04-13 11:42:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:42:59 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_85, value: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}, offset: 70812
2025-04-13 11:42:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_85
2025-04-13 11:43:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}%n
2025-04-13 11:43:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:43:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:43:00 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-44
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:00 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:00 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-44] Instantiated an idempotent producer.
2025-04-13 11:43:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544580718
2025-04-13 11:43:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_85', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:43:00 [kafka-producer-network-thread | producer-44] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-44] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:00 [kafka-producer-network-thread | producer-44] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-44] ProducerId set to 9224 with epoch 0
2025-04-13 11:43:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:00 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_85, value: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}, offset: 70813
2025-04-13 11:43:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_85
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}%n
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:43:02 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-45
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:02 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:02 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-45] Instantiated an idempotent producer.
2025-04-13 11:43:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544582119
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_85', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:43:02 [kafka-producer-network-thread | producer-45] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-45] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:02 [kafka-producer-network-thread | producer-45] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-45] ProducerId set to 9225 with epoch 0
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_32, value: {"_id": {"$oid": "67f10b97558ab0f6396b142a"}, "review": "Fantastic performance and very durable.", "authId": "user_32", "datetime": "2025-01-03T02:26:59.600Z", "sentiment": null}, offset: 70814
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_32
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142a"}, "review": "Fantastic performance and very durable.", "authId": "user_32", "datetime": "2025-01-03T02:26:59.600Z", "sentiment": null}%n
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 11:43:02 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-46
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:02 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:02 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-46] Instantiated an idempotent producer.
2025-04-13 11:43:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544582602
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_32', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 11:43:02 [kafka-producer-network-thread | producer-46] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-46] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:02 [kafka-producer-network-thread | producer-46] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-46] ProducerId set to 9226 with epoch 0
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_74, value: {"_id": {"$oid": "67f10b97558ab0f6396b1454"}, "review": "Works like a charm, perfect!", "authId": "user_74", "datetime": "2024-12-02T13:29:55.232Z", "sentiment": null}, offset: 70815
2025-04-13 11:43:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_74
2025-04-13 11:43:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1454"}, "review": "Works like a charm, perfect!", "authId": "user_74", "datetime": "2024-12-02T13:29:55.232Z", "sentiment": null}%n
2025-04-13 11:43:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:43:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:43:04 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-47
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:04 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:04 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-47] Instantiated an idempotent producer.
2025-04-13 11:43:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544584053
2025-04-13 11:43:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_74', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:43:04 [kafka-producer-network-thread | producer-47] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-47] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:04 [kafka-producer-network-thread | producer-47] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-47] ProducerId set to 9227 with epoch 0
2025-04-13 11:43:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:04 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_89, value: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}, offset: 70816
2025-04-13 11:43:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_89
2025-04-13 11:43:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}%n
2025-04-13 11:43:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:43:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:43:05 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-48
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:05 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:05 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-48] Instantiated an idempotent producer.
2025-04-13 11:43:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544585544
2025-04-13 11:43:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_89', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:43:05 [kafka-producer-network-thread | producer-48] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-48] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:05 [kafka-producer-network-thread | producer-48] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-48] ProducerId set to 9228 with epoch 0
2025-04-13 11:43:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:05 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_14, value: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}, offset: 70817
2025-04-13 11:43:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_14
2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}%n
2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:43:07 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-49
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:07 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:07 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-49] Instantiated an idempotent producer.
2025-04-13 11:43:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544587044
2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_14', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:43:07 [kafka-producer-network-thread | producer-49] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-49] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:07 [kafka-producer-network-thread | producer-49] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-49] ProducerId set to 9229 with epoch 0
2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_28, value: {"_id": {"$oid": "67f10b97558ab0f6396b1426"}, "review": "Horrible customer service.", "authId": "user_28", "datetime": "2025-02-08T18:58:47.653Z", "sentiment": null}, offset: 70818
2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_28
2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1426"}, "review": "Horrible customer service.", "authId": "user_28", "datetime": "2025-02-08T18:58:47.653Z", "sentiment": null}%n
2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:43:07 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-50
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:07 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:07 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-50] Instantiated an idempotent producer.
2025-04-13 11:43:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544587572
2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_28', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:43:07 [kafka-producer-network-thread | producer-50] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-50] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:07 [kafka-producer-network-thread | producer-50] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-50] ProducerId set to 9230 with epoch 0
2025-04-13 11:43:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:08 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_86, value: {"_id": {"$oid": "67f10b97558ab0f6396b1460"}, "review": "Smooth and easy to use.", "authId": "user_86", "datetime": "2025-02-22T20:30:11.182Z", "sentiment": null}, offset: 70819
2025-04-13 11:43:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_86
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1460"}, "review": "Smooth and easy to use.", "authId": "user_86", "datetime": "2025-02-22T20:30:11.182Z", "sentiment": null}%n
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = 
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data 
2025-04-13 11:43:10 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-51
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:10 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:10 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-51] Instantiated an idempotent producer.
2025-04-13 11:43:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544590089
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_86', sentiment=}
2025-04-13 11:43:10 [kafka-producer-network-thread | producer-51] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-51] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:10 [kafka-producer-network-thread | producer-51] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-51] ProducerId set to 9231 with epoch 0
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_35, value: {"_id": {"$oid": "67f10b97558ab0f6396b142d"}, "review": "Terrible experience, very disappointed.", "authId": "user_35", "datetime": "2025-03-06T15:09:01.869Z", "sentiment": null}, offset: 70820
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_35
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142d"}, "review": "Terrible experience, very disappointed.", "authId": "user_35", "datetime": "2025-03-06T15:09:01.869Z", "sentiment": null}%n
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:10 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-52
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:10 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:10 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-52] Instantiated an idempotent producer.
2025-04-13 11:43:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544590626
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_35', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:10 [kafka-producer-network-thread | producer-52] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-52] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:10 [kafka-producer-network-thread | producer-52] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-52] ProducerId set to 9232 with epoch 0
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_85, value: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}, offset: 70821
2025-04-13 11:43:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_85
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}%n
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:43:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-53
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:12 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-53] Instantiated an idempotent producer.
2025-04-13 11:43:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544592139
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_85', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:43:12 [kafka-producer-network-thread | producer-53] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-53] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:12 [kafka-producer-network-thread | producer-53] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-53] ProducerId set to 9233 with epoch 0
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_5, value: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}, offset: 70822
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_5
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}%n
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:43:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-54
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:12 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-54] Instantiated an idempotent producer.
2025-04-13 11:43:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544592663
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:12 [kafka-producer-network-thread | producer-54] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-54] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_5', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:43:12 [kafka-producer-network-thread | producer-54] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-54] ProducerId set to 9234 with epoch 0
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_20, value: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}, offset: 70823
2025-04-13 11:43:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_20
2025-04-13 11:43:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}%n
2025-04-13 11:43:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:43:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:43:14 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-55
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:14 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:14 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-55] Instantiated an idempotent producer.
2025-04-13 11:43:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544594192
2025-04-13 11:43:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_20', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:43:14 [kafka-producer-network-thread | producer-55] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-55] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:14 [kafka-producer-network-thread | producer-55] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-55] ProducerId set to 9235 with epoch 0
2025-04-13 11:43:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:14 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_36, value: {"_id": {"$oid": "67f10b97558ab0f6396b142e"}, "review": "Works like a charm, perfect!", "authId": "user_36", "datetime": "2024-12-07T01:28:05.981Z", "sentiment": null}, offset: 70824
2025-04-13 11:43:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_36
2025-04-13 11:43:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142e"}, "review": "Works like a charm, perfect!", "authId": "user_36", "datetime": "2024-12-07T01:28:05.981Z", "sentiment": null}%n
2025-04-13 11:43:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:43:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:43:15 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-56
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:15 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:15 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-56] Instantiated an idempotent producer.
2025-04-13 11:43:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544595703
2025-04-13 11:43:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_36', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:43:15 [kafka-producer-network-thread | producer-56] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-56] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:15 [kafka-producer-network-thread | producer-56] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-56] ProducerId set to 9236 with epoch 0
2025-04-13 11:43:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:15 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_47, value: {"_id": {"$oid": "67f10b97558ab0f6396b1439"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_47", "datetime": "2025-01-08T17:35:19.231Z", "sentiment": null}, offset: 70825
2025-04-13 11:43:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_47
2025-04-13 11:43:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1439"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_47", "datetime": "2025-01-08T17:35:19.231Z", "sentiment": null}%n
2025-04-13 11:43:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.8
score: 0.4

2025-04-13 11:43:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.8
score: 0.4

2025-04-13 11:43:17 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-57
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:17 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:17 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-57] Instantiated an idempotent producer.
2025-04-13 11:43:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544597337
2025-04-13 11:43:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_47', sentiment=magnitude: 0.8
score: 0.4
}
2025-04-13 11:43:17 [kafka-producer-network-thread | producer-57] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-57] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:17 [kafka-producer-network-thread | producer-57] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-57] ProducerId set to 9237 with epoch 0
2025-04-13 11:43:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:17 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_57, value: {"_id": {"$oid": "67f10b97558ab0f6396b1443"}, "review": "This changed my life!", "authId": "user_57", "datetime": "2025-03-21T00:28:58.918Z", "sentiment": null}, offset: 70826
2025-04-13 11:43:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_57
2025-04-13 11:43:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1443"}, "review": "This changed my life!", "authId": "user_57", "datetime": "2025-03-21T00:28:58.918Z", "sentiment": null}%n
2025-04-13 11:43:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:43:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:43:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-58
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-58] Instantiated an idempotent producer.
2025-04-13 11:43:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544598810
2025-04-13 11:43:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_57', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:43:18 [kafka-producer-network-thread | producer-58] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-58] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:18 [kafka-producer-network-thread | producer-58] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-58] ProducerId set to 9238 with epoch 0
2025-04-13 11:43:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_17, value: {"_id": {"$oid": "67f10b97558ab0f6396b141b"}, "review": "Terrible experience, very disappointed.", "authId": "user_17", "datetime": "2025-03-01T00:30:53.030Z", "sentiment": null}, offset: 70827
2025-04-13 11:43:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_17
2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141b"}, "review": "Terrible experience, very disappointed.", "authId": "user_17", "datetime": "2025-03-01T00:30:53.030Z", "sentiment": null}%n
2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:20 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-59
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:20 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:20 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-59] Instantiated an idempotent producer.
2025-04-13 11:43:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544600340
2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_17', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:20 [kafka-producer-network-thread | producer-59] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-59] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:20 [kafka-producer-network-thread | producer-59] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-59] ProducerId set to 9239 with epoch 0
2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_75, value: {"_id": {"$oid": "67f10b97558ab0f6396b1455"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_75", "datetime": "2025-02-20T20:29:36.163Z", "sentiment": null}, offset: 70828
2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_75
2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1455"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_75", "datetime": "2025-02-20T20:29:36.163Z", "sentiment": null}%n
2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.3
score: 0.6

2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.3
score: 0.6

2025-04-13 11:43:20 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-60
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:20 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:20 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-60] Instantiated an idempotent producer.
2025-04-13 11:43:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544600871
2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_75', sentiment=magnitude: 1.3
score: 0.6
}
2025-04-13 11:43:20 [kafka-producer-network-thread | producer-60] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-60] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:20 [kafka-producer-network-thread | producer-60] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-60] ProducerId set to 9240 with epoch 0
2025-04-13 11:43:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:21 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_77, value: {"_id": {"$oid": "67f10b97558ab0f6396b1457"}, "review": "Smooth and easy to use.", "authId": "user_77", "datetime": "2025-01-05T21:10:33.619Z", "sentiment": null}, offset: 70829
2025-04-13 11:43:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_77
2025-04-13 11:43:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1457"}, "review": "Smooth and easy to use.", "authId": "user_77", "datetime": "2025-01-05T21:10:33.619Z", "sentiment": null}%n
2025-04-13 11:43:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = 
2025-04-13 11:43:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data 
2025-04-13 11:43:23 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-61
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:23 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:23 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-61] Instantiated an idempotent producer.
2025-04-13 11:43:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544603429
2025-04-13 11:43:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_77', sentiment=}
2025-04-13 11:43:23 [kafka-producer-network-thread | producer-61] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-61] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:23 [kafka-producer-network-thread | producer-61] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-61] ProducerId set to 9241 with epoch 0
2025-04-13 11:43:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:23 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_71, value: {"_id": {"$oid": "67f10b97558ab0f6396b1451"}, "review": "Excellent! Totally worth the price.", "authId": "user_71", "datetime": "2025-03-01T23:14:24.186Z", "sentiment": null}, offset: 70830
2025-04-13 11:43:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_71
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1451"}, "review": "Excellent! Totally worth the price.", "authId": "user_71", "datetime": "2025-03-01T23:14:24.186Z", "sentiment": null}%n
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:43:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-62
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-62] Instantiated an idempotent producer.
2025-04-13 11:43:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544606035
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_71', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:43:26 [kafka-producer-network-thread | producer-62] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-62] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:26 [kafka-producer-network-thread | producer-62] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-62] ProducerId set to 9242 with epoch 0
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_92, value: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}, offset: 70831
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_92
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}%n
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:43:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-63
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-63] Instantiated an idempotent producer.
2025-04-13 11:43:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544606572
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:26 [kafka-producer-network-thread | producer-63] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-63] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_92', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:43:26 [kafka-producer-network-thread | producer-63] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-63] ProducerId set to 9243 with epoch 0
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_61, value: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}, offset: 70832
2025-04-13 11:43:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_61
2025-04-13 11:43:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1447"}, "review": "Completely useless and overpriced.", "authId": "user_61", "datetime": "2024-12-04T20:18:24.313Z", "sentiment": null}%n
2025-04-13 11:43:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:28 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-64
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:28 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:28 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-64] Instantiated an idempotent producer.
2025-04-13 11:43:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544608073
2025-04-13 11:43:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_61', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:28 [kafka-producer-network-thread | producer-64] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-64] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:28 [kafka-producer-network-thread | producer-64] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-64] ProducerId set to 9244 with epoch 0
2025-04-13 11:43:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:28 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_45, value: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}, offset: 70833
2025-04-13 11:43:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_45
2025-04-13 11:43:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}%n
2025-04-13 11:43:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:29 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-65
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:29 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:29 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-65] Instantiated an idempotent producer.
2025-04-13 11:43:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544609579
2025-04-13 11:43:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:29 [kafka-producer-network-thread | producer-65] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-65] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_45', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:29 [kafka-producer-network-thread | producer-65] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-65] ProducerId set to 9245 with epoch 0
2025-04-13 11:43:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:29 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_95, value: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}, offset: 70834
2025-04-13 11:43:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_95
2025-04-13 11:43:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}%n
2025-04-13 11:43:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:31 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-66
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:31 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:31 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-66] Instantiated an idempotent producer.
2025-04-13 11:43:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544611155
2025-04-13 11:43:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:31 [kafka-producer-network-thread | producer-66] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-66] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_95', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:31 [kafka-producer-network-thread | producer-66] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-66] ProducerId set to 9246 with epoch 0
2025-04-13 11:43:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:31 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_95, value: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}, offset: 70835
2025-04-13 11:43:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_95
2025-04-13 11:43:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}%n
2025-04-13 11:43:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-67
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-67] Instantiated an idempotent producer.
2025-04-13 11:43:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544612615
2025-04-13 11:43:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_95', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:32 [kafka-producer-network-thread | producer-67] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-67] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:32 [kafka-producer-network-thread | producer-67] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-67] ProducerId set to 9247 with epoch 0
2025-04-13 11:43:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_100, value: {"_id": {"$oid": "67f10b97558ab0f6396b146e"}, "review": "Terrible experience, very disappointed.", "authId": "user_100", "datetime": "2025-02-03T10:22:13.147Z", "sentiment": null}, offset: 70836
2025-04-13 11:43:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_100
2025-04-13 11:43:34 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146e"}, "review": "Terrible experience, very disappointed.", "authId": "user_100", "datetime": "2025-02-03T10:22:13.147Z", "sentiment": null}%n
2025-04-13 11:43:34 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:34 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:34 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-68
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:34 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:34 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-68] Instantiated an idempotent producer.
2025-04-13 11:43:34 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:34 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:34 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544614115
2025-04-13 11:43:34 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:34 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_100', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:34 [kafka-producer-network-thread | producer-68] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-68] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:34 [kafka-producer-network-thread | producer-68] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-68] ProducerId set to 9248 with epoch 0
2025-04-13 11:43:34 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:34 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_30, value: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}, offset: 70837
2025-04-13 11:43:34 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_30
2025-04-13 11:43:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}%n
2025-04-13 11:43:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:43:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:43:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-69
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-69] Instantiated an idempotent producer.
2025-04-13 11:43:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544615574
2025-04-13 11:43:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:35 [kafka-producer-network-thread | producer-69] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-69] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_30', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:43:35 [kafka-producer-network-thread | producer-69] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-69] ProducerId set to 9249 with epoch 0
2025-04-13 11:43:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_2, value: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}, offset: 70838
2025-04-13 11:43:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_2
2025-04-13 11:43:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}%n
2025-04-13 11:43:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:43:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:43:37 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-70
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:37 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:37 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-70] Instantiated an idempotent producer.
2025-04-13 11:43:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544617019
2025-04-13 11:43:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:37 [kafka-producer-network-thread | producer-70] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-70] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_2', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:43:37 [kafka-producer-network-thread | producer-70] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-70] ProducerId set to 9250 with epoch 0
2025-04-13 11:43:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_2, value: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}, offset: 70839
2025-04-13 11:43:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_2
2025-04-13 11:43:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}%n
2025-04-13 11:43:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:43:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:43:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-71
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-71] Instantiated an idempotent producer.
2025-04-13 11:43:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544619513
2025-04-13 11:43:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_2', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:43:39 [kafka-producer-network-thread | producer-71] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-71] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:39 [kafka-producer-network-thread | producer-71] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-71] ProducerId set to 9251 with epoch 0
2025-04-13 11:43:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:39 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_41, value: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}, offset: 70840
2025-04-13 11:43:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_41
2025-04-13 11:43:40 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}%n
2025-04-13 11:43:40 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:40 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-72
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:40 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:40 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-72] Instantiated an idempotent producer.
2025-04-13 11:43:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544621003
2025-04-13 11:43:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_41', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:41 [kafka-producer-network-thread | producer-72] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-72] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:41 [kafka-producer-network-thread | producer-72] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-72] ProducerId set to 9252 with epoch 0
2025-04-13 11:43:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:41 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 70841
2025-04-13 11:43:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-13 11:43:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-13 11:43:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:42 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-73
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:42 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:42 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-73] Instantiated an idempotent producer.
2025-04-13 11:43:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544622470
2025-04-13 11:43:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:42 [kafka-producer-network-thread | producer-73] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-73] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:42 [kafka-producer-network-thread | producer-73] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-73] ProducerId set to 9253 with epoch 0
2025-04-13 11:43:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:42 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_79, value: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}, offset: 70842
2025-04-13 11:43:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_79
2025-04-13 11:43:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}%n
2025-04-13 11:43:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:43 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-74
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:43 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:43 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-74] Instantiated an idempotent producer.
2025-04-13 11:43:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544623967
2025-04-13 11:43:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_79', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:43 [kafka-producer-network-thread | producer-74] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-74] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:43 [kafka-producer-network-thread | producer-74] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-74] ProducerId set to 9254 with epoch 0
2025-04-13 11:43:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:43 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_47, value: {"_id": {"$oid": "67f10b97558ab0f6396b1439"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_47", "datetime": "2025-01-08T17:35:19.231Z", "sentiment": null}, offset: 70843
2025-04-13 11:43:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_47
2025-04-13 11:43:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1439"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_47", "datetime": "2025-01-08T17:35:19.231Z", "sentiment": null}%n
2025-04-13 11:43:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.8
score: 0.4

2025-04-13 11:43:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.8
score: 0.4

2025-04-13 11:43:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-75
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-75] Instantiated an idempotent producer.
2025-04-13 11:43:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544625462
2025-04-13 11:43:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_47', sentiment=magnitude: 0.8
score: 0.4
}
2025-04-13 11:43:45 [kafka-producer-network-thread | producer-75] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-75] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:45 [kafka-producer-network-thread | producer-75] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-75] ProducerId set to 9255 with epoch 0
2025-04-13 11:43:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_30, value: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}, offset: 70844
2025-04-13 11:43:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_30
2025-04-13 11:43:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}%n
2025-04-13 11:43:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:43:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:43:46 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-76
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:46 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:46 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-76] Instantiated an idempotent producer.
2025-04-13 11:43:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544626926
2025-04-13 11:43:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:46 [kafka-producer-network-thread | producer-76] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-76] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_30', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:43:46 [kafka-producer-network-thread | producer-76] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-76] ProducerId set to 9256 with epoch 0
2025-04-13 11:43:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:46 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_33, value: {"_id": {"$oid": "67f10b97558ab0f6396b142b"}, "review": "Terrible experience, very disappointed.", "authId": "user_33", "datetime": "2024-12-24T07:32:50.779Z", "sentiment": null}, offset: 70845
2025-04-13 11:43:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_33
2025-04-13 11:43:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142b"}, "review": "Terrible experience, very disappointed.", "authId": "user_33", "datetime": "2024-12-24T07:32:50.779Z", "sentiment": null}%n
2025-04-13 11:43:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:48 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-77
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:48 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:48 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-77] Instantiated an idempotent producer.
2025-04-13 11:43:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544628416
2025-04-13 11:43:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_33', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:48 [kafka-producer-network-thread | producer-77] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-77] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:48 [kafka-producer-network-thread | producer-77] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-77] ProducerId set to 9257 with epoch 0
2025-04-13 11:43:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:48 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_18, value: {"_id": {"$oid": "67f10b97558ab0f6396b141c"}, "review": "Doesn't work as advertised.", "authId": "user_18", "datetime": "2025-02-20T09:53:18.980Z", "sentiment": null}, offset: 70846
2025-04-13 11:43:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_18
2025-04-13 11:43:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141c"}, "review": "Doesn't work as advertised.", "authId": "user_18", "datetime": "2025-02-20T09:53:18.980Z", "sentiment": null}%n
2025-04-13 11:43:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:49 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-78
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:49 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:49 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-78] Instantiated an idempotent producer.
2025-04-13 11:43:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544629905
2025-04-13 11:43:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_18', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:49 [kafka-producer-network-thread | producer-78] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-78] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:49 [kafka-producer-network-thread | producer-78] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-78] ProducerId set to 9258 with epoch 0
2025-04-13 11:43:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:49 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_86, value: {"_id": {"$oid": "67f10b97558ab0f6396b1460"}, "review": "Smooth and easy to use.", "authId": "user_86", "datetime": "2025-02-22T20:30:11.182Z", "sentiment": null}, offset: 70847
2025-04-13 11:43:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_86
2025-04-13 11:43:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1460"}, "review": "Smooth and easy to use.", "authId": "user_86", "datetime": "2025-02-22T20:30:11.182Z", "sentiment": null}%n
2025-04-13 11:43:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = 
2025-04-13 11:43:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data 
2025-04-13 11:43:51 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-79
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:51 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:51 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-79] Instantiated an idempotent producer.
2025-04-13 11:43:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544631372
2025-04-13 11:43:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_86', sentiment=}
2025-04-13 11:43:51 [kafka-producer-network-thread | producer-79] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-79] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:51 [kafka-producer-network-thread | producer-79] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-79] ProducerId set to 9259 with epoch 0
2025-04-13 11:43:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:51 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_67, value: {"_id": {"$oid": "67f10b97558ab0f6396b144d"}, "review": "Works like a charm, perfect!", "authId": "user_67", "datetime": "2025-02-10T15:03:00.523Z", "sentiment": null}, offset: 70848
2025-04-13 11:43:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_67
2025-04-13 11:43:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144d"}, "review": "Works like a charm, perfect!", "authId": "user_67", "datetime": "2025-02-10T15:03:00.523Z", "sentiment": null}%n
2025-04-13 11:43:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 11:43:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 11:43:52 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-80
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:52 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:52 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-80] Instantiated an idempotent producer.
2025-04-13 11:43:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544632863
2025-04-13 11:43:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_67', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 11:43:52 [kafka-producer-network-thread | producer-80] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-80] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:52 [kafka-producer-network-thread | producer-80] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-80] ProducerId set to 9260 with epoch 0
2025-04-13 11:43:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:53 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_95, value: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}, offset: 70849
2025-04-13 11:43:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_95
2025-04-13 11:43:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}%n
2025-04-13 11:43:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-81
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-81] Instantiated an idempotent producer.
2025-04-13 11:43:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544635384
2025-04-13 11:43:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_95', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:55 [kafka-producer-network-thread | producer-81] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-81] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:55 [kafka-producer-network-thread | producer-81] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-81] ProducerId set to 9261 with epoch 0
2025-04-13 11:43:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_30, value: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}, offset: 70850
2025-04-13 11:43:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_30
2025-04-13 11:43:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}%n
2025-04-13 11:43:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:43:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:43:56 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-82
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:56 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:56 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-82] Instantiated an idempotent producer.
2025-04-13 11:43:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544636894
2025-04-13 11:43:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_30', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:43:56 [kafka-producer-network-thread | producer-82] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-82] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:56 [kafka-producer-network-thread | producer-82] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-82] ProducerId set to 9262 with epoch 0
2025-04-13 11:43:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:56 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_62, value: {"_id": {"$oid": "67f10b97558ab0f6396b1448"}, "review": "Poor build quality, not reliable.", "authId": "user_62", "datetime": "2025-03-04T10:01:05.882Z", "sentiment": null}, offset: 70851
2025-04-13 11:43:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_62
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1448"}, "review": "Poor build quality, not reliable.", "authId": "user_62", "datetime": "2025-03-04T10:01:05.882Z", "sentiment": null}%n
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:59 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-83
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:59 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:59 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-83] Instantiated an idempotent producer.
2025-04-13 11:43:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544639270
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_62', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:59 [kafka-producer-network-thread | producer-83] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-83] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:59 [kafka-producer-network-thread | producer-83] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-83] ProducerId set to 9263 with epoch 0
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_18, value: {"_id": {"$oid": "67f10b97558ab0f6396b141c"}, "review": "Doesn't work as advertised.", "authId": "user_18", "datetime": "2025-02-20T09:53:18.980Z", "sentiment": null}, offset: 70852
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_18
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141c"}, "review": "Doesn't work as advertised.", "authId": "user_18", "datetime": "2025-02-20T09:53:18.980Z", "sentiment": null}%n
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:43:59 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-84
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:43:59 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:43:59 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-84] Instantiated an idempotent producer.
2025-04-13 11:43:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:43:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:43:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544639770
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_18', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:43:59 [kafka-producer-network-thread | producer-84] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-84] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:43:59 [kafka-producer-network-thread | producer-84] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-84] ProducerId set to 9264 with epoch 0
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_6, value: {"_id": {"$oid": "67f10b97558ab0f6396b1410"}, "review": "Im very happy with this purchase!", "authId": "user_6", "datetime": "2025-01-21T18:03:15.045Z", "sentiment": null}, offset: 70853
2025-04-13 11:43:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_6
2025-04-13 11:44:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1410"}, "review": "Im very happy with this purchase!", "authId": "user_6", "datetime": "2025-01-21T18:03:15.045Z", "sentiment": null}%n
2025-04-13 11:44:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:44:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:44:00 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-85
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:00 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:00 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-85] Instantiated an idempotent producer.
2025-04-13 11:44:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544640276
2025-04-13 11:44:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:00 [kafka-producer-network-thread | producer-85] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-85] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_6', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:44:00 [kafka-producer-network-thread | producer-85] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-85] ProducerId set to 9265 with epoch 0
2025-04-13 11:44:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:00 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_40, value: {"_id": {"$oid": "67f10b97558ab0f6396b1432"}, "review": "This changed my life!", "authId": "user_40", "datetime": "2025-03-22T15:49:19.758Z", "sentiment": null}, offset: 70854
2025-04-13 11:44:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_40
2025-04-13 11:44:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1432"}, "review": "This changed my life!", "authId": "user_40", "datetime": "2025-03-22T15:49:19.758Z", "sentiment": null}%n
2025-04-13 11:44:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:44:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:44:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-86
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-86] Instantiated an idempotent producer.
2025-04-13 11:44:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544641736
2025-04-13 11:44:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:01 [kafka-producer-network-thread | producer-86] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-86] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_40', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:44:01 [kafka-producer-network-thread | producer-86] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-86] ProducerId set to 9266 with epoch 0
2025-04-13 11:44:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:01 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_75, value: {"_id": {"$oid": "67f10b97558ab0f6396b1455"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_75", "datetime": "2025-02-20T20:29:36.163Z", "sentiment": null}, offset: 70855
2025-04-13 11:44:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_75
2025-04-13 11:44:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1455"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_75", "datetime": "2025-02-20T20:29:36.163Z", "sentiment": null}%n
2025-04-13 11:44:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.3
score: 0.6

2025-04-13 11:44:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.3
score: 0.6

2025-04-13 11:44:03 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-87
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:03 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:03 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-87] Instantiated an idempotent producer.
2025-04-13 11:44:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544643216
2025-04-13 11:44:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_75', sentiment=magnitude: 1.3
score: 0.6
}
2025-04-13 11:44:03 [kafka-producer-network-thread | producer-87] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-87] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:03 [kafka-producer-network-thread | producer-87] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-87] ProducerId set to 9267 with epoch 0
2025-04-13 11:44:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:03 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_53, value: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}, offset: 70856
2025-04-13 11:44:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_53
2025-04-13 11:44:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}%n
2025-04-13 11:44:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:44:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:44:04 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-88
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:04 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:04 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-88] Instantiated an idempotent producer.
2025-04-13 11:44:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544644724
2025-04-13 11:44:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_53', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:44:04 [kafka-producer-network-thread | producer-88] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-88] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:04 [kafka-producer-network-thread | producer-88] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-88] ProducerId set to 9268 with epoch 0
2025-04-13 11:44:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:04 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_35, value: {"_id": {"$oid": "67f10b97558ab0f6396b142d"}, "review": "Terrible experience, very disappointed.", "authId": "user_35", "datetime": "2025-03-06T15:09:01.869Z", "sentiment": null}, offset: 70857
2025-04-13 11:44:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_35
2025-04-13 11:44:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142d"}, "review": "Terrible experience, very disappointed.", "authId": "user_35", "datetime": "2025-03-06T15:09:01.869Z", "sentiment": null}%n
2025-04-13 11:44:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:44:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:44:06 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-89
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:06 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:06 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-89] Instantiated an idempotent producer.
2025-04-13 11:44:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544646207
2025-04-13 11:44:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_35', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:44:06 [kafka-producer-network-thread | producer-89] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-89] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:06 [kafka-producer-network-thread | producer-89] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-89] ProducerId set to 9269 with epoch 0
2025-04-13 11:44:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:06 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_2, value: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}, offset: 70858
2025-04-13 11:44:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_2
2025-04-13 11:44:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}%n
2025-04-13 11:44:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:44:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:44:07 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-90
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:07 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:07 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-90] Instantiated an idempotent producer.
2025-04-13 11:44:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544647694
2025-04-13 11:44:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_2', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:44:07 [kafka-producer-network-thread | producer-90] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-90] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:07 [kafka-producer-network-thread | producer-90] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-90] ProducerId set to 9270 with epoch 0
2025-04-13 11:44:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:08 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_57, value: {"_id": {"$oid": "67f10b97558ab0f6396b1443"}, "review": "This changed my life!", "authId": "user_57", "datetime": "2025-03-21T00:28:58.918Z", "sentiment": null}, offset: 70859
2025-04-13 11:44:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_57
2025-04-13 11:44:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1443"}, "review": "This changed my life!", "authId": "user_57", "datetime": "2025-03-21T00:28:58.918Z", "sentiment": null}%n
2025-04-13 11:44:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:44:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:44:10 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-91
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:10 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:10 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-91] Instantiated an idempotent producer.
2025-04-13 11:44:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544650224
2025-04-13 11:44:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_57', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:44:10 [kafka-producer-network-thread | producer-91] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-91] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:10 [kafka-producer-network-thread | producer-91] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-91] ProducerId set to 9271 with epoch 0
2025-04-13 11:44:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:10 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_95, value: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}, offset: 70860
2025-04-13 11:44:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_95
2025-04-13 11:44:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}%n
2025-04-13 11:44:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:44:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:44:11 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-92
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:11 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:11 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-92] Instantiated an idempotent producer.
2025-04-13 11:44:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544651696
2025-04-13 11:44:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_95', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:44:11 [kafka-producer-network-thread | producer-92] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-92] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:11 [kafka-producer-network-thread | producer-92] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-92] ProducerId set to 9272 with epoch 0
2025-04-13 11:44:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:11 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_87, value: {"_id": {"$oid": "67f10b97558ab0f6396b1461"}, "review": "Im very happy with this purchase!", "authId": "user_87", "datetime": "2025-03-25T22:05:56.300Z", "sentiment": null}, offset: 70861
2025-04-13 11:44:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_87
2025-04-13 11:44:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1461"}, "review": "Im very happy with this purchase!", "authId": "user_87", "datetime": "2025-03-25T22:05:56.300Z", "sentiment": null}%n
2025-04-13 11:44:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:44:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:44:13 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-93
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:13 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:13 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-93] Instantiated an idempotent producer.
2025-04-13 11:44:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544653157
2025-04-13 11:44:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_87', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:44:13 [kafka-producer-network-thread | producer-93] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-93] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:13 [kafka-producer-network-thread | producer-93] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-93] ProducerId set to 9273 with epoch 0
2025-04-13 11:44:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:13 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_11, value: {"_id": {"$oid": "67f10b97558ab0f6396b1415"}, "review": "Would not recommend to anyone.", "authId": "user_11", "datetime": "2024-12-10T23:20:11.570Z", "sentiment": null}, offset: 70862
2025-04-13 11:44:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_11
2025-04-13 11:44:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1415"}, "review": "Would not recommend to anyone.", "authId": "user_11", "datetime": "2024-12-10T23:20:11.570Z", "sentiment": null}%n
2025-04-13 11:44:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:44:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:44:14 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-94
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:14 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:14 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-94] Instantiated an idempotent producer.
2025-04-13 11:44:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544654625
2025-04-13 11:44:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_11', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:44:14 [kafka-producer-network-thread | producer-94] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-94] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:14 [kafka-producer-network-thread | producer-94] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-94] ProducerId set to 9274 with epoch 0
2025-04-13 11:44:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:14 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_65, value: {"_id": {"$oid": "67f10b97558ab0f6396b144b"}, "review": "Horrible customer service.", "authId": "user_65", "datetime": "2025-01-22T00:39:23.653Z", "sentiment": null}, offset: 70863
2025-04-13 11:44:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_65
2025-04-13 11:44:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144b"}, "review": "Horrible customer service.", "authId": "user_65", "datetime": "2025-01-22T00:39:23.653Z", "sentiment": null}%n
2025-04-13 11:44:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:44:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:44:16 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-95
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:16 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:16 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-95] Instantiated an idempotent producer.
2025-04-13 11:44:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544656151
2025-04-13 11:44:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_65', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:44:16 [kafka-producer-network-thread | producer-95] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-95] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:16 [kafka-producer-network-thread | producer-95] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-95] ProducerId set to 9275 with epoch 0
2025-04-13 11:44:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:16 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_3, value: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}, offset: 70864
2025-04-13 11:44:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_3
2025-04-13 11:44:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}%n
2025-04-13 11:44:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 11:44:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 11:44:17 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-96
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:17 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:17 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-96] Instantiated an idempotent producer.
2025-04-13 11:44:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544657618
2025-04-13 11:44:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:17 [kafka-producer-network-thread | producer-96] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-96] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_3', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 11:44:17 [kafka-producer-network-thread | producer-96] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-96] ProducerId set to 9276 with epoch 0
2025-04-13 11:44:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:17 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_56, value: {"_id": {"$oid": "67f10b97558ab0f6396b1442"}, "review": "It's okay, does the job.", "authId": "user_56", "datetime": "2024-12-14T20:56:35.004Z", "sentiment": null}, offset: 70865
2025-04-13 11:44:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_56
2025-04-13 11:44:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1442"}, "review": "It's okay, does the job.", "authId": "user_56", "datetime": "2024-12-14T20:56:35.004Z", "sentiment": null}%n
2025-04-13 11:44:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-13 11:44:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-13 11:44:19 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-97
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:19 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:19 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-97] Instantiated an idempotent producer.
2025-04-13 11:44:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544659099
2025-04-13 11:44:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_56', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-13 11:44:19 [kafka-producer-network-thread | producer-97] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-97] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:19 [kafka-producer-network-thread | producer-97] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-97] ProducerId set to 9277 with epoch 0
2025-04-13 11:44:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:19 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_8, value: {"_id": {"$oid": "67f10b97558ab0f6396b1412"}, "review": "Serves its purpose, but nothing more.", "authId": "user_8", "datetime": "2024-12-17T21:27:23.328Z", "sentiment": null}, offset: 70866
2025-04-13 11:44:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_8
2025-04-13 11:44:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1412"}, "review": "Serves its purpose, but nothing more.", "authId": "user_8", "datetime": "2024-12-17T21:27:23.328Z", "sentiment": null}%n
2025-04-13 11:44:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:44:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:44:20 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-98
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:20 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:20 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-98] Instantiated an idempotent producer.
2025-04-13 11:44:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544660544
2025-04-13 11:44:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_8', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:44:20 [kafka-producer-network-thread | producer-98] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-98] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:20 [kafka-producer-network-thread | producer-98] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-98] ProducerId set to 9278 with epoch 0
2025-04-13 11:44:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:20 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_37, value: {"_id": {"$oid": "67f10b97558ab0f6396b142f"}, "review": "Mediocre quality, nothing special.", "authId": "user_37", "datetime": "2024-12-22T11:29:54.974Z", "sentiment": null}, offset: 70867
2025-04-13 11:44:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_37
2025-04-13 11:44:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142f"}, "review": "Mediocre quality, nothing special.", "authId": "user_37", "datetime": "2024-12-22T11:29:54.974Z", "sentiment": null}%n
2025-04-13 11:44:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:44:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:44:21 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-99
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:21 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:21 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-99] Instantiated an idempotent producer.
2025-04-13 11:44:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544661046
2025-04-13 11:44:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_37', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:44:21 [kafka-producer-network-thread | producer-99] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-99] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:21 [kafka-producer-network-thread | producer-99] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-99] ProducerId set to 9279 with epoch 0
2025-04-13 11:44:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:21 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_23, value: {"_id": {"$oid": "67f10b97558ab0f6396b1421"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_23", "datetime": "2025-01-08T03:24:13.109Z", "sentiment": null}, offset: 70868
2025-04-13 11:44:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_23
2025-04-13 11:44:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1421"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_23", "datetime": "2025-01-08T03:24:13.109Z", "sentiment": null}%n
2025-04-13 11:44:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:44:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:44:23 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-100
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:23 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:23 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-100] Instantiated an idempotent producer.
2025-04-13 11:44:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544663557
2025-04-13 11:44:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:23 [kafka-producer-network-thread | producer-100] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-100] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_23', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:44:23 [kafka-producer-network-thread | producer-100] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-100] ProducerId set to 9280 with epoch 0
2025-04-13 11:44:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:24 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_76, value: {"_id": {"$oid": "67f10b97558ab0f6396b1456"}, "review": "Great quality, will buy again.", "authId": "user_76", "datetime": "2025-01-27T21:25:36.715Z", "sentiment": null}, offset: 70869
2025-04-13 11:44:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_76
2025-04-13 11:44:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1456"}, "review": "Great quality, will buy again.", "authId": "user_76", "datetime": "2025-01-27T21:25:36.715Z", "sentiment": null}%n
2025-04-13 11:44:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: 0.1

2025-04-13 11:44:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: 0.1

2025-04-13 11:44:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-101
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-101] Instantiated an idempotent producer.
2025-04-13 11:44:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544666062
2025-04-13 11:44:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_76', sentiment=magnitude: 0.1
score: 0.1
}
2025-04-13 11:44:26 [kafka-producer-network-thread | producer-101] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-101] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:26 [kafka-producer-network-thread | producer-101] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-101] ProducerId set to 9281 with epoch 0
2025-04-13 11:44:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:26 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_81, value: {"_id": {"$oid": "67f10b97558ab0f6396b145b"}, "review": "Not bad, not great either.", "authId": "user_81", "datetime": "2025-03-11T12:19:55.240Z", "sentiment": null}, offset: 70870
2025-04-13 11:44:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_81
2025-04-13 11:44:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145b"}, "review": "Not bad, not great either.", "authId": "user_81", "datetime": "2025-03-11T12:19:55.240Z", "sentiment": null}%n
2025-04-13 11:44:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:44:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:44:27 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-102
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:27 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:27 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-102] Instantiated an idempotent producer.
2025-04-13 11:44:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544667679
2025-04-13 11:44:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_81', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:44:27 [kafka-producer-network-thread | producer-102] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-102] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:27 [kafka-producer-network-thread | producer-102] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-102] ProducerId set to 9282 with epoch 0
2025-04-13 11:44:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:27 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_100, value: {"_id": {"$oid": "67f10b97558ab0f6396b146e"}, "review": "Terrible experience, very disappointed.", "authId": "user_100", "datetime": "2025-02-03T10:22:13.147Z", "sentiment": null}, offset: 70871
2025-04-13 11:44:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_100
2025-04-13 11:44:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146e"}, "review": "Terrible experience, very disappointed.", "authId": "user_100", "datetime": "2025-02-03T10:22:13.147Z", "sentiment": null}%n
2025-04-13 11:44:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:44:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:44:29 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-103
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:29 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:29 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-103] Instantiated an idempotent producer.
2025-04-13 11:44:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544669161
2025-04-13 11:44:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_100', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:44:29 [kafka-producer-network-thread | producer-103] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-103] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:29 [kafka-producer-network-thread | producer-103] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-103] ProducerId set to 9283 with epoch 0
2025-04-13 11:44:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:29 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_45, value: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}, offset: 70872
2025-04-13 11:44:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_45
2025-04-13 11:44:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}%n
2025-04-13 11:44:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:44:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:44:31 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-104
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:31 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:31 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-104] Instantiated an idempotent producer.
2025-04-13 11:44:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544671679
2025-04-13 11:44:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_45', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:44:31 [kafka-producer-network-thread | producer-104] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-104] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:31 [kafka-producer-network-thread | producer-104] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-104] ProducerId set to 9284 with epoch 0
2025-04-13 11:44:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:31 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_19, value: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}, offset: 70873
2025-04-13 11:44:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_19
2025-04-13 11:44:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}%n
2025-04-13 11:44:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:44:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:44:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-105
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-105] Instantiated an idempotent producer.
2025-04-13 11:44:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544672172
2025-04-13 11:44:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_19', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:44:32 [kafka-producer-network-thread | producer-105] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-105] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:32 [kafka-producer-network-thread | producer-105] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-105] ProducerId set to 9285 with epoch 0
2025-04-13 11:44:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_19, value: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}, offset: 70874
2025-04-13 11:44:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_19
2025-04-13 11:44:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141d"}, "review": "This changed my life!", "authId": "user_19", "datetime": "2025-03-03T03:16:33.431Z", "sentiment": null}%n
2025-04-13 11:44:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:44:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:44:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-106
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-106] Instantiated an idempotent producer.
2025-04-13 11:44:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544673615
2025-04-13 11:44:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_19', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:44:33 [kafka-producer-network-thread | producer-106] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-106] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:33 [kafka-producer-network-thread | producer-106] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-106] ProducerId set to 9286 with epoch 0
2025-04-13 11:44:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_17, value: {"_id": {"$oid": "67f10b97558ab0f6396b141b"}, "review": "Terrible experience, very disappointed.", "authId": "user_17", "datetime": "2025-03-01T00:30:53.030Z", "sentiment": null}, offset: 70875
2025-04-13 11:44:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_17
2025-04-13 11:44:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141b"}, "review": "Terrible experience, very disappointed.", "authId": "user_17", "datetime": "2025-03-01T00:30:53.030Z", "sentiment": null}%n
2025-04-13 11:44:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:44:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:44:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-107
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-107] Instantiated an idempotent producer.
2025-04-13 11:44:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544675096
2025-04-13 11:44:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_17', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:44:35 [kafka-producer-network-thread | producer-107] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-107] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:35 [kafka-producer-network-thread | producer-107] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-107] ProducerId set to 9287 with epoch 0
2025-04-13 11:44:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_24, value: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}, offset: 70876
2025-04-13 11:44:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_24
2025-04-13 11:44:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}%n
2025-04-13 11:44:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:44:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:44:36 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-108
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:36 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:36 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-108] Instantiated an idempotent producer.
2025-04-13 11:44:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544676547
2025-04-13 11:44:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_24', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:44:36 [kafka-producer-network-thread | producer-108] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-108] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:36 [kafka-producer-network-thread | producer-108] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-108] ProducerId set to 9288 with epoch 0
2025-04-13 11:44:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:36 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_96, value: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}, offset: 70877
2025-04-13 11:44:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_96
2025-04-13 11:44:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}%n
2025-04-13 11:44:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:44:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:44:38 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-109
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:38 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:38 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-109] Instantiated an idempotent producer.
2025-04-13 11:44:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:38 [kafka-producer-network-thread | producer-109] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-109] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544678113
2025-04-13 11:44:38 [kafka-producer-network-thread | producer-109] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-109] ProducerId set to 9289 with epoch 0
2025-04-13 11:44:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_96', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:44:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_62, value: {"_id": {"$oid": "67f10b97558ab0f6396b1448"}, "review": "Poor build quality, not reliable.", "authId": "user_62", "datetime": "2025-03-04T10:01:05.882Z", "sentiment": null}, offset: 70878
2025-04-13 11:44:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_62
2025-04-13 11:44:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1448"}, "review": "Poor build quality, not reliable.", "authId": "user_62", "datetime": "2025-03-04T10:01:05.882Z", "sentiment": null}%n
2025-04-13 11:44:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:44:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:44:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-110
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-110] Instantiated an idempotent producer.
2025-04-13 11:44:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544679740
2025-04-13 11:44:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_62', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:44:39 [kafka-producer-network-thread | producer-110] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-110] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:39 [kafka-producer-network-thread | producer-110] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-110] ProducerId set to 9290 with epoch 0
2025-04-13 11:44:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:40 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_42, value: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}, offset: 70879
2025-04-13 11:44:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_42
2025-04-13 11:44:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}%n
2025-04-13 11:44:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:44:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:44:42 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-111
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:42 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:42 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-111] Instantiated an idempotent producer.
2025-04-13 11:44:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544682342
2025-04-13 11:44:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_42', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:44:42 [kafka-producer-network-thread | producer-111] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-111] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:42 [kafka-producer-network-thread | producer-111] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-111] ProducerId set to 9291 with epoch 0
2025-04-13 11:44:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:42 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_96, value: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}, offset: 70880
2025-04-13 11:44:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_96
2025-04-13 11:44:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}%n
2025-04-13 11:44:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:44:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:44:43 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-112
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:43 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:43 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-112] Instantiated an idempotent producer.
2025-04-13 11:44:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544683836
2025-04-13 11:44:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_96', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:44:43 [kafka-producer-network-thread | producer-112] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-112] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:43 [kafka-producer-network-thread | producer-112] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-112] ProducerId set to 9292 with epoch 0
2025-04-13 11:44:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:43 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_36, value: {"_id": {"$oid": "67f10b97558ab0f6396b142e"}, "review": "Works like a charm, perfect!", "authId": "user_36", "datetime": "2024-12-07T01:28:05.981Z", "sentiment": null}, offset: 70881
2025-04-13 11:44:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_36
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142e"}, "review": "Works like a charm, perfect!", "authId": "user_36", "datetime": "2024-12-07T01:28:05.981Z", "sentiment": null}%n
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:44:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-113
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-113] Instantiated an idempotent producer.
2025-04-13 11:44:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544685281
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_36', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:44:45 [kafka-producer-network-thread | producer-113] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-113] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:45 [kafka-producer-network-thread | producer-113] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-113] ProducerId set to 9293 with epoch 0
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_18, value: {"_id": {"$oid": "67f10b97558ab0f6396b141c"}, "review": "Doesn't work as advertised.", "authId": "user_18", "datetime": "2025-02-20T09:53:18.980Z", "sentiment": null}, offset: 70882
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_18
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141c"}, "review": "Doesn't work as advertised.", "authId": "user_18", "datetime": "2025-02-20T09:53:18.980Z", "sentiment": null}%n
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:44:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-114
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-114] Instantiated an idempotent producer.
2025-04-13 11:44:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544685910
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_18', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:44:45 [kafka-producer-network-thread | producer-114] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-114] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:45 [kafka-producer-network-thread | producer-114] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-114] ProducerId set to 9294 with epoch 0
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_53, value: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}, offset: 70883
2025-04-13 11:44:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_53
2025-04-13 11:44:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}%n
2025-04-13 11:44:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:44:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:44:47 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-115
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:47 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:47 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-115] Instantiated an idempotent producer.
2025-04-13 11:44:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544687397
2025-04-13 11:44:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_53', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:44:47 [kafka-producer-network-thread | producer-115] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-115] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:47 [kafka-producer-network-thread | producer-115] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-115] ProducerId set to 9295 with epoch 0
2025-04-13 11:44:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:47 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_85, value: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}, offset: 70884
2025-04-13 11:44:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_85
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}%n
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:44:49 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-116
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:49 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:49 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-116] Instantiated an idempotent producer.
2025-04-13 11:44:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544689015
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_85', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:44:49 [kafka-producer-network-thread | producer-116] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-116] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:49 [kafka-producer-network-thread | producer-116] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-116] ProducerId set to 9296 with epoch 0
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_70, value: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}, offset: 70885
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_70
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}%n
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:44:49 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-117
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:49 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:49 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-117] Instantiated an idempotent producer.
2025-04-13 11:44:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544689526
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:49 [kafka-producer-network-thread | producer-117] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-117] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_70', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:44:49 [kafka-producer-network-thread | producer-117] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-117] ProducerId set to 9297 with epoch 0
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_74, value: {"_id": {"$oid": "67f10b97558ab0f6396b1454"}, "review": "Works like a charm, perfect!", "authId": "user_74", "datetime": "2024-12-02T13:29:55.232Z", "sentiment": null}, offset: 70886
2025-04-13 11:44:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_74
2025-04-13 11:44:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1454"}, "review": "Works like a charm, perfect!", "authId": "user_74", "datetime": "2024-12-02T13:29:55.232Z", "sentiment": null}%n
2025-04-13 11:44:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:44:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:44:51 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-118
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:51 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:51 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-118] Instantiated an idempotent producer.
2025-04-13 11:44:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544691183
2025-04-13 11:44:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:51 [kafka-producer-network-thread | producer-118] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-118] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_74', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:44:51 [kafka-producer-network-thread | producer-118] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-118] ProducerId set to 9298 with epoch 0
2025-04-13 11:44:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:51 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_15, value: {"_id": {"$oid": "67f10b97558ab0f6396b1419"}, "review": "Mediocre quality, nothing special.", "authId": "user_15", "datetime": "2024-12-29T15:59:17.459Z", "sentiment": null}, offset: 70887
2025-04-13 11:44:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_15
2025-04-13 11:44:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1419"}, "review": "Mediocre quality, nothing special.", "authId": "user_15", "datetime": "2024-12-29T15:59:17.459Z", "sentiment": null}%n
2025-04-13 11:44:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:44:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:44:52 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-119
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:52 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:52 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-119] Instantiated an idempotent producer.
2025-04-13 11:44:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544692649
2025-04-13 11:44:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_15', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:44:52 [kafka-producer-network-thread | producer-119] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-119] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:52 [kafka-producer-network-thread | producer-119] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-119] ProducerId set to 9299 with epoch 0
2025-04-13 11:44:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:52 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_87, value: {"_id": {"$oid": "67f10b97558ab0f6396b1461"}, "review": "Im very happy with this purchase!", "authId": "user_87", "datetime": "2025-03-25T22:05:56.300Z", "sentiment": null}, offset: 70888
2025-04-13 11:44:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_87
2025-04-13 11:44:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1461"}, "review": "Im very happy with this purchase!", "authId": "user_87", "datetime": "2025-03-25T22:05:56.300Z", "sentiment": null}%n
2025-04-13 11:44:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:44:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:44:53 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-120
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:53 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:53 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-120] Instantiated an idempotent producer.
2025-04-13 11:44:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544693180
2025-04-13 11:44:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_87', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:44:53 [kafka-producer-network-thread | producer-120] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-120] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:53 [kafka-producer-network-thread | producer-120] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-120] ProducerId set to 9300 with epoch 0
2025-04-13 11:44:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:54 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_80, value: {"_id": {"$oid": "67f10b97558ab0f6396b145a"}, "review": "Excellent! Totally worth the price.", "authId": "user_80", "datetime": "2024-12-05T07:49:50.162Z", "sentiment": null}, offset: 70889
2025-04-13 11:44:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_80
2025-04-13 11:44:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145a"}, "review": "Excellent! Totally worth the price.", "authId": "user_80", "datetime": "2024-12-05T07:49:50.162Z", "sentiment": null}%n
2025-04-13 11:44:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.4
score: 0.7

2025-04-13 11:44:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.4
score: 0.7

2025-04-13 11:44:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-121
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-121] Instantiated an idempotent producer.
2025-04-13 11:44:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544695645
2025-04-13 11:44:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_80', sentiment=magnitude: 1.4
score: 0.7
}
2025-04-13 11:44:55 [kafka-producer-network-thread | producer-121] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-121] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:55 [kafka-producer-network-thread | producer-121] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-121] ProducerId set to 9301 with epoch 0
2025-04-13 11:44:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_92, value: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}, offset: 70890
2025-04-13 11:44:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_92
2025-04-13 11:44:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}%n
2025-04-13 11:44:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:44:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:44:57 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-122
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:57 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:57 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-122] Instantiated an idempotent producer.
2025-04-13 11:44:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544697324
2025-04-13 11:44:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_92', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:44:57 [kafka-producer-network-thread | producer-122] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-122] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:57 [kafka-producer-network-thread | producer-122] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-122] ProducerId set to 9302 with epoch 0
2025-04-13 11:44:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:57 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_21, value: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}, offset: 70891
2025-04-13 11:44:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_21
2025-04-13 11:44:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}%n
2025-04-13 11:44:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:44:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:44:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-123
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:44:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:44:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-123] Instantiated an idempotent producer.
2025-04-13 11:44:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:44:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:44:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544698774
2025-04-13 11:44:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:44:58 [kafka-producer-network-thread | producer-123] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-123] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:44:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_21', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:44:58 [kafka-producer-network-thread | producer-123] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-123] ProducerId set to 9303 with epoch 0
2025-04-13 11:44:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:44:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_99, value: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}, offset: 70892
2025-04-13 11:44:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_99
2025-04-13 11:45:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}%n
2025-04-13 11:45:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:45:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:45:00 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-124
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:00 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:00 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-124] Instantiated an idempotent producer.
2025-04-13 11:45:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544700253
2025-04-13 11:45:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_99', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:45:00 [kafka-producer-network-thread | producer-124] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-124] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:00 [kafka-producer-network-thread | producer-124] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-124] ProducerId set to 9304 with epoch 0
2025-04-13 11:45:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:00 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_96, value: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}, offset: 70893
2025-04-13 11:45:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_96
2025-04-13 11:45:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}%n
2025-04-13 11:45:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:45:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:45:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-125
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-125] Instantiated an idempotent producer.
2025-04-13 11:45:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544701706
2025-04-13 11:45:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:01 [kafka-producer-network-thread | producer-125] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-125] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_96', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:45:01 [kafka-producer-network-thread | producer-125] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-125] ProducerId set to 9305 with epoch 0
2025-04-13 11:45:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:01 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_16, value: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}, offset: 70894
2025-04-13 11:45:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_16
2025-04-13 11:45:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}%n
2025-04-13 11:45:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:45:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:45:02 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-126
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:02 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:02 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-126] Instantiated an idempotent producer.
2025-04-13 11:45:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544702272
2025-04-13 11:45:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_16', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:45:02 [kafka-producer-network-thread | producer-126] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-126] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:02 [kafka-producer-network-thread | producer-126] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-126] ProducerId set to 9306 with epoch 0
2025-04-13 11:45:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:02 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_74, value: {"_id": {"$oid": "67f10b97558ab0f6396b1454"}, "review": "Works like a charm, perfect!", "authId": "user_74", "datetime": "2024-12-02T13:29:55.232Z", "sentiment": null}, offset: 70895
2025-04-13 11:45:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_74
2025-04-13 11:45:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1454"}, "review": "Works like a charm, perfect!", "authId": "user_74", "datetime": "2024-12-02T13:29:55.232Z", "sentiment": null}%n
2025-04-13 11:45:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:45:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:45:04 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-127
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:04 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:04 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-127] Instantiated an idempotent producer.
2025-04-13 11:45:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544704798
2025-04-13 11:45:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_74', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:45:04 [kafka-producer-network-thread | producer-127] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-127] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:04 [kafka-producer-network-thread | producer-127] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-127] ProducerId set to 9307 with epoch 0
2025-04-13 11:45:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:04 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_13, value: {"_id": {"$oid": "67f10b97558ab0f6396b1417"}, "review": "It broke after a week of use.", "authId": "user_13", "datetime": "2024-12-05T23:56:15.604Z", "sentiment": null}, offset: 70896
2025-04-13 11:45:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_13
2025-04-13 11:45:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1417"}, "review": "It broke after a week of use.", "authId": "user_13", "datetime": "2024-12-05T23:56:15.604Z", "sentiment": null}%n
2025-04-13 11:45:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:45:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:45:05 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-128
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:05 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:05 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-128] Instantiated an idempotent producer.
2025-04-13 11:45:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544705341
2025-04-13 11:45:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:05 [kafka-producer-network-thread | producer-128] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-128] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_13', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:45:05 [kafka-producer-network-thread | producer-128] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-128] ProducerId set to 9308 with epoch 0
2025-04-13 11:45:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:05 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_46, value: {"_id": {"$oid": "67f10b97558ab0f6396b1438"}, "review": "Not bad, not great either.", "authId": "user_46", "datetime": "2025-03-08T03:51:19.059Z", "sentiment": null}, offset: 70897
2025-04-13 11:45:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_46
2025-04-13 11:45:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1438"}, "review": "Not bad, not great either.", "authId": "user_46", "datetime": "2025-03-08T03:51:19.059Z", "sentiment": null}%n
2025-04-13 11:45:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:45:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:45:06 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-129
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:06 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:06 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-129] Instantiated an idempotent producer.
2025-04-13 11:45:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544706849
2025-04-13 11:45:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_46', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:45:06 [kafka-producer-network-thread | producer-129] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-129] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:06 [kafka-producer-network-thread | producer-129] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-129] ProducerId set to 9309 with epoch 0
2025-04-13 11:45:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:06 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_51, value: {"_id": {"$oid": "67f10b97558ab0f6396b143d"}, "review": "Very bad quality, feels cheap.", "authId": "user_51", "datetime": "2025-01-28T22:41:45.413Z", "sentiment": null}, offset: 70898
2025-04-13 11:45:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_51
2025-04-13 11:45:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143d"}, "review": "Very bad quality, feels cheap.", "authId": "user_51", "datetime": "2025-01-28T22:41:45.413Z", "sentiment": null}%n
2025-04-13 11:45:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:08 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-130
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:08 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:08 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-130] Instantiated an idempotent producer.
2025-04-13 11:45:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544708347
2025-04-13 11:45:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_51', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:08 [kafka-producer-network-thread | producer-130] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-130] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:08 [kafka-producer-network-thread | producer-130] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-130] ProducerId set to 9310 with epoch 0
2025-04-13 11:45:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:09 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_33, value: {"_id": {"$oid": "67f10b97558ab0f6396b142b"}, "review": "Terrible experience, very disappointed.", "authId": "user_33", "datetime": "2024-12-24T07:32:50.779Z", "sentiment": null}, offset: 70899
2025-04-13 11:45:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_33
2025-04-13 11:45:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142b"}, "review": "Terrible experience, very disappointed.", "authId": "user_33", "datetime": "2024-12-24T07:32:50.779Z", "sentiment": null}%n
2025-04-13 11:45:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:10 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-131
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:10 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:10 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-131] Instantiated an idempotent producer.
2025-04-13 11:45:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544710921
2025-04-13 11:45:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:10 [kafka-producer-network-thread | producer-131] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-131] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_33', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:10 [kafka-producer-network-thread | producer-131] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-131] ProducerId set to 9311 with epoch 0
2025-04-13 11:45:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:10 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_89, value: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}, offset: 70900
2025-04-13 11:45:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_89
2025-04-13 11:45:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}%n
2025-04-13 11:45:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:45:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:45:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-132
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:12 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-132] Instantiated an idempotent producer.
2025-04-13 11:45:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544712430
2025-04-13 11:45:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_89', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:45:12 [kafka-producer-network-thread | producer-132] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-132] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:12 [kafka-producer-network-thread | producer-132] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-132] ProducerId set to 9312 with epoch 0
2025-04-13 11:45:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:12 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_2, value: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}, offset: 70901
2025-04-13 11:45:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_2
2025-04-13 11:45:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}%n
2025-04-13 11:45:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:45:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:45:13 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-133
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:13 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:13 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-133] Instantiated an idempotent producer.
2025-04-13 11:45:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544713928
2025-04-13 11:45:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_2', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:45:13 [kafka-producer-network-thread | producer-133] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-133] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:13 [kafka-producer-network-thread | producer-133] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-133] ProducerId set to 9313 with epoch 0
2025-04-13 11:45:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:13 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_79, value: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}, offset: 70902
2025-04-13 11:45:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_79
2025-04-13 11:45:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}%n
2025-04-13 11:45:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:14 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-134
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:14 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:14 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-134] Instantiated an idempotent producer.
2025-04-13 11:45:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544714452
2025-04-13 11:45:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:14 [kafka-producer-network-thread | producer-134] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-134] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_79', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:14 [kafka-producer-network-thread | producer-134] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-134] ProducerId set to 9314 with epoch 0
2025-04-13 11:45:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:14 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_30, value: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}, offset: 70903
2025-04-13 11:45:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_30
2025-04-13 11:45:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}%n
2025-04-13 11:45:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:45:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:45:15 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-135
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:15 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:15 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-135] Instantiated an idempotent producer.
2025-04-13 11:45:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544715911
2025-04-13 11:45:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_30', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:45:15 [kafka-producer-network-thread | producer-135] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-135] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:15 [kafka-producer-network-thread | producer-135] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-135] ProducerId set to 9315 with epoch 0
2025-04-13 11:45:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:15 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_89, value: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}, offset: 70904
2025-04-13 11:45:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_89
2025-04-13 11:45:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}%n
2025-04-13 11:45:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:45:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:45:17 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-136
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:17 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:17 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-136] Instantiated an idempotent producer.
2025-04-13 11:45:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544717365
2025-04-13 11:45:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_89', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:45:17 [kafka-producer-network-thread | producer-136] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-136] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:17 [kafka-producer-network-thread | producer-136] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-136] ProducerId set to 9316 with epoch 0
2025-04-13 11:45:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:17 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_97, value: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}, offset: 70905
2025-04-13 11:45:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_97
2025-04-13 11:45:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}%n
2025-04-13 11:45:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-137
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-137] Instantiated an idempotent producer.
2025-04-13 11:45:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544718836
2025-04-13 11:45:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:18 [kafka-producer-network-thread | producer-137] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-137] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_97', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:18 [kafka-producer-network-thread | producer-137] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-137] ProducerId set to 9317 with epoch 0
2025-04-13 11:45:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_65, value: {"_id": {"$oid": "67f10b97558ab0f6396b144b"}, "review": "Horrible customer service.", "authId": "user_65", "datetime": "2025-01-22T00:39:23.653Z", "sentiment": null}, offset: 70906
2025-04-13 11:45:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_65
2025-04-13 11:45:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144b"}, "review": "Horrible customer service.", "authId": "user_65", "datetime": "2025-01-22T00:39:23.653Z", "sentiment": null}%n
2025-04-13 11:45:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:45:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:45:20 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-138
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:20 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:20 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-138] Instantiated an idempotent producer.
2025-04-13 11:45:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544720340
2025-04-13 11:45:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_65', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:45:20 [kafka-producer-network-thread | producer-138] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-138] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:20 [kafka-producer-network-thread | producer-138] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-138] ProducerId set to 9318 with epoch 0
2025-04-13 11:45:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:20 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_34, value: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}, offset: 70907
2025-04-13 11:45:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_34
2025-04-13 11:45:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}%n
2025-04-13 11:45:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:45:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:45:21 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-139
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:21 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:21 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-139] Instantiated an idempotent producer.
2025-04-13 11:45:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544721846
2025-04-13 11:45:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_34', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:45:21 [kafka-producer-network-thread | producer-139] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-139] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:21 [kafka-producer-network-thread | producer-139] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-139] ProducerId set to 9319 with epoch 0
2025-04-13 11:45:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:21 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_72, value: {"_id": {"$oid": "67f10b97558ab0f6396b1452"}, "review": "Mediocre quality, nothing special.", "authId": "user_72", "datetime": "2025-01-26T15:04:47.280Z", "sentiment": null}, offset: 70908
2025-04-13 11:45:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_72
2025-04-13 11:45:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1452"}, "review": "Mediocre quality, nothing special.", "authId": "user_72", "datetime": "2025-01-26T15:04:47.280Z", "sentiment": null}%n
2025-04-13 11:45:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:23 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-140
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:23 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:23 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-140] Instantiated an idempotent producer.
2025-04-13 11:45:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:23 [kafka-producer-network-thread | producer-140] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-140] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:23 [kafka-producer-network-thread | producer-140] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-140] ProducerId set to 9320 with epoch 0
2025-04-13 11:45:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544723292
2025-04-13 11:45:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_72', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:24 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_82, value: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}, offset: 70909
2025-04-13 11:45:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_82
2025-04-13 11:45:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}%n
2025-04-13 11:45:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:24 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-141
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:24 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:24 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-141] Instantiated an idempotent producer.
2025-04-13 11:45:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544724835
2025-04-13 11:45:24 [kafka-producer-network-thread | producer-141] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-141] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:24 [kafka-producer-network-thread | producer-141] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-141] ProducerId set to 9321 with epoch 0
2025-04-13 11:45:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_82', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:24 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_69, value: {"_id": {"$oid": "67f10b97558ab0f6396b144f"}, "review": "Decent, but not amazing.", "authId": "user_69", "datetime": "2024-12-28T22:45:48.069Z", "sentiment": null}, offset: 70910
2025-04-13 11:45:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_69
2025-04-13 11:45:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144f"}, "review": "Decent, but not amazing.", "authId": "user_69", "datetime": "2024-12-28T22:45:48.069Z", "sentiment": null}%n
2025-04-13 11:45:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:45:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:45:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-142
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-142] Instantiated an idempotent producer.
2025-04-13 11:45:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544726287
2025-04-13 11:45:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_69', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:45:26 [kafka-producer-network-thread | producer-142] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-142] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:26 [kafka-producer-network-thread | producer-142] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-142] ProducerId set to 9322 with epoch 0
2025-04-13 11:45:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:26 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_43, value: {"_id": {"$oid": "67f10b97558ab0f6396b1435"}, "review": "Not bad, not great either.", "authId": "user_43", "datetime": "2025-01-07T00:44:08.424Z", "sentiment": null}, offset: 70911
2025-04-13 11:45:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_43
2025-04-13 11:45:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1435"}, "review": "Not bad, not great either.", "authId": "user_43", "datetime": "2025-01-07T00:44:08.424Z", "sentiment": null}%n
2025-04-13 11:45:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:45:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:45:27 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-143
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:27 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:27 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-143] Instantiated an idempotent producer.
2025-04-13 11:45:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544727776
2025-04-13 11:45:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:27 [kafka-producer-network-thread | producer-143] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-143] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_43', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:45:27 [kafka-producer-network-thread | producer-143] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-143] ProducerId set to 9323 with epoch 0
2025-04-13 11:45:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:27 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_89, value: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}, offset: 70912
2025-04-13 11:45:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_89
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}%n
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:45:28 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-144
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:28 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:28 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-144] Instantiated an idempotent producer.
2025-04-13 11:45:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544728307
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_89', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:45:28 [kafka-producer-network-thread | producer-144] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-144] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:28 [kafka-producer-network-thread | producer-144] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-144] ProducerId set to 9324 with epoch 0
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_17, value: {"_id": {"$oid": "67f10b97558ab0f6396b141b"}, "review": "Terrible experience, very disappointed.", "authId": "user_17", "datetime": "2025-03-01T00:30:53.030Z", "sentiment": null}, offset: 70913
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_17
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141b"}, "review": "Terrible experience, very disappointed.", "authId": "user_17", "datetime": "2025-03-01T00:30:53.030Z", "sentiment": null}%n
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:28 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-145
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:28 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:28 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-145] Instantiated an idempotent producer.
2025-04-13 11:45:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544728809
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_17', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:28 [kafka-producer-network-thread | producer-145] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-145] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:28 [kafka-producer-network-thread | producer-145] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-145] ProducerId set to 9325 with epoch 0
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_98, value: {"_id": {"$oid": "67f10b97558ab0f6396b146c"}, "review": "Not bad, not great either.", "authId": "user_98", "datetime": "2025-03-21T23:54:21.817Z", "sentiment": null}, offset: 70914
2025-04-13 11:45:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_98
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146c"}, "review": "Not bad, not great either.", "authId": "user_98", "datetime": "2025-03-21T23:54:21.817Z", "sentiment": null}%n
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:45:30 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-146
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:30 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:30 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-146] Instantiated an idempotent producer.
2025-04-13 11:45:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544730267
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_98', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:45:30 [kafka-producer-network-thread | producer-146] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-146] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:30 [kafka-producer-network-thread | producer-146] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-146] ProducerId set to 9326 with epoch 0
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 70915
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:30 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-147
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:30 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:30 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-147] Instantiated an idempotent producer.
2025-04-13 11:45:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544730763
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:30 [kafka-producer-network-thread | producer-147] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-147] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:30 [kafka-producer-network-thread | producer-147] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-147] ProducerId set to 9327 with epoch 0
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_70, value: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}, offset: 70916
2025-04-13 11:45:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_70
2025-04-13 11:45:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}%n
2025-04-13 11:45:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:45:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:45:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-148
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-148] Instantiated an idempotent producer.
2025-04-13 11:45:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:32 [kafka-producer-network-thread | producer-148] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-148] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:32 [kafka-producer-network-thread | producer-148] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-148] ProducerId set to 9328 with epoch 0
2025-04-13 11:45:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544732296
2025-04-13 11:45:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_70', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:45:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_4, value: {"_id": {"$oid": "67f10b97558ab0f6396b140e"}, "review": "Expected more, but it's fine.", "authId": "user_4", "datetime": "2024-12-28T03:01:39.603Z", "sentiment": null}, offset: 70917
2025-04-13 11:45:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_4
2025-04-13 11:45:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140e"}, "review": "Expected more, but it's fine.", "authId": "user_4", "datetime": "2024-12-28T03:01:39.603Z", "sentiment": null}%n
2025-04-13 11:45:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:45:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:45:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-149
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-149] Instantiated an idempotent producer.
2025-04-13 11:45:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544733789
2025-04-13 11:45:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_4', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:45:33 [kafka-producer-network-thread | producer-149] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-149] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:33 [kafka-producer-network-thread | producer-149] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-149] ProducerId set to 9329 with epoch 0
2025-04-13 11:45:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_52, value: {"_id": {"$oid": "67f10b97558ab0f6396b143e"}, "review": "This changed my life!", "authId": "user_52", "datetime": "2025-01-10T06:57:54.745Z", "sentiment": null}, offset: 70918
2025-04-13 11:45:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_52
2025-04-13 11:45:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143e"}, "review": "This changed my life!", "authId": "user_52", "datetime": "2025-01-10T06:57:54.745Z", "sentiment": null}%n
2025-04-13 11:45:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:45:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:45:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-150
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-150] Instantiated an idempotent producer.
2025-04-13 11:45:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544735252
2025-04-13 11:45:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_52', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:45:35 [kafka-producer-network-thread | producer-150] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-150] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:35 [kafka-producer-network-thread | producer-150] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-150] ProducerId set to 9330 with epoch 0
2025-04-13 11:45:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:36 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_82, value: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}, offset: 70919
2025-04-13 11:45:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_82
2025-04-13 11:45:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145c"}, "review": "Doesn't work as advertised.", "authId": "user_82", "datetime": "2025-03-31T12:18:06.328Z", "sentiment": null}%n
2025-04-13 11:45:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:38 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-151
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:38 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:38 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-151] Instantiated an idempotent producer.
2025-04-13 11:45:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544738767
2025-04-13 11:45:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_82', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:38 [kafka-producer-network-thread | producer-151] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-151] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:38 [kafka-producer-network-thread | producer-151] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-151] ProducerId set to 9331 with epoch 0
2025-04-13 11:45:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_62, value: {"_id": {"$oid": "67f10b97558ab0f6396b1448"}, "review": "Poor build quality, not reliable.", "authId": "user_62", "datetime": "2025-03-04T10:01:05.882Z", "sentiment": null}, offset: 70920
2025-04-13 11:45:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_62
2025-04-13 11:45:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1448"}, "review": "Poor build quality, not reliable.", "authId": "user_62", "datetime": "2025-03-04T10:01:05.882Z", "sentiment": null}%n
2025-04-13 11:45:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-152
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-152] Instantiated an idempotent producer.
2025-04-13 11:45:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544739285
2025-04-13 11:45:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_62', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:39 [kafka-producer-network-thread | producer-152] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-152] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:39 [kafka-producer-network-thread | producer-152] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-152] ProducerId set to 9332 with epoch 0
2025-04-13 11:45:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:39 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_20, value: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}, offset: 70921
2025-04-13 11:45:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_20
2025-04-13 11:45:40 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}%n
2025-04-13 11:45:40 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:45:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:45:40 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-153
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:40 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:40 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-153] Instantiated an idempotent producer.
2025-04-13 11:45:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544740735
2025-04-13 11:45:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_20', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:45:40 [kafka-producer-network-thread | producer-153] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-153] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:40 [kafka-producer-network-thread | producer-153] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-153] ProducerId set to 9333 with epoch 0
2025-04-13 11:45:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:40 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_6, value: {"_id": {"$oid": "67f10b97558ab0f6396b1410"}, "review": "Im very happy with this purchase!", "authId": "user_6", "datetime": "2025-01-21T18:03:15.045Z", "sentiment": null}, offset: 70922
2025-04-13 11:45:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_6
2025-04-13 11:45:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1410"}, "review": "Im very happy with this purchase!", "authId": "user_6", "datetime": "2025-01-21T18:03:15.045Z", "sentiment": null}%n
2025-04-13 11:45:42 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:45:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:45:42 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-154
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:42 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:42 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-154] Instantiated an idempotent producer.
2025-04-13 11:45:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:42 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544742207
2025-04-13 11:45:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_6', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:45:42 [kafka-producer-network-thread | producer-154] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-154] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:42 [kafka-producer-network-thread | producer-154] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-154] ProducerId set to 9334 with epoch 0
2025-04-13 11:45:42 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:42 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_99, value: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}, offset: 70923
2025-04-13 11:45:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_99
2025-04-13 11:45:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}%n
2025-04-13 11:45:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:45:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:45:43 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-155
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:43 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:43 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-155] Instantiated an idempotent producer.
2025-04-13 11:45:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544743722
2025-04-13 11:45:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_99', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:45:43 [kafka-producer-network-thread | producer-155] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-155] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:43 [kafka-producer-network-thread | producer-155] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-155] ProducerId set to 9335 with epoch 0
2025-04-13 11:45:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:43 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_58, value: {"_id": {"$oid": "67f10b97558ab0f6396b1444"}, "review": "Decent, but not amazing.", "authId": "user_58", "datetime": "2025-01-01T07:11:14.890Z", "sentiment": null}, offset: 70924
2025-04-13 11:45:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_58
2025-04-13 11:45:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1444"}, "review": "Decent, but not amazing.", "authId": "user_58", "datetime": "2025-01-01T07:11:14.890Z", "sentiment": null}%n
2025-04-13 11:45:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:45:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:45:44 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-156
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:44 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:44 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-156] Instantiated an idempotent producer.
2025-04-13 11:45:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544744241
2025-04-13 11:45:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_58', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:45:44 [kafka-producer-network-thread | producer-156] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-156] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:44 [kafka-producer-network-thread | producer-156] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-156] ProducerId set to 9336 with epoch 0
2025-04-13 11:45:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_33, value: {"_id": {"$oid": "67f10b97558ab0f6396b142b"}, "review": "Terrible experience, very disappointed.", "authId": "user_33", "datetime": "2024-12-24T07:32:50.779Z", "sentiment": null}, offset: 70925
2025-04-13 11:45:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_33
2025-04-13 11:45:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142b"}, "review": "Terrible experience, very disappointed.", "authId": "user_33", "datetime": "2024-12-24T07:32:50.779Z", "sentiment": null}%n
2025-04-13 11:45:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-157
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-157] Instantiated an idempotent producer.
2025-04-13 11:45:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544745739
2025-04-13 11:45:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_33', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:45 [kafka-producer-network-thread | producer-157] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-157] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:45 [kafka-producer-network-thread | producer-157] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-157] ProducerId set to 9337 with epoch 0
2025-04-13 11:45:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_42, value: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}, offset: 70926
2025-04-13 11:45:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_42
2025-04-13 11:45:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}%n
2025-04-13 11:45:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:47 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-158
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:47 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:47 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-158] Instantiated an idempotent producer.
2025-04-13 11:45:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544747239
2025-04-13 11:45:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:47 [kafka-producer-network-thread | producer-158] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-158] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_42', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:47 [kafka-producer-network-thread | producer-158] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-158] ProducerId set to 9338 with epoch 0
2025-04-13 11:45:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:47 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_8, value: {"_id": {"$oid": "67f10b97558ab0f6396b1412"}, "review": "Serves its purpose, but nothing more.", "authId": "user_8", "datetime": "2024-12-17T21:27:23.328Z", "sentiment": null}, offset: 70927
2025-04-13 11:45:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_8
2025-04-13 11:45:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1412"}, "review": "Serves its purpose, but nothing more.", "authId": "user_8", "datetime": "2024-12-17T21:27:23.328Z", "sentiment": null}%n
2025-04-13 11:45:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:45:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:45:48 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-159
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:48 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:48 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-159] Instantiated an idempotent producer.
2025-04-13 11:45:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544748755
2025-04-13 11:45:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_8', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:45:48 [kafka-producer-network-thread | producer-159] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-159] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:48 [kafka-producer-network-thread | producer-159] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-159] ProducerId set to 9339 with epoch 0
2025-04-13 11:45:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:48 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_48, value: {"_id": {"$oid": "67f10b97558ab0f6396b143a"}, "review": "Terrible experience, very disappointed.", "authId": "user_48", "datetime": "2025-01-15T02:57:46.191Z", "sentiment": null}, offset: 70928
2025-04-13 11:45:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_48
2025-04-13 11:45:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143a"}, "review": "Terrible experience, very disappointed.", "authId": "user_48", "datetime": "2025-01-15T02:57:46.191Z", "sentiment": null}%n
2025-04-13 11:45:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:50 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-160
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:50 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:50 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-160] Instantiated an idempotent producer.
2025-04-13 11:45:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544750238
2025-04-13 11:45:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_48', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:50 [kafka-producer-network-thread | producer-160] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-160] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:50 [kafka-producer-network-thread | producer-160] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-160] ProducerId set to 9340 with epoch 0
2025-04-13 11:45:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:51 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_73, value: {"_id": {"$oid": "67f10b97558ab0f6396b1453"}, "review": "Not satisfied at all.", "authId": "user_73", "datetime": "2025-03-31T12:24:38.653Z", "sentiment": null}, offset: 70929
2025-04-13 11:45:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_73
2025-04-13 11:45:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1453"}, "review": "Not satisfied at all.", "authId": "user_73", "datetime": "2025-03-31T12:24:38.653Z", "sentiment": null}%n
2025-04-13 11:45:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:52 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-161
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:52 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:52 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-161] Instantiated an idempotent producer.
2025-04-13 11:45:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544752803
2025-04-13 11:45:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:52 [kafka-producer-network-thread | producer-161] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-161] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_73', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:52 [kafka-producer-network-thread | producer-161] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-161] ProducerId set to 9341 with epoch 0
2025-04-13 11:45:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:52 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_42, value: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}, offset: 70930
2025-04-13 11:45:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_42
2025-04-13 11:45:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1434"}, "review": "Poor build quality, not reliable.", "authId": "user_42", "datetime": "2025-01-19T04:11:26.866Z", "sentiment": null}%n
2025-04-13 11:45:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:54 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-162
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:54 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:54 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-162] Instantiated an idempotent producer.
2025-04-13 11:45:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544754281
2025-04-13 11:45:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_42', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:54 [kafka-producer-network-thread | producer-162] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-162] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:54 [kafka-producer-network-thread | producer-162] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-162] ProducerId set to 9342 with epoch 0
2025-04-13 11:45:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:54 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_71, value: {"_id": {"$oid": "67f10b97558ab0f6396b1451"}, "review": "Excellent! Totally worth the price.", "authId": "user_71", "datetime": "2025-03-01T23:14:24.186Z", "sentiment": null}, offset: 70931
2025-04-13 11:45:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_71
2025-04-13 11:45:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1451"}, "review": "Excellent! Totally worth the price.", "authId": "user_71", "datetime": "2025-03-01T23:14:24.186Z", "sentiment": null}%n
2025-04-13 11:45:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:45:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:45:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-163
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-163] Instantiated an idempotent producer.
2025-04-13 11:45:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544755747
2025-04-13 11:45:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_71', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:45:55 [kafka-producer-network-thread | producer-163] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-163] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:55 [kafka-producer-network-thread | producer-163] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-163] ProducerId set to 9343 with epoch 0
2025-04-13 11:45:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_49, value: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "Its alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}, offset: 70932
2025-04-13 11:45:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_49
2025-04-13 11:45:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "Its alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}%n
2025-04-13 11:45:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-13 11:45:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-13 11:45:57 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-164
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:57 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:57 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-164] Instantiated an idempotent producer.
2025-04-13 11:45:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544757209
2025-04-13 11:45:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_49', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-13 11:45:57 [kafka-producer-network-thread | producer-164] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-164] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:57 [kafka-producer-network-thread | producer-164] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-164] ProducerId set to 9344 with epoch 0
2025-04-13 11:45:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:57 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_35, value: {"_id": {"$oid": "67f10b97558ab0f6396b142d"}, "review": "Terrible experience, very disappointed.", "authId": "user_35", "datetime": "2025-03-06T15:09:01.869Z", "sentiment": null}, offset: 70933
2025-04-13 11:45:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_35
2025-04-13 11:45:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142d"}, "review": "Terrible experience, very disappointed.", "authId": "user_35", "datetime": "2025-03-06T15:09:01.869Z", "sentiment": null}%n
2025-04-13 11:45:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:45:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:45:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-165
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:45:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:45:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-165] Instantiated an idempotent producer.
2025-04-13 11:45:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:45:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:45:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544758703
2025-04-13 11:45:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:45:58 [kafka-producer-network-thread | producer-165] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-165] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:45:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_35', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:45:58 [kafka-producer-network-thread | producer-165] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-165] ProducerId set to 9345 with epoch 0
2025-04-13 11:45:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:45:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_2, value: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}, offset: 70934
2025-04-13 11:45:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_2
2025-04-13 11:46:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}%n
2025-04-13 11:46:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:46:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:46:00 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-166
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:00 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:00 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-166] Instantiated an idempotent producer.
2025-04-13 11:46:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544760198
2025-04-13 11:46:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:00 [kafka-producer-network-thread | producer-166] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-166] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_2', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:46:00 [kafka-producer-network-thread | producer-166] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-166] ProducerId set to 9346 with epoch 0
2025-04-13 11:46:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:00 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_24, value: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}, offset: 70935
2025-04-13 11:46:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_24
2025-04-13 11:46:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}%n
2025-04-13 11:46:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:46:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:46:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-167
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-167] Instantiated an idempotent producer.
2025-04-13 11:46:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544761721
2025-04-13 11:46:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_24', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:46:01 [kafka-producer-network-thread | producer-167] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-167] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:01 [kafka-producer-network-thread | producer-167] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-167] ProducerId set to 9347 with epoch 0
2025-04-13 11:46:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:01 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_6, value: {"_id": {"$oid": "67f10b97558ab0f6396b1410"}, "review": "Im very happy with this purchase!", "authId": "user_6", "datetime": "2025-01-21T18:03:15.045Z", "sentiment": null}, offset: 70936
2025-04-13 11:46:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_6
2025-04-13 11:46:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1410"}, "review": "Im very happy with this purchase!", "authId": "user_6", "datetime": "2025-01-21T18:03:15.045Z", "sentiment": null}%n
2025-04-13 11:46:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:46:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:46:03 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-168
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:03 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:03 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-168] Instantiated an idempotent producer.
2025-04-13 11:46:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544763182
2025-04-13 11:46:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_6', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:46:03 [kafka-producer-network-thread | producer-168] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-168] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:03 [kafka-producer-network-thread | producer-168] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-168] ProducerId set to 9348 with epoch 0
2025-04-13 11:46:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:03 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_14, value: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}, offset: 70937
2025-04-13 11:46:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_14
2025-04-13 11:46:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}%n
2025-04-13 11:46:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:46:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:46:04 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-169
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:04 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:04 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-169] Instantiated an idempotent producer.
2025-04-13 11:46:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544764646
2025-04-13 11:46:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_14', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:46:04 [kafka-producer-network-thread | producer-169] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-169] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:04 [kafka-producer-network-thread | producer-169] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-169] ProducerId set to 9349 with epoch 0
2025-04-13 11:46:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:04 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_96, value: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}, offset: 70938
2025-04-13 11:46:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_96
2025-04-13 11:46:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}%n
2025-04-13 11:46:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:46:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:46:06 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-170
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:06 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:06 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-170] Instantiated an idempotent producer.
2025-04-13 11:46:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544766135
2025-04-13 11:46:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_96', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:46:06 [kafka-producer-network-thread | producer-170] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-170] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:06 [kafka-producer-network-thread | producer-170] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-170] ProducerId set to 9350 with epoch 0
2025-04-13 11:46:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:07 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_27, value: {"_id": {"$oid": "67f10b97558ab0f6396b1425"}, "review": "This changed my life!", "authId": "user_27", "datetime": "2024-12-10T09:12:46.567Z", "sentiment": null}, offset: 70939
2025-04-13 11:46:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_27
2025-04-13 11:46:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1425"}, "review": "This changed my life!", "authId": "user_27", "datetime": "2024-12-10T09:12:46.567Z", "sentiment": null}%n
2025-04-13 11:46:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:46:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:46:08 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-171
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:08 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:08 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-171] Instantiated an idempotent producer.
2025-04-13 11:46:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544768625
2025-04-13 11:46:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_27', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:46:08 [kafka-producer-network-thread | producer-171] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-171] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:08 [kafka-producer-network-thread | producer-171] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-171] ProducerId set to 9351 with epoch 0
2025-04-13 11:46:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:08 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_90, value: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}, offset: 70940
2025-04-13 11:46:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_90
2025-04-13 11:46:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}%n
2025-04-13 11:46:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: -0.1

2025-04-13 11:46:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: -0.1

2025-04-13 11:46:11 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-172
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:11 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:11 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-172] Instantiated an idempotent producer.
2025-04-13 11:46:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544771138
2025-04-13 11:46:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_90', sentiment=magnitude: 0.1
score: -0.1
}
2025-04-13 11:46:11 [kafka-producer-network-thread | producer-172] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-172] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:11 [kafka-producer-network-thread | producer-172] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-172] ProducerId set to 9352 with epoch 0
2025-04-13 11:46:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:11 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_35, value: {"_id": {"$oid": "67f10b97558ab0f6396b142d"}, "review": "Terrible experience, very disappointed.", "authId": "user_35", "datetime": "2025-03-06T15:09:01.869Z", "sentiment": null}, offset: 70941
2025-04-13 11:46:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_35
2025-04-13 11:46:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142d"}, "review": "Terrible experience, very disappointed.", "authId": "user_35", "datetime": "2025-03-06T15:09:01.869Z", "sentiment": null}%n
2025-04-13 11:46:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-173
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:12 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-173] Instantiated an idempotent producer.
2025-04-13 11:46:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544772623
2025-04-13 11:46:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_35', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:12 [kafka-producer-network-thread | producer-173] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-173] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:12 [kafka-producer-network-thread | producer-173] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-173] ProducerId set to 9353 with epoch 0
2025-04-13 11:46:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:12 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_79, value: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}, offset: 70942
2025-04-13 11:46:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_79
2025-04-13 11:46:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}%n
2025-04-13 11:46:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:14 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-174
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:14 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:14 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-174] Instantiated an idempotent producer.
2025-04-13 11:46:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544774098
2025-04-13 11:46:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:14 [kafka-producer-network-thread | producer-174] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-174] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_79', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:14 [kafka-producer-network-thread | producer-174] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-174] ProducerId set to 9354 with epoch 0
2025-04-13 11:46:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:14 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 70943
2025-04-13 11:46:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-13 11:46:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-13 11:46:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:46:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:46:15 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-175
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:15 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:15 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-175] Instantiated an idempotent producer.
2025-04-13 11:46:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544775558
2025-04-13 11:46:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:46:15 [kafka-producer-network-thread | producer-175] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-175] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:15 [kafka-producer-network-thread | producer-175] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-175] ProducerId set to 9355 with epoch 0
2025-04-13 11:46:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:15 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_85, value: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}, offset: 70944
2025-04-13 11:46:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_85
2025-04-13 11:46:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}%n
2025-04-13 11:46:17 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:46:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:46:17 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-176
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:17 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:17 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-176] Instantiated an idempotent producer.
2025-04-13 11:46:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:17 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544777057
2025-04-13 11:46:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_85', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:46:17 [kafka-producer-network-thread | producer-176] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-176] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:17 [kafka-producer-network-thread | producer-176] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-176] ProducerId set to 9356 with epoch 0
2025-04-13 11:46:17 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:17 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_1, value: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}, offset: 70945
2025-04-13 11:46:17 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_1
2025-04-13 11:46:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}%n
2025-04-13 11:46:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-177
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-177] Instantiated an idempotent producer.
2025-04-13 11:46:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544778507
2025-04-13 11:46:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:18 [kafka-producer-network-thread | producer-177] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-177] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_1', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:18 [kafka-producer-network-thread | producer-177] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-177] ProducerId set to 9357 with epoch 0
2025-04-13 11:46:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_63, value: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}, offset: 70946
2025-04-13 11:46:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_63
2025-04-13 11:46:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}%n
2025-04-13 11:46:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 11:46:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 11:46:19 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-178
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:19 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:19 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-178] Instantiated an idempotent producer.
2025-04-13 11:46:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544779049
2025-04-13 11:46:19 [kafka-producer-network-thread | producer-178] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-178] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:19 [kafka-producer-network-thread | producer-178] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-178] ProducerId set to 9358 with epoch 0
2025-04-13 11:46:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_63', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 11:46:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:19 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_30, value: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}, offset: 70947
2025-04-13 11:46:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_30
2025-04-13 11:46:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}%n
2025-04-13 11:46:20 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:46:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:46:20 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-179
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:20 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:20 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-179] Instantiated an idempotent producer.
2025-04-13 11:46:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:20 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544780544
2025-04-13 11:46:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:20 [kafka-producer-network-thread | producer-179] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-179] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_30', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:46:20 [kafka-producer-network-thread | producer-179] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-179] ProducerId set to 9359 with epoch 0
2025-04-13 11:46:20 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:20 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_79, value: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}, offset: 70948
2025-04-13 11:46:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_79
2025-04-13 11:46:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}%n
2025-04-13 11:46:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:21 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-180
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:21 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:21 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-180] Instantiated an idempotent producer.
2025-04-13 11:46:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544781096
2025-04-13 11:46:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_79', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:21 [kafka-producer-network-thread | producer-180] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-180] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:21 [kafka-producer-network-thread | producer-180] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-180] ProducerId set to 9360 with epoch 0
2025-04-13 11:46:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:22 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_13, value: {"_id": {"$oid": "67f10b97558ab0f6396b1417"}, "review": "It broke after a week of use.", "authId": "user_13", "datetime": "2024-12-05T23:56:15.604Z", "sentiment": null}, offset: 70949
2025-04-13 11:46:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_13
2025-04-13 11:46:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1417"}, "review": "It broke after a week of use.", "authId": "user_13", "datetime": "2024-12-05T23:56:15.604Z", "sentiment": null}%n
2025-04-13 11:46:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:46:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:46:23 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-181
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:23 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:23 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-181] Instantiated an idempotent producer.
2025-04-13 11:46:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544783584
2025-04-13 11:46:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:23 [kafka-producer-network-thread | producer-181] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-181] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_13', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:46:23 [kafka-producer-network-thread | producer-181] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-181] ProducerId set to 9361 with epoch 0
2025-04-13 11:46:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:23 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_94, value: {"_id": {"$oid": "67f10b97558ab0f6396b1468"}, "review": "Great quality, will buy again.", "authId": "user_94", "datetime": "2025-03-23T03:36:43.648Z", "sentiment": null}, offset: 70950
2025-04-13 11:46:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_94
2025-04-13 11:46:25 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1468"}, "review": "Great quality, will buy again.", "authId": "user_94", "datetime": "2025-03-23T03:36:43.648Z", "sentiment": null}%n
2025-04-13 11:46:25 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: 0.1

2025-04-13 11:46:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: 0.1

2025-04-13 11:46:25 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-182
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:25 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:25 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-182] Instantiated an idempotent producer.
2025-04-13 11:46:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544785051
2025-04-13 11:46:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_94', sentiment=magnitude: 0.1
score: 0.1
}
2025-04-13 11:46:25 [kafka-producer-network-thread | producer-182] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-182] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:25 [kafka-producer-network-thread | producer-182] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-182] ProducerId set to 9362 with epoch 0
2025-04-13 11:46:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:25 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_45, value: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}, offset: 70951
2025-04-13 11:46:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_45
2025-04-13 11:46:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}%n
2025-04-13 11:46:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-183
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-183] Instantiated an idempotent producer.
2025-04-13 11:46:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544786529
2025-04-13 11:46:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_45', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:26 [kafka-producer-network-thread | producer-183] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-183] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:26 [kafka-producer-network-thread | producer-183] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-183] ProducerId set to 9363 with epoch 0
2025-04-13 11:46:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:26 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_45, value: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}, offset: 70952
2025-04-13 11:46:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_45
2025-04-13 11:46:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}%n
2025-04-13 11:46:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:27 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-184
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:27 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:27 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-184] Instantiated an idempotent producer.
2025-04-13 11:46:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544787998
2025-04-13 11:46:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_45', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:28 [kafka-producer-network-thread | producer-184] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-184] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:28 [kafka-producer-network-thread | producer-184] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-184] ProducerId set to 9364 with epoch 0
2025-04-13 11:46:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:28 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_51, value: {"_id": {"$oid": "67f10b97558ab0f6396b143d"}, "review": "Very bad quality, feels cheap.", "authId": "user_51", "datetime": "2025-01-28T22:41:45.413Z", "sentiment": null}, offset: 70953
2025-04-13 11:46:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_51
2025-04-13 11:46:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143d"}, "review": "Very bad quality, feels cheap.", "authId": "user_51", "datetime": "2025-01-28T22:41:45.413Z", "sentiment": null}%n
2025-04-13 11:46:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:29 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-185
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:29 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:29 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-185] Instantiated an idempotent producer.
2025-04-13 11:46:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544789479
2025-04-13 11:46:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_51', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:29 [kafka-producer-network-thread | producer-185] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-185] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:29 [kafka-producer-network-thread | producer-185] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-185] ProducerId set to 9365 with epoch 0
2025-04-13 11:46:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:29 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_85, value: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}, offset: 70954
2025-04-13 11:46:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_85
2025-04-13 11:46:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}%n
2025-04-13 11:46:30 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:46:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:46:30 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-186
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:30 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:30 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-186] Instantiated an idempotent producer.
2025-04-13 11:46:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:30 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544790937
2025-04-13 11:46:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_85', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:46:30 [kafka-producer-network-thread | producer-186] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-186] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:30 [kafka-producer-network-thread | producer-186] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-186] ProducerId set to 9366 with epoch 0
2025-04-13 11:46:30 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:30 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_10, value: {"_id": {"$oid": "67f10b97558ab0f6396b1414"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_10", "datetime": "2025-03-16T01:58:57.821Z", "sentiment": null}, offset: 70955
2025-04-13 11:46:30 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_10
2025-04-13 11:46:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1414"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_10", "datetime": "2025-03-16T01:58:57.821Z", "sentiment": null}%n
2025-04-13 11:46:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.4
score: 0.7

2025-04-13 11:46:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.4
score: 0.7

2025-04-13 11:46:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-187
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-187] Instantiated an idempotent producer.
2025-04-13 11:46:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544792402
2025-04-13 11:46:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_10', sentiment=magnitude: 1.4
score: 0.7
}
2025-04-13 11:46:32 [kafka-producer-network-thread | producer-187] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-187] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:32 [kafka-producer-network-thread | producer-187] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-187] ProducerId set to 9367 with epoch 0
2025-04-13 11:46:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_41, value: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}, offset: 70956
2025-04-13 11:46:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_41
2025-04-13 11:46:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}%n
2025-04-13 11:46:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-188
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-188] Instantiated an idempotent producer.
2025-04-13 11:46:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544793844
2025-04-13 11:46:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:33 [kafka-producer-network-thread | producer-188] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-188] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_41', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:33 [kafka-producer-network-thread | producer-188] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-188] ProducerId set to 9368 with epoch 0
2025-04-13 11:46:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_90, value: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}, offset: 70957
2025-04-13 11:46:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_90
2025-04-13 11:46:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}%n
2025-04-13 11:46:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: -0.1

2025-04-13 11:46:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: -0.1

2025-04-13 11:46:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-189
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-189] Instantiated an idempotent producer.
2025-04-13 11:46:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544795307
2025-04-13 11:46:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_90', sentiment=magnitude: 0.1
score: -0.1
}
2025-04-13 11:46:35 [kafka-producer-network-thread | producer-189] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-189] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:35 [kafka-producer-network-thread | producer-189] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-189] ProducerId set to 9369 with epoch 0
2025-04-13 11:46:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_4, value: {"_id": {"$oid": "67f10b97558ab0f6396b140e"}, "review": "Expected more, but it's fine.", "authId": "user_4", "datetime": "2024-12-28T03:01:39.603Z", "sentiment": null}, offset: 70958
2025-04-13 11:46:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_4
2025-04-13 11:46:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140e"}, "review": "Expected more, but it's fine.", "authId": "user_4", "datetime": "2024-12-28T03:01:39.603Z", "sentiment": null}%n
2025-04-13 11:46:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:46:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:46:36 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-190
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:36 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:36 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-190] Instantiated an idempotent producer.
2025-04-13 11:46:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544796786
2025-04-13 11:46:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_4', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:46:36 [kafka-producer-network-thread | producer-190] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-190] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:36 [kafka-producer-network-thread | producer-190] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-190] ProducerId set to 9370 with epoch 0
2025-04-13 11:46:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:37 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 70959
2025-04-13 11:46:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-13 11:46:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}%n
2025-04-13 11:46:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:46:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:46:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-191
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-191] Instantiated an idempotent producer.
2025-04-13 11:46:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544799224
2025-04-13 11:46:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:39 [kafka-producer-network-thread | producer-191] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-191] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_91', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:46:39 [kafka-producer-network-thread | producer-191] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-191] ProducerId set to 9371 with epoch 0
2025-04-13 11:46:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:39 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_76, value: {"_id": {"$oid": "67f10b97558ab0f6396b1456"}, "review": "Great quality, will buy again.", "authId": "user_76", "datetime": "2025-01-27T21:25:36.715Z", "sentiment": null}, offset: 70960
2025-04-13 11:46:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_76
2025-04-13 11:46:40 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1456"}, "review": "Great quality, will buy again.", "authId": "user_76", "datetime": "2025-01-27T21:25:36.715Z", "sentiment": null}%n
2025-04-13 11:46:40 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: 0.1

2025-04-13 11:46:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: 0.1

2025-04-13 11:46:40 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-192
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:40 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:40 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-192] Instantiated an idempotent producer.
2025-04-13 11:46:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544800719
2025-04-13 11:46:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:40 [kafka-producer-network-thread | producer-192] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-192] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_76', sentiment=magnitude: 0.1
score: 0.1
}
2025-04-13 11:46:40 [kafka-producer-network-thread | producer-192] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-192] ProducerId set to 9372 with epoch 0
2025-04-13 11:46:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:40 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_26, value: {"_id": {"$oid": "67f10b97558ab0f6396b1424"}, "review": "Doesn't work as advertised.", "authId": "user_26", "datetime": "2025-02-15T00:52:47.341Z", "sentiment": null}, offset: 70961
2025-04-13 11:46:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_26
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1424"}, "review": "Doesn't work as advertised.", "authId": "user_26", "datetime": "2025-02-15T00:52:47.341Z", "sentiment": null}%n
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:41 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-193
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:41 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:41 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-193] Instantiated an idempotent producer.
2025-04-13 11:46:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544801232
2025-04-13 11:46:41 [kafka-producer-network-thread | producer-193] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-193] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:41 [kafka-producer-network-thread | producer-193] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-193] ProducerId set to 9373 with epoch 0
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_26', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_24, value: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}, offset: 70962
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_24
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}%n
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:46:41 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-194
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:41 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:41 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-194] Instantiated an idempotent producer.
2025-04-13 11:46:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544801773
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_24', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:46:41 [kafka-producer-network-thread | producer-194] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-194] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:41 [kafka-producer-network-thread | producer-194] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-194] ProducerId set to 9374 with epoch 0
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_89, value: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}, offset: 70963
2025-04-13 11:46:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_89
2025-04-13 11:46:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}%n
2025-04-13 11:46:43 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:46:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:46:43 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-195
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:43 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:43 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-195] Instantiated an idempotent producer.
2025-04-13 11:46:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:43 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544803271
2025-04-13 11:46:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_89', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:46:43 [kafka-producer-network-thread | producer-195] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-195] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:43 [kafka-producer-network-thread | producer-195] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-195] ProducerId set to 9375 with epoch 0
2025-04-13 11:46:43 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:43 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_79, value: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}, offset: 70964
2025-04-13 11:46:43 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_79
2025-04-13 11:46:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}%n
2025-04-13 11:46:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:44 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-196
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:44 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:44 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-196] Instantiated an idempotent producer.
2025-04-13 11:46:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544804830
2025-04-13 11:46:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_79', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:44 [kafka-producer-network-thread | producer-196] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-196] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:44 [kafka-producer-network-thread | producer-196] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-196] ProducerId set to 9376 with epoch 0
2025-04-13 11:46:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_22, value: {"_id": {"$oid": "67f10b97558ab0f6396b1420"}, "review": "Poor build quality, not reliable.", "authId": "user_22", "datetime": "2024-12-05T07:32:28.430Z", "sentiment": null}, offset: 70965
2025-04-13 11:46:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_22
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1420"}, "review": "Poor build quality, not reliable.", "authId": "user_22", "datetime": "2024-12-05T07:32:28.430Z", "sentiment": null}%n
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:46 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-197
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:46 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:46 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-197] Instantiated an idempotent producer.
2025-04-13 11:46:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544806309
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_22', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:46 [kafka-producer-network-thread | producer-197] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-197] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:46 [kafka-producer-network-thread | producer-197] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-197] ProducerId set to 9377 with epoch 0
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_89, value: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}, offset: 70966
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_89
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}%n
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:46:46 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-198
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:46 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:46 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-198] Instantiated an idempotent producer.
2025-04-13 11:46:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544806822
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_89', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:46:46 [kafka-producer-network-thread | producer-198] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-198] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:46 [kafka-producer-network-thread | producer-198] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-198] ProducerId set to 9378 with epoch 0
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_34, value: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}, offset: 70967
2025-04-13 11:46:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_34
2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142c"}, "review": "Waste of money, avoid at all costs.", "authId": "user_34", "datetime": "2025-01-22T04:27:11.909Z", "sentiment": null}%n
2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:46:47 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-199
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:47 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:47 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-199] Instantiated an idempotent producer.
2025-04-13 11:46:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544807350
2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_34', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:46:47 [kafka-producer-network-thread | producer-199] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-199] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:47 [kafka-producer-network-thread | producer-199] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-199] ProducerId set to 9379 with epoch 0
2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_78, value: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}, offset: 70968
2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_78
2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}%n
2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:46:47 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-200
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:47 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:47 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-200] Instantiated an idempotent producer.
2025-04-13 11:46:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:47 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544807881
2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_78', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:46:47 [kafka-producer-network-thread | producer-200] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-200] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:47 [kafka-producer-network-thread | producer-200] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-200] ProducerId set to 9380 with epoch 0
2025-04-13 11:46:47 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:48 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_23, value: {"_id": {"$oid": "67f10b97558ab0f6396b1421"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_23", "datetime": "2025-01-08T03:24:13.109Z", "sentiment": null}, offset: 70969
2025-04-13 11:46:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_23
2025-04-13 11:46:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1421"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_23", "datetime": "2025-01-08T03:24:13.109Z", "sentiment": null}%n
2025-04-13 11:46:50 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:46:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:46:50 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-201
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:50 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:50 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-201] Instantiated an idempotent producer.
2025-04-13 11:46:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:50 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544810340
2025-04-13 11:46:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_23', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:46:50 [kafka-producer-network-thread | producer-201] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-201] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:50 [kafka-producer-network-thread | producer-201] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-201] ProducerId set to 9381 with epoch 0
2025-04-13 11:46:50 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:50 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_3, value: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}, offset: 70970
2025-04-13 11:46:50 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_3
2025-04-13 11:46:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}%n
2025-04-13 11:46:51 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 11:46:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 11:46:51 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-202
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:51 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:51 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-202] Instantiated an idempotent producer.
2025-04-13 11:46:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:51 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544811914
2025-04-13 11:46:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_3', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 11:46:51 [kafka-producer-network-thread | producer-202] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-202] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:51 [kafka-producer-network-thread | producer-202] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-202] ProducerId set to 9382 with epoch 0
2025-04-13 11:46:51 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:51 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_73, value: {"_id": {"$oid": "67f10b97558ab0f6396b1453"}, "review": "Not satisfied at all.", "authId": "user_73", "datetime": "2025-03-31T12:24:38.653Z", "sentiment": null}, offset: 70971
2025-04-13 11:46:51 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_73
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1453"}, "review": "Not satisfied at all.", "authId": "user_73", "datetime": "2025-03-31T12:24:38.653Z", "sentiment": null}%n
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:52 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-203
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:52 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:52 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-203] Instantiated an idempotent producer.
2025-04-13 11:46:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544812448
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_73', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:52 [kafka-producer-network-thread | producer-203] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-203] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:52 [kafka-producer-network-thread | producer-203] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-203] ProducerId set to 9383 with epoch 0
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_13, value: {"_id": {"$oid": "67f10b97558ab0f6396b1417"}, "review": "It broke after a week of use.", "authId": "user_13", "datetime": "2024-12-05T23:56:15.604Z", "sentiment": null}, offset: 70972
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_13
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1417"}, "review": "It broke after a week of use.", "authId": "user_13", "datetime": "2024-12-05T23:56:15.604Z", "sentiment": null}%n
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:46:52 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-204
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:52 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:52 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-204] Instantiated an idempotent producer.
2025-04-13 11:46:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544812940
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_13', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:46:52 [kafka-producer-network-thread | producer-204] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-204] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:52 [kafka-producer-network-thread | producer-204] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-204] ProducerId set to 9384 with epoch 0
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_30, value: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}, offset: 70973
2025-04-13 11:46:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_30
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}%n
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:46:54 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-205
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:54 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:54 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-205] Instantiated an idempotent producer.
2025-04-13 11:46:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544814406
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_30', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:46:54 [kafka-producer-network-thread | producer-205] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-205] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:54 [kafka-producer-network-thread | producer-205] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-205] ProducerId set to 9385 with epoch 0
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_2, value: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}, offset: 70974
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_2
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140c"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_2", "datetime": "2025-02-16T12:22:32.981Z", "sentiment": null}%n
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:46:54 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-206
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:54 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:54 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-206] Instantiated an idempotent producer.
2025-04-13 11:46:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544814956
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_2', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:46:54 [kafka-producer-network-thread | producer-206] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-206] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:54 [kafka-producer-network-thread | producer-206] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-206] ProducerId set to 9386 with epoch 0
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_100, value: {"_id": {"$oid": "67f10b97558ab0f6396b146e"}, "review": "Terrible experience, very disappointed.", "authId": "user_100", "datetime": "2025-02-03T10:22:13.147Z", "sentiment": null}, offset: 70975
2025-04-13 11:46:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_100
2025-04-13 11:46:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146e"}, "review": "Terrible experience, very disappointed.", "authId": "user_100", "datetime": "2025-02-03T10:22:13.147Z", "sentiment": null}%n
2025-04-13 11:46:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:46:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:46:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-207
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-207] Instantiated an idempotent producer.
2025-04-13 11:46:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544815462
2025-04-13 11:46:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_100', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:46:55 [kafka-producer-network-thread | producer-207] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-207] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:55 [kafka-producer-network-thread | producer-207] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-207] ProducerId set to 9387 with epoch 0
2025-04-13 11:46:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_55, value: {"_id": {"$oid": "67f10b97558ab0f6396b1441"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_55", "datetime": "2025-02-18T10:02:17.105Z", "sentiment": null}, offset: 70976
2025-04-13 11:46:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_55
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1441"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_55", "datetime": "2025-02-18T10:02:17.105Z", "sentiment": null}%n
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.3
score: 0.6

2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.3
score: 0.6

2025-04-13 11:46:56 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-208
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:56 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:56 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-208] Instantiated an idempotent producer.
2025-04-13 11:46:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544816031
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_55', sentiment=magnitude: 1.3
score: 0.6
}
2025-04-13 11:46:56 [kafka-producer-network-thread | producer-208] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-208] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:56 [kafka-producer-network-thread | producer-208] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-208] ProducerId set to 9388 with epoch 0
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_39, value: {"_id": {"$oid": "67f10b97558ab0f6396b1431"}, "review": "Some features are good, others not so much.", "authId": "user_39", "datetime": "2025-01-13T19:31:50.928Z", "sentiment": null}, offset: 70977
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_39
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1431"}, "review": "Some features are good, others not so much.", "authId": "user_39", "datetime": "2025-01-13T19:31:50.928Z", "sentiment": null}%n
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:46:56 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-209
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:56 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:56 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-209] Instantiated an idempotent producer.
2025-04-13 11:46:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:56 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544816547
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:56 [kafka-producer-network-thread | producer-209] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-209] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_39', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:46:56 [kafka-producer-network-thread | producer-209] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-209] ProducerId set to 9389 with epoch 0
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_88, value: {"_id": {"$oid": "67f10b97558ab0f6396b1462"}, "review": "Decent, but not amazing.", "authId": "user_88", "datetime": "2025-03-21T21:19:57.894Z", "sentiment": null}, offset: 70978
2025-04-13 11:46:56 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_88
2025-04-13 11:46:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1462"}, "review": "Decent, but not amazing.", "authId": "user_88", "datetime": "2025-03-21T21:19:57.894Z", "sentiment": null}%n
2025-04-13 11:46:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:46:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:46:57 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-210
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:57 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:57 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-210] Instantiated an idempotent producer.
2025-04-13 11:46:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544817104
2025-04-13 11:46:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:57 [kafka-producer-network-thread | producer-210] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-210] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_88', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:46:57 [kafka-producer-network-thread | producer-210] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-210] ProducerId set to 9390 with epoch 0
2025-04-13 11:46:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_88, value: {"_id": {"$oid": "67f10b97558ab0f6396b1462"}, "review": "Decent, but not amazing.", "authId": "user_88", "datetime": "2025-03-21T21:19:57.894Z", "sentiment": null}, offset: 70979
2025-04-13 11:46:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_88
2025-04-13 11:46:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1462"}, "review": "Decent, but not amazing.", "authId": "user_88", "datetime": "2025-03-21T21:19:57.894Z", "sentiment": null}%n
2025-04-13 11:46:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:46:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:46:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-211
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-211] Instantiated an idempotent producer.
2025-04-13 11:46:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544818830
2025-04-13 11:46:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_88', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:46:58 [kafka-producer-network-thread | producer-211] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-211] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:58 [kafka-producer-network-thread | producer-211] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-211] ProducerId set to 9391 with epoch 0
2025-04-13 11:46:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:58 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_54, value: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}, offset: 70980
2025-04-13 11:46:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_54
2025-04-13 11:46:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}%n
2025-04-13 11:46:59 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:46:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:46:59 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-212
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:46:59 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:46:59 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-212] Instantiated an idempotent producer.
2025-04-13 11:46:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:46:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:46:59 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544819359
2025-04-13 11:46:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:46:59 [kafka-producer-network-thread | producer-212] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-212] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:46:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_54', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:46:59 [kafka-producer-network-thread | producer-212] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-212] ProducerId set to 9392 with epoch 0
2025-04-13 11:46:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:46:59 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_25, value: {"_id": {"$oid": "67f10b97558ab0f6396b1423"}, "review": "Serves its purpose, but nothing more.", "authId": "user_25", "datetime": "2025-03-27T11:39:09.683Z", "sentiment": null}, offset: 70981
2025-04-13 11:46:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_25
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1423"}, "review": "Serves its purpose, but nothing more.", "authId": "user_25", "datetime": "2025-03-27T11:39:09.683Z", "sentiment": null}%n
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:47:00 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-213
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:00 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:00 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-213] Instantiated an idempotent producer.
2025-04-13 11:47:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544820118
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_25', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:47:00 [kafka-producer-network-thread | producer-213] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-213] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:00 [kafka-producer-network-thread | producer-213] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-213] ProducerId set to 9393 with epoch 0
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_90, value: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}, offset: 70982
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_90
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}%n
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: -0.1

2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: -0.1

2025-04-13 11:47:00 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-214
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:00 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:00 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-214] Instantiated an idempotent producer.
2025-04-13 11:47:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544820710
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_90', sentiment=magnitude: 0.1
score: -0.1
}
2025-04-13 11:47:00 [kafka-producer-network-thread | producer-214] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-214] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:00 [kafka-producer-network-thread | producer-214] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-214] ProducerId set to 9394 with epoch 0
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_45, value: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}, offset: 70983
2025-04-13 11:47:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_45
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}%n
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-215
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-215] Instantiated an idempotent producer.
2025-04-13 11:47:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544821215
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_45', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:01 [kafka-producer-network-thread | producer-215] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-215] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:01 [kafka-producer-network-thread | producer-215] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-215] ProducerId set to 9395 with epoch 0
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_85, value: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}, offset: 70984
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_85
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}%n
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:47:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-216
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-216] Instantiated an idempotent producer.
2025-04-13 11:47:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:01 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544821725
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_85', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:47:01 [kafka-producer-network-thread | producer-216] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-216] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:01 [kafka-producer-network-thread | producer-216] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-216] ProducerId set to 9396 with epoch 0
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_68, value: {"_id": {"$oid": "67f10b97558ab0f6396b144e"}, "review": "Poor build quality, not reliable.", "authId": "user_68", "datetime": "2025-01-15T21:21:03.997Z", "sentiment": null}, offset: 70985
2025-04-13 11:47:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_68
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144e"}, "review": "Poor build quality, not reliable.", "authId": "user_68", "datetime": "2025-01-15T21:21:03.997Z", "sentiment": null}%n
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:02 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-217
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:02 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:02 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-217] Instantiated an idempotent producer.
2025-04-13 11:47:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544822249
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_68', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:02 [kafka-producer-network-thread | producer-217] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-217] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:02 [kafka-producer-network-thread | producer-217] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-217] ProducerId set to 9397 with epoch 0
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_1, value: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}, offset: 70986
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_1
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}%n
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:02 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-218
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:02 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:02 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-218] Instantiated an idempotent producer.
2025-04-13 11:47:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544822766
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_1', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:02 [kafka-producer-network-thread | producer-218] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-218] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:02 [kafka-producer-network-thread | producer-218] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-218] ProducerId set to 9398 with epoch 0
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_7, value: {"_id": {"$oid": "67f10b97558ab0f6396b1411"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_7", "datetime": "2025-03-02T02:14:03.248Z", "sentiment": null}, offset: 70987
2025-04-13 11:47:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_7
2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1411"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_7", "datetime": "2025-03-02T02:14:03.248Z", "sentiment": null}%n
2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:47:03 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-219
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:03 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:03 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-219] Instantiated an idempotent producer.
2025-04-13 11:47:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544823264
2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_7', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:47:03 [kafka-producer-network-thread | producer-219] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-219] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:03 [kafka-producer-network-thread | producer-219] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-219] ProducerId set to 9399 with epoch 0
2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_9, value: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}, offset: 70988
2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_9
2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}%n
2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:47:03 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-220
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:03 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:03 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-220] Instantiated an idempotent producer.
2025-04-13 11:47:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544823794
2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_9', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:47:03 [kafka-producer-network-thread | producer-220] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-220] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:03 [kafka-producer-network-thread | producer-220] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-220] ProducerId set to 9400 with epoch 0
2025-04-13 11:47:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:04 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_80, value: {"_id": {"$oid": "67f10b97558ab0f6396b145a"}, "review": "Excellent! Totally worth the price.", "authId": "user_80", "datetime": "2024-12-05T07:49:50.162Z", "sentiment": null}, offset: 70989
2025-04-13 11:47:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_80
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145a"}, "review": "Excellent! Totally worth the price.", "authId": "user_80", "datetime": "2024-12-05T07:49:50.162Z", "sentiment": null}%n
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.4
score: 0.7

2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.4
score: 0.7

2025-04-13 11:47:05 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-221
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:05 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:05 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-221] Instantiated an idempotent producer.
2025-04-13 11:47:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544825329
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_80', sentiment=magnitude: 1.4
score: 0.7
}
2025-04-13 11:47:05 [kafka-producer-network-thread | producer-221] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-221] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:05 [kafka-producer-network-thread | producer-221] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-221] ProducerId set to 9401 with epoch 0
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_25, value: {"_id": {"$oid": "67f10b97558ab0f6396b1423"}, "review": "Serves its purpose, but nothing more.", "authId": "user_25", "datetime": "2025-03-27T11:39:09.683Z", "sentiment": null}, offset: 70990
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_25
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1423"}, "review": "Serves its purpose, but nothing more.", "authId": "user_25", "datetime": "2025-03-27T11:39:09.683Z", "sentiment": null}%n
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:47:05 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-222
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:05 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:05 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-222] Instantiated an idempotent producer.
2025-04-13 11:47:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:05 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544825846
2025-04-13 11:47:05 [kafka-producer-network-thread | producer-222] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-222] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:05 [kafka-producer-network-thread | producer-222] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-222] ProducerId set to 9402 with epoch 0
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_25', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_1, value: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}, offset: 70991
2025-04-13 11:47:05 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_1
2025-04-13 11:47:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}%n
2025-04-13 11:47:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:06 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-223
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:06 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:06 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-223] Instantiated an idempotent producer.
2025-04-13 11:47:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544826339
2025-04-13 11:47:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_1', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:06 [kafka-producer-network-thread | producer-223] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-223] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:06 [kafka-producer-network-thread | producer-223] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-223] ProducerId set to 9403 with epoch 0
2025-04-13 11:47:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:06 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 70992
2025-04-13 11:47:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-13 11:47:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}%n
2025-04-13 11:47:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:47:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:47:07 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-224
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:07 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:07 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-224] Instantiated an idempotent producer.
2025-04-13 11:47:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544827669
2025-04-13 11:47:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_91', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:47:07 [kafka-producer-network-thread | producer-224] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-224] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:07 [kafka-producer-network-thread | producer-224] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-224] ProducerId set to 9404 with epoch 0
2025-04-13 11:47:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:07 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_97, value: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}, offset: 70993
2025-04-13 11:47:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_97
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146b"}, "review": "Mediocre quality, nothing special.", "authId": "user_97", "datetime": "2025-01-28T15:08:57.912Z", "sentiment": null}%n
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:08 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-225
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:08 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:08 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-225] Instantiated an idempotent producer.
2025-04-13 11:47:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544828161
2025-04-13 11:47:08 [kafka-producer-network-thread | producer-225] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-225] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:08 [kafka-producer-network-thread | producer-225] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-225] ProducerId set to 9405 with epoch 0
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_97', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_1, value: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}, offset: 70994
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_1
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}%n
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:08 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-226
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:08 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:08 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-226] Instantiated an idempotent producer.
2025-04-13 11:47:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:08 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544828679
2025-04-13 11:47:08 [kafka-producer-network-thread | producer-226] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-226] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:08 [kafka-producer-network-thread | producer-226] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-226] ProducerId set to 9406 with epoch 0
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_1', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_13, value: {"_id": {"$oid": "67f10b97558ab0f6396b1417"}, "review": "It broke after a week of use.", "authId": "user_13", "datetime": "2024-12-05T23:56:15.604Z", "sentiment": null}, offset: 70995
2025-04-13 11:47:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_13
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1417"}, "review": "It broke after a week of use.", "authId": "user_13", "datetime": "2024-12-05T23:56:15.604Z", "sentiment": null}%n
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:47:10 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-227
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:10 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:10 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-227] Instantiated an idempotent producer.
2025-04-13 11:47:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:10 [kafka-producer-network-thread | producer-227] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-227] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:10 [kafka-producer-network-thread | producer-227] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-227] ProducerId set to 9407 with epoch 0
2025-04-13 11:47:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544830194
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_13', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_5, value: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}, offset: 70996
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_5
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140f"}, "review": "Decent, but not amazing.", "authId": "user_5", "datetime": "2024-12-09T15:12:06.317Z", "sentiment": null}%n
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:47:10 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-228
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:10 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:10 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-228] Instantiated an idempotent producer.
2025-04-13 11:47:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544830936
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_5', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:47:10 [kafka-producer-network-thread | producer-228] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-228] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:10 [kafka-producer-network-thread | producer-228] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-228] ProducerId set to 9408 with epoch 0
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 70997
2025-04-13 11:47:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-13 11:47:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-13 11:47:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:11 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-229
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:11 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:11 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-229] Instantiated an idempotent producer.
2025-04-13 11:47:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:11 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544831441
2025-04-13 11:47:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:11 [kafka-producer-network-thread | producer-229] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-229] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:11 [kafka-producer-network-thread | producer-229] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-229] ProducerId set to 9409 with epoch 0
2025-04-13 11:47:11 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:11 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_98, value: {"_id": {"$oid": "67f10b97558ab0f6396b146c"}, "review": "Not bad, not great either.", "authId": "user_98", "datetime": "2025-03-21T23:54:21.817Z", "sentiment": null}, offset: 70998
2025-04-13 11:47:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_98
2025-04-13 11:47:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146c"}, "review": "Not bad, not great either.", "authId": "user_98", "datetime": "2025-03-21T23:54:21.817Z", "sentiment": null}%n
2025-04-13 11:47:12 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:47:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:47:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-230
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:12 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-230] Instantiated an idempotent producer.
2025-04-13 11:47:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544832917
2025-04-13 11:47:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_98', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:47:12 [kafka-producer-network-thread | producer-230] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-230] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:12 [kafka-producer-network-thread | producer-230] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-230] ProducerId set to 9410 with epoch 0
2025-04-13 11:47:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:13 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_100, value: {"_id": {"$oid": "67f10b97558ab0f6396b146e"}, "review": "Terrible experience, very disappointed.", "authId": "user_100", "datetime": "2025-02-03T10:22:13.147Z", "sentiment": null}, offset: 70999
2025-04-13 11:47:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_100
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146e"}, "review": "Terrible experience, very disappointed.", "authId": "user_100", "datetime": "2025-02-03T10:22:13.147Z", "sentiment": null}%n
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:15 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-231
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:15 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:15 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-231] Instantiated an idempotent producer.
2025-04-13 11:47:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544835386
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_100', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:15 [kafka-producer-network-thread | producer-231] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-231] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:15 [kafka-producer-network-thread | producer-231] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-231] ProducerId set to 9411 with epoch 0
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_45, value: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}, offset: 71000
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_45
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}%n
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:15 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-232
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:15 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:15 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-232] Instantiated an idempotent producer.
2025-04-13 11:47:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544835960
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:15 [kafka-producer-network-thread | producer-232] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-232] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_45', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:15 [kafka-producer-network-thread | producer-232] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-232] ProducerId set to 9412 with epoch 0
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_96, value: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}, offset: 71001
2025-04-13 11:47:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_96
2025-04-13 11:47:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}%n
2025-04-13 11:47:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:47:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:47:16 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-233
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:16 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:16 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-233] Instantiated an idempotent producer.
2025-04-13 11:47:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544836522
2025-04-13 11:47:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_96', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:47:16 [kafka-producer-network-thread | producer-233] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-233] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:16 [kafka-producer-network-thread | producer-233] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-233] ProducerId set to 9413 with epoch 0
2025-04-13 11:47:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:16 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_96, value: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}, offset: 71002
2025-04-13 11:47:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_96
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}%n
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:47:19 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-234
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:19 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:19 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-234] Instantiated an idempotent producer.
2025-04-13 11:47:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544839060
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_96', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:47:19 [kafka-producer-network-thread | producer-234] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-234] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:19 [kafka-producer-network-thread | producer-234] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-234] ProducerId set to 9414 with epoch 0
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_43, value: {"_id": {"$oid": "67f10b97558ab0f6396b1435"}, "review": "Not bad, not great either.", "authId": "user_43", "datetime": "2025-01-07T00:44:08.424Z", "sentiment": null}, offset: 71003
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_43
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1435"}, "review": "Not bad, not great either.", "authId": "user_43", "datetime": "2025-01-07T00:44:08.424Z", "sentiment": null}%n
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:47:19 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-235
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:19 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:19 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-235] Instantiated an idempotent producer.
2025-04-13 11:47:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544839582
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_43', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:47:19 [kafka-producer-network-thread | producer-235] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-235] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:19 [kafka-producer-network-thread | producer-235] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-235] ProducerId set to 9415 with epoch 0
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_3, value: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}, offset: 71004
2025-04-13 11:47:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_3
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}%n
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 11:47:21 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-236
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:21 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:21 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-236] Instantiated an idempotent producer.
2025-04-13 11:47:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544841028
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_3', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 11:47:21 [kafka-producer-network-thread | producer-236] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-236] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:21 [kafka-producer-network-thread | producer-236] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-236] ProducerId set to 9416 with epoch 0
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_3, value: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}, offset: 71005
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_3
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140d"}, "review": "Fantastic performance and very durable.", "authId": "user_3", "datetime": "2025-02-25T04:22:40.515Z", "sentiment": null}%n
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 11:47:21 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-237
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:21 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:21 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-237] Instantiated an idempotent producer.
2025-04-13 11:47:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544841560
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_3', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 11:47:21 [kafka-producer-network-thread | producer-237] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-237] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:21 [kafka-producer-network-thread | producer-237] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-237] ProducerId set to 9417 with epoch 0
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_85, value: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}, offset: 71006
2025-04-13 11:47:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_85
2025-04-13 11:47:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b145f"}, "review": "It's okay, does the job.", "authId": "user_85", "datetime": "2025-02-13T10:05:46.461Z", "sentiment": null}%n
2025-04-13 11:47:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:47:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:47:22 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-238
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:22 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:22 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-238] Instantiated an idempotent producer.
2025-04-13 11:47:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544842264
2025-04-13 11:47:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_85', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:47:22 [kafka-producer-network-thread | producer-238] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-238] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:22 [kafka-producer-network-thread | producer-238] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-238] ProducerId set to 9418 with epoch 0
2025-04-13 11:47:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:22 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_70, value: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}, offset: 71007
2025-04-13 11:47:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_70
2025-04-13 11:47:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1450"}, "review": "Not bad, not great either.", "authId": "user_70", "datetime": "2025-02-04T17:31:50.204Z", "sentiment": null}%n
2025-04-13 11:47:23 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:47:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:47:23 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-239
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:23 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:23 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-239] Instantiated an idempotent producer.
2025-04-13 11:47:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:23 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544843734
2025-04-13 11:47:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_70', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:47:23 [kafka-producer-network-thread | producer-239] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-239] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:23 [kafka-producer-network-thread | producer-239] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-239] ProducerId set to 9419 with epoch 0
2025-04-13 11:47:23 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:23 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_99, value: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}, offset: 71008
2025-04-13 11:47:23 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_99
2025-04-13 11:47:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}%n
2025-04-13 11:47:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:47:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:47:24 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-240
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:24 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:24 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-240] Instantiated an idempotent producer.
2025-04-13 11:47:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544844464
2025-04-13 11:47:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_99', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:47:24 [kafka-producer-network-thread | producer-240] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-240] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:24 [kafka-producer-network-thread | producer-240] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-240] ProducerId set to 9420 with epoch 0
2025-04-13 11:47:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:25 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_78, value: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}, offset: 71009
2025-04-13 11:47:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_78
2025-04-13 11:47:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1458"}, "review": "Some features are good, others not so much.", "authId": "user_78", "datetime": "2025-03-27T01:59:52.899Z", "sentiment": null}%n
2025-04-13 11:47:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:47:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:47:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-241
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-241] Instantiated an idempotent producer.
2025-04-13 11:47:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544846923
2025-04-13 11:47:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_78', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:47:26 [kafka-producer-network-thread | producer-241] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-241] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:26 [kafka-producer-network-thread | producer-241] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-241] ProducerId set to 9421 with epoch 0
2025-04-13 11:47:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:26 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_54, value: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}, offset: 71010
2025-04-13 11:47:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_54
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}%n
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:47:27 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-242
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:27 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:27 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-242] Instantiated an idempotent producer.
2025-04-13 11:47:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544847427
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_54', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:47:27 [kafka-producer-network-thread | producer-242] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-242] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:27 [kafka-producer-network-thread | producer-242] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-242] ProducerId set to 9422 with epoch 0
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_11, value: {"_id": {"$oid": "67f10b97558ab0f6396b1415"}, "review": "Would not recommend to anyone.", "authId": "user_11", "datetime": "2024-12-10T23:20:11.570Z", "sentiment": null}, offset: 71011
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_11
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1415"}, "review": "Would not recommend to anyone.", "authId": "user_11", "datetime": "2024-12-10T23:20:11.570Z", "sentiment": null}%n
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:27 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-243
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:27 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:27 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-243] Instantiated an idempotent producer.
2025-04-13 11:47:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544847948
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_11', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:27 [kafka-producer-network-thread | producer-243] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-243] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:27 [kafka-producer-network-thread | producer-243] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-243] ProducerId set to 9423 with epoch 0
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_20, value: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}, offset: 71012
2025-04-13 11:47:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_20
2025-04-13 11:47:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141e"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_20", "datetime": "2025-02-18T12:03:20.168Z", "sentiment": null}%n
2025-04-13 11:47:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:47:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:47:28 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-244
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:28 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:28 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-244] Instantiated an idempotent producer.
2025-04-13 11:47:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544848447
2025-04-13 11:47:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_20', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:47:28 [kafka-producer-network-thread | producer-244] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-244] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:28 [kafka-producer-network-thread | producer-244] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-244] ProducerId set to 9424 with epoch 0
2025-04-13 11:47:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:28 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_94, value: {"_id": {"$oid": "67f10b97558ab0f6396b1468"}, "review": "Great quality, will buy again.", "authId": "user_94", "datetime": "2025-03-23T03:36:43.648Z", "sentiment": null}, offset: 71013
2025-04-13 11:47:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_94
2025-04-13 11:47:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1468"}, "review": "Great quality, will buy again.", "authId": "user_94", "datetime": "2025-03-23T03:36:43.648Z", "sentiment": null}%n
2025-04-13 11:47:29 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: 0.1

2025-04-13 11:47:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: 0.1

2025-04-13 11:47:29 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-245
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:29 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:29 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-245] Instantiated an idempotent producer.
2025-04-13 11:47:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:29 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544849916
2025-04-13 11:47:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:29 [kafka-producer-network-thread | producer-245] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-245] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_94', sentiment=magnitude: 0.1
score: 0.1
}
2025-04-13 11:47:29 [kafka-producer-network-thread | producer-245] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-245] ProducerId set to 9425 with epoch 0
2025-04-13 11:47:29 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:29 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_99, value: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}, offset: 71014
2025-04-13 11:47:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_99
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}%n
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:47:31 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-246
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:31 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:31 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-246] Instantiated an idempotent producer.
2025-04-13 11:47:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544851416
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_99', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:47:31 [kafka-producer-network-thread | producer-246] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-246] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:31 [kafka-producer-network-thread | producer-246] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-246] ProducerId set to 9426 with epoch 0
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_60, value: {"_id": {"$oid": "67f10b97558ab0f6396b1446"}, "review": "Not satisfied at all.", "authId": "user_60", "datetime": "2025-03-10T13:21:03.219Z", "sentiment": null}, offset: 71015
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_60
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1446"}, "review": "Not satisfied at all.", "authId": "user_60", "datetime": "2025-03-10T13:21:03.219Z", "sentiment": null}%n
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:31 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-247
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:31 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:31 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-247] Instantiated an idempotent producer.
2025-04-13 11:47:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544851926
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_60', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:31 [kafka-producer-network-thread | producer-247] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-247] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:31 [kafka-producer-network-thread | producer-247] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-247] ProducerId set to 9427 with epoch 0
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_92, value: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}, offset: 71016
2025-04-13 11:47:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_92
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}%n
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:47:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-248
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-248] Instantiated an idempotent producer.
2025-04-13 11:47:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:33 [kafka-producer-network-thread | producer-248] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-248] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544853386
2025-04-13 11:47:33 [kafka-producer-network-thread | producer-248] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-248] ProducerId set to 9428 with epoch 0
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_92', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_30, value: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}, offset: 71017
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_30
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1428"}, "review": "Fairly average, you get what you pay for.", "authId": "user_30", "datetime": "2024-12-13T03:57:18.549Z", "sentiment": null}%n
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:47:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-249
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-249] Instantiated an idempotent producer.
2025-04-13 11:47:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544853943
2025-04-13 11:47:33 [kafka-producer-network-thread | producer-249] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-249] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:33 [kafka-producer-network-thread | producer-249] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-249] ProducerId set to 9429 with epoch 0
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_30', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_63, value: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}, offset: 71018
2025-04-13 11:47:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_63
2025-04-13 11:47:34 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}%n
2025-04-13 11:47:34 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 11:47:34 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 11:47:34 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-250
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:34 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:34 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-250] Instantiated an idempotent producer.
2025-04-13 11:47:34 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:34 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:34 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544854470
2025-04-13 11:47:34 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:34 [kafka-producer-network-thread | producer-250] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-250] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:34 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_63', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 11:47:34 [kafka-producer-network-thread | producer-250] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-250] ProducerId set to 9430 with epoch 0
2025-04-13 11:47:34 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_12, value: {"_id": {"$oid": "67f10b97558ab0f6396b1416"}, "review": "Not satisfied at all.", "authId": "user_12", "datetime": "2025-01-13T23:31:43.542Z", "sentiment": null}, offset: 71019
2025-04-13 11:47:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_12
2025-04-13 11:47:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1416"}, "review": "Not satisfied at all.", "authId": "user_12", "datetime": "2025-01-13T23:31:43.542Z", "sentiment": null}%n
2025-04-13 11:47:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-251
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-251] Instantiated an idempotent producer.
2025-04-13 11:47:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544855961
2025-04-13 11:47:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:35 [kafka-producer-network-thread | producer-251] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-251] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_12', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:35 [kafka-producer-network-thread | producer-251] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-251] ProducerId set to 9431 with epoch 0
2025-04-13 11:47:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_63, value: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}, offset: 71020
2025-04-13 11:47:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_63
2025-04-13 11:47:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}%n
2025-04-13 11:47:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 11:47:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 11:47:36 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-252
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:36 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:36 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-252] Instantiated an idempotent producer.
2025-04-13 11:47:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544856591
2025-04-13 11:47:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_63', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 11:47:36 [kafka-producer-network-thread | producer-252] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-252] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:36 [kafka-producer-network-thread | producer-252] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-252] ProducerId set to 9432 with epoch 0
2025-04-13 11:47:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:36 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_95, value: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}, offset: 71021
2025-04-13 11:47:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_95
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1469"}, "review": "Not satisfied at all.", "authId": "user_95", "datetime": "2024-12-16T14:03:49.208Z", "sentiment": null}%n
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:37 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-253
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:37 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:37 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-253] Instantiated an idempotent producer.
2025-04-13 11:47:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544857157
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_95', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:37 [kafka-producer-network-thread | producer-253] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-253] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:37 [kafka-producer-network-thread | producer-253] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-253] ProducerId set to 9433 with epoch 0
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_87, value: {"_id": {"$oid": "67f10b97558ab0f6396b1461"}, "review": "Im very happy with this purchase!", "authId": "user_87", "datetime": "2025-03-25T22:05:56.300Z", "sentiment": null}, offset: 71022
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_87
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1461"}, "review": "Im very happy with this purchase!", "authId": "user_87", "datetime": "2025-03-25T22:05:56.300Z", "sentiment": null}%n
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:47:37 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-254
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:37 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:37 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-254] Instantiated an idempotent producer.
2025-04-13 11:47:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544857701
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_87', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:47:37 [kafka-producer-network-thread | producer-254] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-254] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:37 [kafka-producer-network-thread | producer-254] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-254] ProducerId set to 9434 with epoch 0
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_99, value: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}, offset: 71023
2025-04-13 11:47:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_99
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146d"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_99", "datetime": "2025-04-01T13:25:28.546Z", "sentiment": null}%n
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:47:38 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-255
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:38 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:38 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-255] Instantiated an idempotent producer.
2025-04-13 11:47:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544858202
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_99', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:47:38 [kafka-producer-network-thread | producer-255] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-255] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:38 [kafka-producer-network-thread | producer-255] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-255] ProducerId set to 9435 with epoch 0
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_64, value: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}, offset: 71024
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_64
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144a"}, "review": "Poor build quality, not reliable.", "authId": "user_64", "datetime": "2025-03-26T18:17:49.947Z", "sentiment": null}%n
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:38 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-256
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:38 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:38 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-256] Instantiated an idempotent producer.
2025-04-13 11:47:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544858775
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:38 [kafka-producer-network-thread | producer-256] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-256] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_64', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:38 [kafka-producer-network-thread | producer-256] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-256] ProducerId set to 9436 with epoch 0
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_14, value: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}, offset: 71025
2025-04-13 11:47:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_14
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}%n
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:47:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-257
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-257] Instantiated an idempotent producer.
2025-04-13 11:47:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544859304
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:39 [kafka-producer-network-thread | producer-257] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-257] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_14', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:47:39 [kafka-producer-network-thread | producer-257] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-257] ProducerId set to 9437 with epoch 0
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_92, value: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}, offset: 71026
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_92
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}%n
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:47:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-258
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-258] Instantiated an idempotent producer.
2025-04-13 11:47:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544859836
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:39 [kafka-producer-network-thread | producer-258] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-258] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_92', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:47:39 [kafka-producer-network-thread | producer-258] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-258] ProducerId set to 9438 with epoch 0
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_73, value: {"_id": {"$oid": "67f10b97558ab0f6396b1453"}, "review": "Not satisfied at all.", "authId": "user_73", "datetime": "2025-03-31T12:24:38.653Z", "sentiment": null}, offset: 71027
2025-04-13 11:47:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_73
2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1453"}, "review": "Not satisfied at all.", "authId": "user_73", "datetime": "2025-03-31T12:24:38.653Z", "sentiment": null}%n
2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:41 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-259
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:41 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:41 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-259] Instantiated an idempotent producer.
2025-04-13 11:47:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544861303
2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:41 [kafka-producer-network-thread | producer-259] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-259] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_73', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:41 [kafka-producer-network-thread | producer-259] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-259] ProducerId set to 9439 with epoch 0
2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_72, value: {"_id": {"$oid": "67f10b97558ab0f6396b1452"}, "review": "Mediocre quality, nothing special.", "authId": "user_72", "datetime": "2025-01-26T15:04:47.280Z", "sentiment": null}, offset: 71028
2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_72
2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1452"}, "review": "Mediocre quality, nothing special.", "authId": "user_72", "datetime": "2025-01-26T15:04:47.280Z", "sentiment": null}%n
2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:41 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-260
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:41 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:41 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-260] Instantiated an idempotent producer.
2025-04-13 11:47:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:41 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544861793
2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:41 [kafka-producer-network-thread | producer-260] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-260] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_72', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:41 [kafka-producer-network-thread | producer-260] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-260] ProducerId set to 9440 with epoch 0
2025-04-13 11:47:41 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:42 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_96, value: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}, offset: 71029
2025-04-13 11:47:42 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_96
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b146a"}, "review": "Decent, but not amazing.", "authId": "user_96", "datetime": "2025-02-28T22:22:03.683Z", "sentiment": null}%n
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:47:44 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-261
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:44 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:44 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-261] Instantiated an idempotent producer.
2025-04-13 11:47:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544864232
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_96', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:47:44 [kafka-producer-network-thread | producer-261] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-261] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:44 [kafka-producer-network-thread | producer-261] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-261] ProducerId set to 9441 with epoch 0
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_15, value: {"_id": {"$oid": "67f10b97558ab0f6396b1419"}, "review": "Mediocre quality, nothing special.", "authId": "user_15", "datetime": "2024-12-29T15:59:17.459Z", "sentiment": null}, offset: 71030
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_15
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1419"}, "review": "Mediocre quality, nothing special.", "authId": "user_15", "datetime": "2024-12-29T15:59:17.459Z", "sentiment": null}%n
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:44 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-262
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:44 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:44 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-262] Instantiated an idempotent producer.
2025-04-13 11:47:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:44 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544864754
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_15', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:44 [kafka-producer-network-thread | producer-262] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-262] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:44 [kafka-producer-network-thread | producer-262] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-262] ProducerId set to 9442 with epoch 0
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_89, value: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}, offset: 71031
2025-04-13 11:47:44 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_89
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}%n
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:47:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-263
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-263] Instantiated an idempotent producer.
2025-04-13 11:47:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544865320
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_89', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:47:45 [kafka-producer-network-thread | producer-263] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-263] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:45 [kafka-producer-network-thread | producer-263] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-263] ProducerId set to 9443 with epoch 0
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_14, value: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}, offset: 71032
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_14
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1418"}, "review": "Expected more, but it's fine.", "authId": "user_14", "datetime": "2025-01-31T11:16:57.627Z", "sentiment": null}%n
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:47:45 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-264
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:45 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:45 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-264] Instantiated an idempotent producer.
2025-04-13 11:47:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:45 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544865936
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_14', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:47:45 [kafka-producer-network-thread | producer-264] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-264] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:45 [kafka-producer-network-thread | producer-264] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-264] ProducerId set to 9444 with epoch 0
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_53, value: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}, offset: 71033
2025-04-13 11:47:45 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_53
2025-04-13 11:47:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143f"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_53", "datetime": "2024-12-08T11:01:11.696Z", "sentiment": null}%n
2025-04-13 11:47:46 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:47:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:47:46 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-265
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:46 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:46 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-265] Instantiated an idempotent producer.
2025-04-13 11:47:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:46 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544866728
2025-04-13 11:47:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_53', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:47:46 [kafka-producer-network-thread | producer-265] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-265] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:46 [kafka-producer-network-thread | producer-265] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-265] ProducerId set to 9445 with epoch 0
2025-04-13 11:47:46 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:46 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_49, value: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "Its alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}, offset: 71034
2025-04-13 11:47:46 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_49
2025-04-13 11:47:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "Its alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}%n
2025-04-13 11:47:48 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-13 11:47:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-13 11:47:48 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-266
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:48 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:48 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-266] Instantiated an idempotent producer.
2025-04-13 11:47:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:48 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544868175
2025-04-13 11:47:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_49', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-13 11:47:48 [kafka-producer-network-thread | producer-266] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-266] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:48 [kafka-producer-network-thread | producer-266] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-266] ProducerId set to 9446 with epoch 0
2025-04-13 11:47:48 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:48 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 71035
2025-04-13 11:47:48 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-13 11:47:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-13 11:47:49 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:47:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:47:49 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-267
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:49 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:49 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-267] Instantiated an idempotent producer.
2025-04-13 11:47:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:49 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544869665
2025-04-13 11:47:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:47:49 [kafka-producer-network-thread | producer-267] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-267] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:49 [kafka-producer-network-thread | producer-267] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-267] ProducerId set to 9447 with epoch 0
2025-04-13 11:47:49 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:49 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_1, value: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}, offset: 71036
2025-04-13 11:47:49 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_1
2025-04-13 11:47:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}%n
2025-04-13 11:47:52 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:52 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-268
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:52 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:52 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-268] Instantiated an idempotent producer.
2025-04-13 11:47:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:52 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544872315
2025-04-13 11:47:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:52 [kafka-producer-network-thread | producer-268] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-268] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_1', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:52 [kafka-producer-network-thread | producer-268] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-268] ProducerId set to 9448 with epoch 0
2025-04-13 11:47:52 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:52 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_63, value: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}, offset: 71037
2025-04-13 11:47:52 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_63
2025-04-13 11:47:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}%n
2025-04-13 11:47:53 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 11:47:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 11:47:53 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-269
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:53 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:53 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-269] Instantiated an idempotent producer.
2025-04-13 11:47:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:53 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544873850
2025-04-13 11:47:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:53 [kafka-producer-network-thread | producer-269] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-269] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_63', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 11:47:53 [kafka-producer-network-thread | producer-269] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-269] ProducerId set to 9449 with epoch 0
2025-04-13 11:47:53 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:53 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_37, value: {"_id": {"$oid": "67f10b97558ab0f6396b142f"}, "review": "Mediocre quality, nothing special.", "authId": "user_37", "datetime": "2024-12-22T11:29:54.974Z", "sentiment": null}, offset: 71038
2025-04-13 11:47:53 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_37
2025-04-13 11:47:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142f"}, "review": "Mediocre quality, nothing special.", "authId": "user_37", "datetime": "2024-12-22T11:29:54.974Z", "sentiment": null}%n
2025-04-13 11:47:54 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:47:54 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:47:54 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-270
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:54 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:54 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-270] Instantiated an idempotent producer.
2025-04-13 11:47:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:54 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544874355
2025-04-13 11:47:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:54 [kafka-producer-network-thread | producer-270] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-270] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_37', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:47:54 [kafka-producer-network-thread | producer-270] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-270] ProducerId set to 9450 with epoch 0
2025-04-13 11:47:54 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_54, value: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}, offset: 71039
2025-04-13 11:47:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_54
2025-04-13 11:47:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1440"}, "review": "Excellent! Totally worth the price.", "authId": "user_54", "datetime": "2025-02-21T00:14:55.130Z", "sentiment": null}%n
2025-04-13 11:47:55 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:47:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:47:55 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-271
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:55 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:55 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-271] Instantiated an idempotent producer.
2025-04-13 11:47:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:55 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544875891
2025-04-13 11:47:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_54', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:47:55 [kafka-producer-network-thread | producer-271] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-271] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:55 [kafka-producer-network-thread | producer-271] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-271] ProducerId set to 9451 with epoch 0
2025-04-13 11:47:55 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:55 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_63, value: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}, offset: 71040
2025-04-13 11:47:55 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_63
2025-04-13 11:47:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1449"}, "review": "Fantastic performance and very durable.", "authId": "user_63", "datetime": "2025-02-03T10:12:02.742Z", "sentiment": null}%n
2025-04-13 11:47:57 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: 0.3

2025-04-13 11:47:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: 0.3

2025-04-13 11:47:57 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-272
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:57 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:57 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-272] Instantiated an idempotent producer.
2025-04-13 11:47:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:57 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544877382
2025-04-13 11:47:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_63', sentiment=magnitude: 0.3
score: 0.3
}
2025-04-13 11:47:57 [kafka-producer-network-thread | producer-272] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-272] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:57 [kafka-producer-network-thread | producer-272] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-272] ProducerId set to 9452 with epoch 0
2025-04-13 11:47:57 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:57 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_49, value: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "Its alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}, offset: 71041
2025-04-13 11:47:57 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_49
2025-04-13 11:47:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143b"}, "review": "Its alright for the price.", "authId": "user_49", "datetime": "2025-02-09T13:25:35.412Z", "sentiment": null}%n
2025-04-13 11:47:58 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-13 11:47:58 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-13 11:47:58 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-273
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:47:58 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:47:58 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-273] Instantiated an idempotent producer.
2025-04-13 11:47:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:47:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:47:58 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544878904
2025-04-13 11:47:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:47:58 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_49', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-13 11:47:59 [kafka-producer-network-thread | producer-273] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-273] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:47:59 [kafka-producer-network-thread | producer-273] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-273] ProducerId set to 9453 with epoch 0
2025-04-13 11:47:59 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:47:59 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_60, value: {"_id": {"$oid": "67f10b97558ab0f6396b1446"}, "review": "Not satisfied at all.", "authId": "user_60", "datetime": "2025-03-10T13:21:03.219Z", "sentiment": null}, offset: 71042
2025-04-13 11:47:59 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_60
2025-04-13 11:48:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1446"}, "review": "Not satisfied at all.", "authId": "user_60", "datetime": "2025-03-10T13:21:03.219Z", "sentiment": null}%n
2025-04-13 11:48:00 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:00 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-274
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:00 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:00 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-274] Instantiated an idempotent producer.
2025-04-13 11:48:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:00 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544880518
2025-04-13 11:48:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_60', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:00 [kafka-producer-network-thread | producer-274] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-274] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:00 [kafka-producer-network-thread | producer-274] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-274] ProducerId set to 9454 with epoch 0
2025-04-13 11:48:00 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:00 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 71043
2025-04-13 11:48:00 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-13 11:48:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-13 11:48:01 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:01 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:01 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-275
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:01 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:01 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-275] Instantiated an idempotent producer.
2025-04-13 11:48:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544882001
2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:02 [kafka-producer-network-thread | producer-275] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-275] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:02 [kafka-producer-network-thread | producer-275] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-275] ProducerId set to 9455 with epoch 0
2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_41, value: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}, offset: 71044
2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_41
2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1433"}, "review": "Mediocre quality, nothing special.", "authId": "user_41", "datetime": "2024-12-16T06:59:16.471Z", "sentiment": null}%n
2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:02 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-276
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:02 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:02 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-276] Instantiated an idempotent producer.
2025-04-13 11:48:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:02 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544882510
2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_41', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:02 [kafka-producer-network-thread | producer-276] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-276] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:02 [kafka-producer-network-thread | producer-276] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-276] ProducerId set to 9456 with epoch 0
2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 71045
2025-04-13 11:48:02 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-13 11:48:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}%n
2025-04-13 11:48:03 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:48:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:48:03 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-277
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:03 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:03 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-277] Instantiated an idempotent producer.
2025-04-13 11:48:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:03 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544883058
2025-04-13 11:48:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_91', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:48:03 [kafka-producer-network-thread | producer-277] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-277] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:03 [kafka-producer-network-thread | producer-277] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-277] ProducerId set to 9457 with epoch 0
2025-04-13 11:48:03 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:03 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_52, value: {"_id": {"$oid": "67f10b97558ab0f6396b143e"}, "review": "This changed my life!", "authId": "user_52", "datetime": "2025-01-10T06:57:54.745Z", "sentiment": null}, offset: 71046
2025-04-13 11:48:03 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_52
2025-04-13 11:48:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b143e"}, "review": "This changed my life!", "authId": "user_52", "datetime": "2025-01-10T06:57:54.745Z", "sentiment": null}%n
2025-04-13 11:48:04 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:48:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:48:04 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-278
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:04 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:04 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-278] Instantiated an idempotent producer.
2025-04-13 11:48:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:04 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544884506
2025-04-13 11:48:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_52', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:48:04 [kafka-producer-network-thread | producer-278] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-278] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:04 [kafka-producer-network-thread | producer-278] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-278] ProducerId set to 9458 with epoch 0
2025-04-13 11:48:04 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:04 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_75, value: {"_id": {"$oid": "67f10b97558ab0f6396b1455"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_75", "datetime": "2025-02-20T20:29:36.163Z", "sentiment": null}, offset: 71047
2025-04-13 11:48:04 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_75
2025-04-13 11:48:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1455"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_75", "datetime": "2025-02-20T20:29:36.163Z", "sentiment": null}%n
2025-04-13 11:48:06 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.3
score: 0.6

2025-04-13 11:48:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.3
score: 0.6

2025-04-13 11:48:06 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-279
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:06 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:06 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-279] Instantiated an idempotent producer.
2025-04-13 11:48:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:06 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544886009
2025-04-13 11:48:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:06 [kafka-producer-network-thread | producer-279] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-279] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_75', sentiment=magnitude: 1.3
score: 0.6
}
2025-04-13 11:48:06 [kafka-producer-network-thread | producer-279] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-279] ProducerId set to 9459 with epoch 0
2025-04-13 11:48:06 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:06 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_9, value: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}, offset: 71048
2025-04-13 11:48:06 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_9
2025-04-13 11:48:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}%n
2025-04-13 11:48:07 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:48:07 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:48:07 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-280
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:07 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:07 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-280] Instantiated an idempotent producer.
2025-04-13 11:48:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:07 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544887482
2025-04-13 11:48:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:07 [kafka-producer-network-thread | producer-280] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-280] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_9', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:48:07 [kafka-producer-network-thread | producer-280] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-280] ProducerId set to 9460 with epoch 0
2025-04-13 11:48:07 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:08 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_15, value: {"_id": {"$oid": "67f10b97558ab0f6396b1419"}, "review": "Mediocre quality, nothing special.", "authId": "user_15", "datetime": "2024-12-29T15:59:17.459Z", "sentiment": null}, offset: 71049
2025-04-13 11:48:08 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_15
2025-04-13 11:48:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1419"}, "review": "Mediocre quality, nothing special.", "authId": "user_15", "datetime": "2024-12-29T15:59:17.459Z", "sentiment": null}%n
2025-04-13 11:48:09 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:09 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-281
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:09 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:09 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-281] Instantiated an idempotent producer.
2025-04-13 11:48:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:09 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544889025
2025-04-13 11:48:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_15', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:09 [kafka-producer-network-thread | producer-281] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-281] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:09 [kafka-producer-network-thread | producer-281] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-281] ProducerId set to 9461 with epoch 0
2025-04-13 11:48:09 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:09 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 71050
2025-04-13 11:48:09 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-13 11:48:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-13 11:48:10 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:48:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:48:10 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-282
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:10 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:10 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-282] Instantiated an idempotent producer.
2025-04-13 11:48:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:10 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544890485
2025-04-13 11:48:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:10 [kafka-producer-network-thread | producer-282] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-282] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:48:10 [kafka-producer-network-thread | producer-282] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-282] ProducerId set to 9462 with epoch 0
2025-04-13 11:48:10 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:10 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_71, value: {"_id": {"$oid": "67f10b97558ab0f6396b1451"}, "review": "Excellent! Totally worth the price.", "authId": "user_71", "datetime": "2025-03-01T23:14:24.186Z", "sentiment": null}, offset: 71051
2025-04-13 11:48:10 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_71
2025-04-13 11:48:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1451"}, "review": "Excellent! Totally worth the price.", "authId": "user_71", "datetime": "2025-03-01T23:14:24.186Z", "sentiment": null}%n
2025-04-13 11:48:11 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:48:11 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:48:12 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-283
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:12 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:12 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-283] Instantiated an idempotent producer.
2025-04-13 11:48:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:12 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544892006
2025-04-13 11:48:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:12 [kafka-producer-network-thread | producer-283] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-283] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_71', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:48:12 [kafka-producer-network-thread | producer-283] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-283] ProducerId set to 9463 with epoch 0
2025-04-13 11:48:12 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:12 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_29, value: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}, offset: 71052
2025-04-13 11:48:12 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_29
2025-04-13 11:48:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1427"}, "review": "Very bad quality, feels cheap.", "authId": "user_29", "datetime": "2025-03-11T08:35:47.362Z", "sentiment": null}%n
2025-04-13 11:48:13 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:13 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-284
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:13 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:13 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-284] Instantiated an idempotent producer.
2025-04-13 11:48:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:13 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544893456
2025-04-13 11:48:13 [kafka-producer-network-thread | producer-284] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-284] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:13 [kafka-producer-network-thread | producer-284] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-284] ProducerId set to 9464 with epoch 0
2025-04-13 11:48:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_29', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:13 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:13 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_1, value: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}, offset: 71053
2025-04-13 11:48:13 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_1
2025-04-13 11:48:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b140b"}, "review": "Would not recommend to anyone.", "authId": "user_1", "datetime": "2025-02-11T21:08:49.821Z", "sentiment": null}%n
2025-04-13 11:48:14 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:14 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-285
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:14 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:14 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-285] Instantiated an idempotent producer.
2025-04-13 11:48:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:14 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544894892
2025-04-13 11:48:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:14 [kafka-producer-network-thread | producer-285] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-285] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_1', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:14 [kafka-producer-network-thread | producer-285] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-285] ProducerId set to 9465 with epoch 0
2025-04-13 11:48:14 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:14 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_59, value: {"_id": {"$oid": "67f10b97558ab0f6396b1445"}, "review": "It's okay, does the job.", "authId": "user_59", "datetime": "2024-12-03T13:32:51.362Z", "sentiment": null}, offset: 71054
2025-04-13 11:48:14 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_59
2025-04-13 11:48:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1445"}, "review": "It's okay, does the job.", "authId": "user_59", "datetime": "2024-12-03T13:32:51.362Z", "sentiment": null}%n
2025-04-13 11:48:15 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-13 11:48:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-13 11:48:15 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-286
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:15 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:15 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-286] Instantiated an idempotent producer.
2025-04-13 11:48:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:15 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544895390
2025-04-13 11:48:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_59', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-13 11:48:15 [kafka-producer-network-thread | producer-286] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-286] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:15 [kafka-producer-network-thread | producer-286] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-286] ProducerId set to 9466 with epoch 0
2025-04-13 11:48:15 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:15 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_79, value: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}, offset: 71055
2025-04-13 11:48:15 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_79
2025-04-13 11:48:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1459"}, "review": "Very bad quality, feels cheap.", "authId": "user_79", "datetime": "2025-01-19T05:55:16.222Z", "sentiment": null}%n
2025-04-13 11:48:16 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:16 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-287
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:16 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:16 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-287] Instantiated an idempotent producer.
2025-04-13 11:48:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:16 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544896848
2025-04-13 11:48:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:16 [kafka-producer-network-thread | producer-287] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-287] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_79', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:16 [kafka-producer-network-thread | producer-287] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-287] ProducerId set to 9467 with epoch 0
2025-04-13 11:48:16 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:16 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_45, value: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}, offset: 71056
2025-04-13 11:48:16 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_45
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}%n
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-288
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-288] Instantiated an idempotent producer.
2025-04-13 11:48:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544898294
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_45', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:18 [kafka-producer-network-thread | producer-288] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-288] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:18 [kafka-producer-network-thread | producer-288] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-288] ProducerId set to 9468 with epoch 0
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_72, value: {"_id": {"$oid": "67f10b97558ab0f6396b1452"}, "review": "Mediocre quality, nothing special.", "authId": "user_72", "datetime": "2025-01-26T15:04:47.280Z", "sentiment": null}, offset: 71057
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_72
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1452"}, "review": "Mediocre quality, nothing special.", "authId": "user_72", "datetime": "2025-01-26T15:04:47.280Z", "sentiment": null}%n
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:18 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-289
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:18 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:18 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-289] Instantiated an idempotent producer.
2025-04-13 11:48:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:18 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544898813
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_72', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:18 [kafka-producer-network-thread | producer-289] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-289] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:18 [kafka-producer-network-thread | producer-289] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-289] ProducerId set to 9469 with epoch 0
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_22, value: {"_id": {"$oid": "67f10b97558ab0f6396b1420"}, "review": "Poor build quality, not reliable.", "authId": "user_22", "datetime": "2024-12-05T07:32:28.430Z", "sentiment": null}, offset: 71058
2025-04-13 11:48:18 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_22
2025-04-13 11:48:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1420"}, "review": "Poor build quality, not reliable.", "authId": "user_22", "datetime": "2024-12-05T07:32:28.430Z", "sentiment": null}%n
2025-04-13 11:48:19 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:19 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:19 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-290
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:19 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:19 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-290] Instantiated an idempotent producer.
2025-04-13 11:48:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:19 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544899312
2025-04-13 11:48:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:19 [kafka-producer-network-thread | producer-290] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-290] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_22', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:19 [kafka-producer-network-thread | producer-290] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-290] ProducerId set to 9470 with epoch 0
2025-04-13 11:48:19 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:20 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_24, value: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}, offset: 71059
2025-04-13 11:48:20 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_24
2025-04-13 11:48:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}%n
2025-04-13 11:48:21 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:48:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:48:21 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-291
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:21 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:21 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-291] Instantiated an idempotent producer.
2025-04-13 11:48:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:21 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544901800
2025-04-13 11:48:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_24', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:48:21 [kafka-producer-network-thread | producer-291] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-291] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:21 [kafka-producer-network-thread | producer-291] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-291] ProducerId set to 9471 with epoch 0
2025-04-13 11:48:21 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:21 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_90, value: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}, offset: 71060
2025-04-13 11:48:21 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_90
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}%n
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: -0.1

2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: -0.1

2025-04-13 11:48:22 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-292
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:22 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:22 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-292] Instantiated an idempotent producer.
2025-04-13 11:48:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544902324
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:22 [kafka-producer-network-thread | producer-292] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-292] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_90', sentiment=magnitude: 0.1
score: -0.1
}
2025-04-13 11:48:22 [kafka-producer-network-thread | producer-292] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-292] ProducerId set to 9472 with epoch 0
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_16, value: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}, offset: 71061
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_16
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141a"}, "review": "Fairly average, you get what you pay for.", "authId": "user_16", "datetime": "2025-01-16T20:51:26.326Z", "sentiment": null}%n
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:48:22 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-293
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:22 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:22 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-293] Instantiated an idempotent producer.
2025-04-13 11:48:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:22 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544902861
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_16', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:48:22 [kafka-producer-network-thread | producer-293] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-293] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:22 [kafka-producer-network-thread | producer-293] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-293] ProducerId set to 9473 with epoch 0
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_6, value: {"_id": {"$oid": "67f10b97558ab0f6396b1410"}, "review": "Im very happy with this purchase!", "authId": "user_6", "datetime": "2025-01-21T18:03:15.045Z", "sentiment": null}, offset: 71062
2025-04-13 11:48:22 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_6
2025-04-13 11:48:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1410"}, "review": "Im very happy with this purchase!", "authId": "user_6", "datetime": "2025-01-21T18:03:15.045Z", "sentiment": null}%n
2025-04-13 11:48:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: 0.2

2025-04-13 11:48:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: 0.2

2025-04-13 11:48:24 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-294
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:24 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:24 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-294] Instantiated an idempotent producer.
2025-04-13 11:48:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:24 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544904455
2025-04-13 11:48:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:24 [kafka-producer-network-thread | producer-294] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-294] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_6', sentiment=magnitude: 0.2
score: 0.2
}
2025-04-13 11:48:24 [kafka-producer-network-thread | producer-294] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-294] ProducerId set to 9474 with epoch 0
2025-04-13 11:48:24 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:24 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_75, value: {"_id": {"$oid": "67f10b97558ab0f6396b1455"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_75", "datetime": "2025-02-20T20:29:36.163Z", "sentiment": null}, offset: 71063
2025-04-13 11:48:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_75
2025-04-13 11:48:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1455"}, "review": "Absolutely love it! Highly recommend.", "authId": "user_75", "datetime": "2025-02-20T20:29:36.163Z", "sentiment": null}%n
2025-04-13 11:48:24 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.3
score: 0.6

2025-04-13 11:48:24 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.3
score: 0.6

2025-04-13 11:48:25 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-295
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:25 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:25 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-295] Instantiated an idempotent producer.
2025-04-13 11:48:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:25 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544905011
2025-04-13 11:48:25 [kafka-producer-network-thread | producer-295] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-295] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:25 [kafka-producer-network-thread | producer-295] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-295] ProducerId set to 9475 with epoch 0
2025-04-13 11:48:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_75', sentiment=magnitude: 1.3
score: 0.6
}
2025-04-13 11:48:25 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:25 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_68, value: {"_id": {"$oid": "67f10b97558ab0f6396b144e"}, "review": "Poor build quality, not reliable.", "authId": "user_68", "datetime": "2025-01-15T21:21:03.997Z", "sentiment": null}, offset: 71064
2025-04-13 11:48:25 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_68
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b144e"}, "review": "Poor build quality, not reliable.", "authId": "user_68", "datetime": "2025-01-15T21:21:03.997Z", "sentiment": null}%n
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-296
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-296] Instantiated an idempotent producer.
2025-04-13 11:48:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544906469
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:26 [kafka-producer-network-thread | producer-296] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-296] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_68', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:26 [kafka-producer-network-thread | producer-296] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-296] ProducerId set to 9476 with epoch 0
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_57, value: {"_id": {"$oid": "67f10b97558ab0f6396b1443"}, "review": "This changed my life!", "authId": "user_57", "datetime": "2025-03-21T00:28:58.918Z", "sentiment": null}, offset: 71065
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_57
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1443"}, "review": "This changed my life!", "authId": "user_57", "datetime": "2025-03-21T00:28:58.918Z", "sentiment": null}%n
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:48:26 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-297
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:26 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:26 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-297] Instantiated an idempotent producer.
2025-04-13 11:48:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:26 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544906991
2025-04-13 11:48:26 [kafka-producer-network-thread | producer-297] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-297] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:26 [kafka-producer-network-thread | producer-297] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-297] ProducerId set to 9477 with epoch 0
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_57', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:26 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_92, value: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}, offset: 71066
2025-04-13 11:48:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_92
2025-04-13 11:48:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1466"}, "review": "Horrible customer service.", "authId": "user_92", "datetime": "2025-02-16T16:39:31.912Z", "sentiment": null}%n
2025-04-13 11:48:27 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:48:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:48:27 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-298
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:27 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:27 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-298] Instantiated an idempotent producer.
2025-04-13 11:48:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:27 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544907513
2025-04-13 11:48:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:27 [kafka-producer-network-thread | producer-298] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-298] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_92', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:48:27 [kafka-producer-network-thread | producer-298] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-298] ProducerId set to 9478 with epoch 0
2025-04-13 11:48:27 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:27 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_86, value: {"_id": {"$oid": "67f10b97558ab0f6396b1460"}, "review": "Smooth and easy to use.", "authId": "user_86", "datetime": "2025-02-22T20:30:11.182Z", "sentiment": null}, offset: 71067
2025-04-13 11:48:27 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_86
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1460"}, "review": "Smooth and easy to use.", "authId": "user_86", "datetime": "2025-02-22T20:30:11.182Z", "sentiment": null}%n
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = 
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data 
2025-04-13 11:48:28 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-299
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:28 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:28 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-299] Instantiated an idempotent producer.
2025-04-13 11:48:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544908026
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:28 [kafka-producer-network-thread | producer-299] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-299] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_86', sentiment=}
2025-04-13 11:48:28 [kafka-producer-network-thread | producer-299] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-299] ProducerId set to 9479 with epoch 0
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_9, value: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}, offset: 71068
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_9
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1413"}, "review": "Excellent! Totally worth the price.", "authId": "user_9", "datetime": "2025-03-10T19:20:58.735Z", "sentiment": null}%n
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 1.5
score: 0.7

2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 1.5
score: 0.7

2025-04-13 11:48:28 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-300
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:28 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:28 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-300] Instantiated an idempotent producer.
2025-04-13 11:48:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:28 [kafka-producer-network-thread | producer-300] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-300] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:28 [kafka-producer-network-thread | producer-300] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-300] ProducerId set to 9480 with epoch 0
2025-04-13 11:48:28 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544908568
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_9', sentiment=magnitude: 1.5
score: 0.7
}
2025-04-13 11:48:28 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:29 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 71069
2025-04-13 11:48:29 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:48:31 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-301
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:31 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:31 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-301] Instantiated an idempotent producer.
2025-04-13 11:48:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544911127
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:48:31 [kafka-producer-network-thread | producer-301] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-301] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:31 [kafka-producer-network-thread | producer-301] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-301] ProducerId set to 9481 with epoch 0
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_7, value: {"_id": {"$oid": "67f10b97558ab0f6396b1411"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_7", "datetime": "2025-03-02T02:14:03.248Z", "sentiment": null}, offset: 71070
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_7
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1411"}, "review": "Top-notch! Will definitely recommend.", "authId": "user_7", "datetime": "2025-03-02T02:14:03.248Z", "sentiment": null}%n
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.9
score: 0.4

2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.9
score: 0.4

2025-04-13 11:48:31 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-302
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:31 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:31 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-302] Instantiated an idempotent producer.
2025-04-13 11:48:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:31 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544911662
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:31 [kafka-producer-network-thread | producer-302] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-302] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_7', sentiment=magnitude: 0.9
score: 0.4
}
2025-04-13 11:48:31 [kafka-producer-network-thread | producer-302] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-302] ProducerId set to 9482 with epoch 0
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_59, value: {"_id": {"$oid": "67f10b97558ab0f6396b1445"}, "review": "It's okay, does the job.", "authId": "user_59", "datetime": "2024-12-03T13:32:51.362Z", "sentiment": null}, offset: 71071
2025-04-13 11:48:31 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_59
2025-04-13 11:48:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1445"}, "review": "It's okay, does the job.", "authId": "user_59", "datetime": "2024-12-03T13:32:51.362Z", "sentiment": null}%n
2025-04-13 11:48:32 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-13 11:48:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-13 11:48:32 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-303
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:32 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:32 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-303] Instantiated an idempotent producer.
2025-04-13 11:48:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:32 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544912379
2025-04-13 11:48:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_59', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-13 11:48:32 [kafka-producer-network-thread | producer-303] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-303] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:32 [kafka-producer-network-thread | producer-303] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-303] ProducerId set to 9483 with epoch 0
2025-04-13 11:48:32 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:32 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_90, value: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}, offset: 71072
2025-04-13 11:48:32 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_90
2025-04-13 11:48:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1464"}, "review": "This changed my life!", "authId": "user_90", "datetime": "2025-02-19T23:47:30.750Z", "sentiment": null}%n
2025-04-13 11:48:33 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.1
score: -0.1

2025-04-13 11:48:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.1
score: -0.1

2025-04-13 11:48:33 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-304
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:33 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:33 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-304] Instantiated an idempotent producer.
2025-04-13 11:48:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:33 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544913822
2025-04-13 11:48:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_90', sentiment=magnitude: 0.1
score: -0.1
}
2025-04-13 11:48:33 [kafka-producer-network-thread | producer-304] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-304] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:33 [kafka-producer-network-thread | producer-304] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-304] ProducerId set to 9484 with epoch 0
2025-04-13 11:48:33 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:33 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_38, value: {"_id": {"$oid": "67f10b97558ab0f6396b1430"}, "review": "Completely useless and overpriced.", "authId": "user_38", "datetime": "2025-01-03T15:06:04.506Z", "sentiment": null}, offset: 71073
2025-04-13 11:48:33 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_38
2025-04-13 11:48:34 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1430"}, "review": "Completely useless and overpriced.", "authId": "user_38", "datetime": "2025-01-03T15:06:04.506Z", "sentiment": null}%n
2025-04-13 11:48:34 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:34 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:34 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-305
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:34 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:34 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-305] Instantiated an idempotent producer.
2025-04-13 11:48:34 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:34 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:34 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544914428
2025-04-13 11:48:34 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:34 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_38', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:34 [kafka-producer-network-thread | producer-305] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-305] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:34 [kafka-producer-network-thread | producer-305] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-305] ProducerId set to 9485 with epoch 0
2025-04-13 11:48:34 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:34 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_21, value: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}, offset: 71074
2025-04-13 11:48:34 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_21
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b141f"}, "review": "Horrible customer service.", "authId": "user_21", "datetime": "2025-02-08T01:09:32.028Z", "sentiment": null}%n
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.5
score: -0.5

2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.5
score: -0.5

2025-04-13 11:48:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-306
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-306] Instantiated an idempotent producer.
2025-04-13 11:48:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544915049
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_21', sentiment=magnitude: 0.5
score: -0.5
}
2025-04-13 11:48:35 [kafka-producer-network-thread | producer-306] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-306] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:35 [kafka-producer-network-thread | producer-306] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-306] ProducerId set to 9486 with epoch 0
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_35, value: {"_id": {"$oid": "67f10b97558ab0f6396b142d"}, "review": "Terrible experience, very disappointed.", "authId": "user_35", "datetime": "2025-03-06T15:09:01.869Z", "sentiment": null}, offset: 71075
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_35
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b142d"}, "review": "Terrible experience, very disappointed.", "authId": "user_35", "datetime": "2025-03-06T15:09:01.869Z", "sentiment": null}%n
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:35 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-307
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:35 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:35 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-307] Instantiated an idempotent producer.
2025-04-13 11:48:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:35 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544915561
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_35', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:35 [kafka-producer-network-thread | producer-307] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-307] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:35 [kafka-producer-network-thread | producer-307] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-307] ProducerId set to 9487 with epoch 0
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_38, value: {"_id": {"$oid": "67f10b97558ab0f6396b1430"}, "review": "Completely useless and overpriced.", "authId": "user_38", "datetime": "2025-01-03T15:06:04.506Z", "sentiment": null}, offset: 71076
2025-04-13 11:48:35 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_38
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1430"}, "review": "Completely useless and overpriced.", "authId": "user_38", "datetime": "2025-01-03T15:06:04.506Z", "sentiment": null}%n
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.7
score: -0.7

2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.7
score: -0.7

2025-04-13 11:48:36 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-308
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:36 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:36 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-308] Instantiated an idempotent producer.
2025-04-13 11:48:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544916078
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_38', sentiment=magnitude: 0.7
score: -0.7
}
2025-04-13 11:48:36 [kafka-producer-network-thread | producer-308] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-308] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:36 [kafka-producer-network-thread | producer-308] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-308] ProducerId set to 9488 with epoch 0
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_89, value: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}, offset: 71077
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_89
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1463"}, "review": "It broke after a week of use.", "authId": "user_89", "datetime": "2025-01-07T07:29:12.974Z", "sentiment": null}%n
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:48:36 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-309
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:36 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:36 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-309] Instantiated an idempotent producer.
2025-04-13 11:48:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:36 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544916568
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_89', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:48:36 [kafka-producer-network-thread | producer-309] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-309] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:36 [kafka-producer-network-thread | producer-309] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-309] ProducerId set to 9489 with epoch 0
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_24, value: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}, offset: 71078
2025-04-13 11:48:36 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_24
2025-04-13 11:48:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1422"}, "review": "Would not recommend to anyone.", "authId": "user_24", "datetime": "2025-02-23T15:21:37.169Z", "sentiment": null}%n
2025-04-13 11:48:37 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.6
score: -0.6

2025-04-13 11:48:37 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.6
score: -0.6

2025-04-13 11:48:37 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-310
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:37 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:37 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-310] Instantiated an idempotent producer.
2025-04-13 11:48:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:37 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544917171
2025-04-13 11:48:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_24', sentiment=magnitude: 0.6
score: -0.6
}
2025-04-13 11:48:37 [kafka-producer-network-thread | producer-310] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-310] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:37 [kafka-producer-network-thread | producer-310] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-310] ProducerId set to 9490 with epoch 0
2025-04-13 11:48:37 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_27, value: {"_id": {"$oid": "67f10b97558ab0f6396b1425"}, "review": "This changed my life!", "authId": "user_27", "datetime": "2024-12-10T09:12:46.567Z", "sentiment": null}, offset: 71079
2025-04-13 11:48:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_27
2025-04-13 11:48:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1425"}, "review": "This changed my life!", "authId": "user_27", "datetime": "2024-12-10T09:12:46.567Z", "sentiment": null}%n
2025-04-13 11:48:38 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.2
score: -0.2

2025-04-13 11:48:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.2
score: -0.2

2025-04-13 11:48:38 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-311
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:38 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:38 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-311] Instantiated an idempotent producer.
2025-04-13 11:48:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:38 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544918770
2025-04-13 11:48:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:38 [kafka-producer-network-thread | producer-311] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-311] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_27', sentiment=magnitude: 0.2
score: -0.2
}
2025-04-13 11:48:38 [kafka-producer-network-thread | producer-311] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-311] ProducerId set to 9491 with epoch 0
2025-04-13 11:48:38 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:38 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_44, value: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}, offset: 71080
2025-04-13 11:48:38 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_44
2025-04-13 11:48:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1436"}, "review": "Not bad, not great either.", "authId": "user_44", "datetime": "2024-12-21T16:54:59.782Z", "sentiment": null}%n
2025-04-13 11:48:39 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.4
score: -0.4

2025-04-13 11:48:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.4
score: -0.4

2025-04-13 11:48:39 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-312
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:39 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:39 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-312] Instantiated an idempotent producer.
2025-04-13 11:48:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:39 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544919277
2025-04-13 11:48:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_44', sentiment=magnitude: 0.4
score: -0.4
}
2025-04-13 11:48:39 [kafka-producer-network-thread | producer-312] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-312] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:39 [kafka-producer-network-thread | producer-312] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-312] ProducerId set to 9492 with epoch 0
2025-04-13 11:48:39 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:39 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_59, value: {"_id": {"$oid": "67f10b97558ab0f6396b1445"}, "review": "It's okay, does the job.", "authId": "user_59", "datetime": "2024-12-03T13:32:51.362Z", "sentiment": null}, offset: 71081
2025-04-13 11:48:39 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_59
2025-04-13 11:48:40 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Text: {"_id": {"$oid": "67f10b97558ab0f6396b1445"}, "review": "It's okay, does the job.", "authId": "user_59", "datetime": "2024-12-03T13:32:51.362Z", "sentiment": null}%n
2025-04-13 11:48:40 [Worker-1] INFO  o.s.t.s.GoogleNaturalLanguageService - Sentiment: = magnitude: 0.3
score: -0.3

2025-04-13 11:48:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - Sentiment of the data magnitude: 0.3
score: -0.3

2025-04-13 11:48:40 [Worker-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-313
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2025-04-13 11:48:40 [Worker-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-04-13 11:48:40 [Worker-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-313] Instantiated an idempotent producer.
2025-04-13 11:48:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.8.0
2025-04-13 11:48:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 771b9576b00ecf5b
2025-04-13 11:48:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1744544920719
2025-04-13 11:48:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - Processing one random record...
2025-04-13 11:48:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - sent one ProcessedMessage: ProcessedMessage{authId='user_59', sentiment=magnitude: 0.3
score: -0.3
}
2025-04-13 11:48:40 [kafka-producer-network-thread | producer-313] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-313] Cluster ID: 5L6g3nShT-eMCtK--X86sw
2025-04-13 11:48:40 [kafka-producer-network-thread | producer-313] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-313] ProducerId set to 9493 with epoch 0
2025-04-13 11:48:40 [Worker-1] INFO  o.s.t.kafka.producer.Producer - ProcessedMessage sent to Kafka topic: processed-data-topic
2025-04-13 11:48:40 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_57, value: {"_id": {"$oid": "67f10b97558ab0f6396b1443"}, "review": "This changed my life!", "authId": "user_57", "datetime": "2025-03-21T00:28:58.918Z", "sentiment": null}, offset: 71082
2025-04-13 11:48:40 [Worker-1] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_57
2025-04-13 11:48:40 [Worker-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Revoke previously assigned partitions test-topic-0
2025-04-13 11:48:40 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Member consumer-contacts-test-group-1-715df488-e3ea-42e4-a146-027c5bda4234 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-13 11:48:40 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-13 11:48:40 [Worker-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-1, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-13 11:48:40 [Worker-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-13 11:48:40 [Worker-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-13 11:48:40 [Worker-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-13 11:48:40 [Worker-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-13 11:48:40 [Worker-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-1 unregistered
2025-04-13 11:48:40 [Worker-1] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-13 11:48:42 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:42 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:42 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:42 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:42 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:42 [Worker-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:42 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:43 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:43 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:43 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:43 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:43 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:43 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:43 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:43 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=118, memberId='consumer-contacts-test-group-2-d65807e0-4f2b-497b-ba07-f5030e3b20d4', protocol='range'}
2025-04-13 11:48:43 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=118, memberId='consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32', protocol='range'}
2025-04-13 11:48:43 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=118, memberId='consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2', protocol='range'}
2025-04-13 11:48:43 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=118, memberId='consumer-contacts-test-group-3-950f0571-d3f2-40d7-b664-06529a6ce9ab', protocol='range'}
2025-04-13 11:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Finished assignment for group at generation 118: {consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32=Assignment(partitions=[]), consumer-contacts-test-group-2-d65807e0-4f2b-497b-ba07-f5030e3b20d4=Assignment(partitions=[test-topic-0]), consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda=Assignment(partitions=[]), consumer-contacts-test-group-3-950f0571-d3f2-40d7-b664-06529a6ce9ab=Assignment(partitions=[]), consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2=Assignment(partitions=[])}
2025-04-13 11:48:43 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=118, memberId='consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda', protocol='range'}
2025-04-13 11:48:43 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=118, memberId='consumer-contacts-test-group-3-950f0571-d3f2-40d7-b664-06529a6ce9ab', protocol='range'}
2025-04-13 11:48:43 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=118, memberId='consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32', protocol='range'}
2025-04-13 11:48:43 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=118, memberId='consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda', protocol='range'}
2025-04-13 11:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=118, memberId='consumer-contacts-test-group-2-d65807e0-4f2b-497b-ba07-f5030e3b20d4', protocol='range'}
2025-04-13 11:48:43 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=118, memberId='consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2', protocol='range'}
2025-04-13 11:48:43 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-13 11:48:43 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:48:43 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:48:43 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:48:43 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:48:43 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-13 11:48:43 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:48:43 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:48:43 [Worker-2] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=71089, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=16}}
2025-04-13 11:48:43 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_91, value: {"_id": {"$oid": "67f10b97558ab0f6396b1465"}, "review": "Im very happy with this purchase!", "authId": "user_91", "datetime": "2025-03-06T05:11:20.005Z", "sentiment": null}, offset: 71089
2025-04-13 11:48:43 [Worker-2] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_91
2025-04-13 11:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Revoke previously assigned partitions test-topic-0
2025-04-13 11:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Member consumer-contacts-test-group-2-d65807e0-4f2b-497b-ba07-f5030e3b20d4 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-13 11:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-13 11:48:43 [Worker-2] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-2, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-13 11:48:43 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-13 11:48:43 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-13 11:48:43 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-13 11:48:43 [Worker-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-13 11:48:43 [Worker-2] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-2 unregistered
2025-04-13 11:48:43 [Worker-2] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-13 11:48:46 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:46 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:46 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:46 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:47 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:47 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:47 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:47 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:47 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:47 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:47 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:47 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:47 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=119, memberId='consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda', protocol='range'}
2025-04-13 11:48:47 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=119, memberId='consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2', protocol='range'}
2025-04-13 11:48:47 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=119, memberId='consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32', protocol='range'}
2025-04-13 11:48:47 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=119, memberId='consumer-contacts-test-group-3-950f0571-d3f2-40d7-b664-06529a6ce9ab', protocol='range'}
2025-04-13 11:48:47 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Finished assignment for group at generation 119: {consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32=Assignment(partitions=[]), consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda=Assignment(partitions=[]), consumer-contacts-test-group-3-950f0571-d3f2-40d7-b664-06529a6ce9ab=Assignment(partitions=[test-topic-0]), consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2=Assignment(partitions=[])}
2025-04-13 11:48:47 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=119, memberId='consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda', protocol='range'}
2025-04-13 11:48:47 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=119, memberId='consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2', protocol='range'}
2025-04-13 11:48:47 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=119, memberId='consumer-contacts-test-group-3-950f0571-d3f2-40d7-b664-06529a6ce9ab', protocol='range'}
2025-04-13 11:48:47 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=119, memberId='consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32', protocol='range'}
2025-04-13 11:48:47 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:48:47 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:48:47 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:48:47 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-13 11:48:47 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:48:47 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:48:47 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-13 11:48:47 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:48:47 [Worker-3] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=71099, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=16}}
2025-04-13 11:48:47 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_45, value: {"_id": {"$oid": "67f10b97558ab0f6396b1437"}, "review": "Poor build quality, not reliable.", "authId": "user_45", "datetime": "2025-03-14T07:03:12.767Z", "sentiment": null}, offset: 71099
2025-04-13 11:48:47 [Worker-3] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_45
2025-04-13 11:48:47 [Worker-3] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Revoke previously assigned partitions test-topic-0
2025-04-13 11:48:47 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Member consumer-contacts-test-group-3-950f0571-d3f2-40d7-b664-06529a6ce9ab sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-13 11:48:47 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-13 11:48:47 [Worker-3] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-3, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-13 11:48:47 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-13 11:48:47 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-13 11:48:47 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-13 11:48:47 [Worker-3] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-13 11:48:47 [Worker-3] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-3 unregistered
2025-04-13 11:48:47 [Worker-3] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-13 11:48:50 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:50 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:50 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:51 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:51 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:51 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:51 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:51 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=120, memberId='consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2', protocol='range'}
2025-04-13 11:48:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=120, memberId='consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda', protocol='range'}
2025-04-13 11:48:51 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=120, memberId='consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32', protocol='range'}
2025-04-13 11:48:51 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Finished assignment for group at generation 120: {consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32=Assignment(partitions=[]), consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda=Assignment(partitions=[test-topic-0]), consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2=Assignment(partitions=[])}
2025-04-13 11:48:51 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=120, memberId='consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2', protocol='range'}
2025-04-13 11:48:51 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=120, memberId='consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32', protocol='range'}
2025-04-13 11:48:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=120, memberId='consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda', protocol='range'}
2025-04-13 11:48:51 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:48:51 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:48:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-13 11:48:51 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:48:51 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:48:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-13 11:48:51 [Worker-4] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=71109, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=16}}
2025-04-13 11:48:51 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_43, value: {"_id": {"$oid": "67f10b97558ab0f6396b1435"}, "review": "Not bad, not great either.", "authId": "user_43", "datetime": "2025-01-07T00:44:08.424Z", "sentiment": null}, offset: 71109
2025-04-13 11:48:51 [Worker-4] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_43
2025-04-13 11:48:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Revoke previously assigned partitions test-topic-0
2025-04-13 11:48:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Member consumer-contacts-test-group-4-9c52a3d3-08fc-4ac0-8796-a5f276003dda sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-13 11:48:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-13 11:48:51 [Worker-4] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-4, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-13 11:48:51 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-13 11:48:51 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-13 11:48:51 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-13 11:48:51 [Worker-4] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-13 11:48:51 [Worker-4] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-4 unregistered
2025-04-13 11:48:51 [Worker-4] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-13 11:48:54 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:54 [kafka-coordinator-heartbeat-thread | contacts-test-group] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:55 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:55 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:55 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:55 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:55 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=121, memberId='consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2', protocol='range'}
2025-04-13 11:48:55 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=121, memberId='consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32', protocol='range'}
2025-04-13 11:48:55 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Finished assignment for group at generation 121: {consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32=Assignment(partitions=[]), consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2=Assignment(partitions=[test-topic-0])}
2025-04-13 11:48:55 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=121, memberId='consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32', protocol='range'}
2025-04-13 11:48:55 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=121, memberId='consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2', protocol='range'}
2025-04-13 11:48:55 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[])
2025-04-13 11:48:55 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-13 11:48:55 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: 
2025-04-13 11:48:55 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-13 11:48:55 [Worker-5] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=71119, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=16}}
2025-04-13 11:48:55 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_62, value: {"_id": {"$oid": "67f10b97558ab0f6396b1448"}, "review": "Poor build quality, not reliable.", "authId": "user_62", "datetime": "2025-03-04T10:01:05.882Z", "sentiment": null}, offset: 71119
2025-04-13 11:48:55 [Worker-5] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_62
2025-04-13 11:48:55 [Worker-5] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Revoke previously assigned partitions test-topic-0
2025-04-13 11:48:55 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Member consumer-contacts-test-group-5-ed63f21d-443d-43e8-80e3-7ce0985a76b2 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-13 11:48:55 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-13 11:48:55 [Worker-5] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-5, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-13 11:48:55 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-13 11:48:55 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-13 11:48:55 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-13 11:48:55 [Worker-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-13 11:48:55 [Worker-5] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-5 unregistered
2025-04-13 11:48:55 [Worker-5] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: group is already rebalancing
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Revoke previously assigned partitions 
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] (Re-)joining group
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully joined group with generation Generation{generationId=122, memberId='consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32', protocol='range'}
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Finished assignment for group at generation 122: {consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32=Assignment(partitions=[test-topic-0])}
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Successfully synced group in generation Generation{generationId=122, memberId='consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32', protocol='range'}
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Notifying assignor about the new Assignment(partitions=[test-topic-0])
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Adding newly assigned partitions: test-topic-0
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition test-topic-0 to the committed offset FetchPosition{offset=71129, offsetEpoch=Optional[2], currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=16}}
2025-04-13 11:48:58 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Processing message - key: user_68, value: {"_id": {"$oid": "67f10b97558ab0f6396b144e"}, "review": "Poor build quality, not reliable.", "authId": "user_68", "datetime": "2025-01-15T21:21:03.997Z", "sentiment": null}, offset: 71129
2025-04-13 11:48:58 [Worker-6] INFO  o.s.t.kafka.consumer.ReviewsConsumer - ProcessedMessage key user_68
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Revoke previously assigned partitions test-topic-0
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Member consumer-contacts-test-group-6-8c57706b-b9e3-4e98-8e2a-c4ad1d3bdc32 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer is being closed
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-04-13 11:48:58 [Worker-6] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-contacts-test-group-6, groupId=contacts-test-group] Request joining group due to: consumer pro-actively leaving the group
2025-04-13 11:48:58 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-04-13 11:48:58 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-04-13 11:48:58 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-04-13 11:48:58 [Worker-6] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-04-13 11:48:58 [Worker-6] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-contacts-test-group-6 unregistered
2025-04-13 11:48:58 [Worker-6] INFO  o.s.t.k.consumer.ConsumerTemplate - Kafka consumer closed.
